{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Risk Model Pipeline - Master End-to-End Notebook\n",
    "\n",
    "Bu notebook baÅŸtan sona tÃ¼m pipeline'Ä± Ã§alÄ±ÅŸtÄ±rÄ±r:\n",
    "- Kurulum (pip install)\n",
    "- Data hazÄ±rlama\n",
    "- Veri sÃ¶zlÃ¼ÄŸÃ¼ oluÅŸturma\n",
    "- Model eÄŸitimi\n",
    "- Kalibrasyon\n",
    "- Skorlama\n",
    "- Raporlama\n",
    "\n",
    "**NOT:** TÃ¼m hÃ¼creler sÄ±rayla Ã§alÄ±ÅŸtÄ±rÄ±lmalÄ±dÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ BÃ–LÃœM 1: KURULUM VE IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 - Paket Kurulumu (ilk kurulum iÃ§in)\n",
    "# NOT: EÄŸer zaten kuruluysa bu hÃ¼creyi atlayabilirsiniz\n",
    "\n",
    "# GitHub'dan kurulum\n",
    "!pip install git+https://github.com/selimoksuz/risk-model-pipeline.git\n",
    "\n",
    "# Veya lokal kurulum (eÄŸer clone yaptÄ±ysanÄ±z)\n",
    "# !pip install -e ../  # notebook klasÃ¶rÃ¼nden bir Ã¼st dizin\n",
    "\n",
    "print(\"âœ… Paket kurulumu tamamlandÄ±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 - Gerekli kÃ¼tÃ¼phaneleri import et\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Pipeline imports\n",
    "try:\n",
    "    from risk_pipeline.pipeline16 import RiskModelPipeline, Config\n",
    "    from risk_pipeline.utils.pipeline_runner import run_pipeline_from_dataframe\n",
    "    from risk_pipeline.utils.scoring import score_data, load_model_artifacts\n",
    "    print(\"âœ… Pipeline modÃ¼lleri baÅŸarÄ±yla import edildi (pip install)\")\n",
    "except ImportError:\n",
    "    # Lokal import (eÄŸer pip install yapmadÄ±ysanÄ±z)\n",
    "    sys.path.append('..')\n",
    "    from src.risk_pipeline.pipeline16 import RiskModelPipeline, Config\n",
    "    from src.risk_pipeline.utils.pipeline_runner import run_pipeline_from_dataframe\n",
    "    from src.risk_pipeline.utils.scoring import score_data, load_model_artifacts\n",
    "    print(\"âœ… Pipeline modÃ¼lleri baÅŸarÄ±yla import edildi (lokal)\")\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š BÃ–LÃœM 2: VERÄ° HAZIRLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 - Ana veri setini oluÅŸtur\n",
    "np.random.seed(42)  # Tekrarlanabilirlik iÃ§in\n",
    "\n",
    "# Parametreler\n",
    "n_train = 10000  # EÄŸitim verisi boyutu\n",
    "n_calibration = 2000  # Kalibrasyon verisi boyutu\n",
    "n_scoring = 3000  # Skorlama verisi boyutu\n",
    "\n",
    "print(\"ğŸ“Š Veri setleri oluÅŸturuluyor...\")\n",
    "\n",
    "# YardÄ±mcÄ± fonksiyon: Veri seti oluÅŸtur\n",
    "def create_dataset(n_samples, start_date, id_prefix, target_rate=0.2, add_missing=True):\n",
    "    \"\"\"Risk modeli iÃ§in sentetik veri oluÅŸtur\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        # ID ve tarih\n",
    "        'app_id': [f'{id_prefix}_{i:06d}' for i in range(n_samples)],\n",
    "        'app_dt': pd.date_range(start_date, periods=n_samples, freq='D'),\n",
    "        \n",
    "        # Target\n",
    "        'target': np.random.binomial(1, target_rate, n_samples),\n",
    "        \n",
    "        # Numerik Ã¶zellikler\n",
    "        'yas': np.random.randint(18, 70, n_samples),\n",
    "        'gelir': np.random.lognormal(10, 0.5, n_samples),\n",
    "        'kredi_skoru': np.random.normal(650, 100, n_samples).clip(300, 850),\n",
    "        'borc_tutari': np.random.exponential(30000, n_samples),\n",
    "        'kredi_tutari': np.random.exponential(50000, n_samples),\n",
    "        'calisma_suresi': np.random.exponential(5, n_samples).clip(0, 40),\n",
    "        'bagimlÄ±_sayisi': np.random.poisson(1.5, n_samples).clip(0, 6),\n",
    "        'ev_deger': np.random.lognormal(12, 0.8, n_samples),\n",
    "        \n",
    "        # Kategorik Ã¶zellikler\n",
    "        'egitim': np.random.choice(['Ilkokul', 'Lise', 'Lisans', 'Y.Lisans', 'Doktora'], \n",
    "                                   n_samples, p=[0.1, 0.3, 0.35, 0.2, 0.05]),\n",
    "        'istihdam': np.random.choice(['Maasli', 'Serbest', 'Emekli', 'Ogrenci', 'Issiz'], \n",
    "                                     n_samples, p=[0.5, 0.25, 0.1, 0.05, 0.1]),\n",
    "        'medeni_hal': np.random.choice(['Bekar', 'Evli', 'Bosanmis', 'Dul'], \n",
    "                                       n_samples, p=[0.3, 0.5, 0.15, 0.05]),\n",
    "        'konut_durumu': np.random.choice(['Kendi', 'Kira', 'Aile', 'Lojman'], \n",
    "                                         n_samples, p=[0.4, 0.35, 0.2, 0.05]),\n",
    "        'sehir_tipi': np.random.choice(['Buyuksehir', 'Sehir', 'Ilce', 'Koy'], \n",
    "                                       n_samples, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "        'bolge': np.random.choice(['Marmara', 'Ege', 'Akdeniz', 'IC_Anadolu', 'Karadeniz', 'Dogu', 'GDogu'], \n",
    "                                 n_samples, p=[0.3, 0.15, 0.15, 0.15, 0.1, 0.1, 0.05]),\n",
    "        'sektor': np.random.choice(['Kamu', 'Ozel', 'Serbest'], n_samples, p=[0.3, 0.5, 0.2]),\n",
    "        'cinsiyet': np.random.choice(['E', 'K'], n_samples, p=[0.52, 0.48])\n",
    "    })\n",
    "    \n",
    "    # Eksik deÄŸerler ekle (gerÃ§ekÃ§ilik iÃ§in)\n",
    "    if add_missing:\n",
    "        missing_cols = ['calisma_suresi', 'bagimlÄ±_sayisi', 'medeni_hal', 'sektor']\n",
    "        for col in missing_cols:\n",
    "            missing_idx = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
    "            df.loc[missing_idx, col] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"âœ… Veri oluÅŸturma fonksiyonu hazÄ±r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 - Veri setlerini oluÅŸtur\n",
    "\n",
    "# EÄŸitim verisi\n",
    "train_df = create_dataset(\n",
    "    n_samples=n_train,\n",
    "    start_date='2022-01-01',\n",
    "    id_prefix='TRAIN',\n",
    "    target_rate=0.15  # %15 default rate\n",
    ")\n",
    "\n",
    "# Kalibrasyon verisi (biraz farklÄ± daÄŸÄ±lÄ±m)\n",
    "calibration_df = create_dataset(\n",
    "    n_samples=n_calibration,\n",
    "    start_date='2023-07-01',\n",
    "    id_prefix='CAL',\n",
    "    target_rate=0.18  # Biraz daha yÃ¼ksek default rate\n",
    ")\n",
    "\n",
    "# Skorlama verisi (bazÄ±larÄ± target'sÄ±z)\n",
    "scoring_df = create_dataset(\n",
    "    n_samples=n_scoring,\n",
    "    start_date='2023-10-01',\n",
    "    id_prefix='SCORE',\n",
    "    target_rate=0.20\n",
    ")\n",
    "\n",
    "# Skorlama verisinin bir kÄ±smÄ±nÄ± target'sÄ±z yap\n",
    "no_target_idx = np.random.choice(scoring_df.index, size=int(0.6 * len(scoring_df)), replace=False)\n",
    "scoring_df.loc[no_target_idx, 'target'] = np.nan\n",
    "\n",
    "print(\"ğŸ“Š Veri Setleri Ã–zeti:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"1. EÄŸitim Verisi:\")\n",
    "print(f\"   - Shape: {train_df.shape}\")\n",
    "print(f\"   - Target rate: {train_df['target'].mean():.2%}\")\n",
    "print(f\"   - Tarih aralÄ±ÄŸÄ±: {train_df['app_dt'].min()} - {train_df['app_dt'].max()}\")\n",
    "\n",
    "print(f\"\\n2. Kalibrasyon Verisi:\")\n",
    "print(f\"   - Shape: {calibration_df.shape}\")\n",
    "print(f\"   - Target rate: {calibration_df['target'].mean():.2%}\")\n",
    "print(f\"   - Tarih aralÄ±ÄŸÄ±: {calibration_df['app_dt'].min()} - {calibration_df['app_dt'].max()}\")\n",
    "\n",
    "print(f\"\\n3. Skorlama Verisi:\")\n",
    "print(f\"   - Shape: {scoring_df.shape}\")\n",
    "print(f\"   - Target olan: {(~scoring_df['target'].isna()).sum()} kayÄ±t\")\n",
    "print(f\"   - Target olmayan: {scoring_df['target'].isna().sum()} kayÄ±t\")\n",
    "print(f\"   - Tarih aralÄ±ÄŸÄ±: {scoring_df['app_dt'].min()} - {scoring_df['app_dt'].max()}\")\n",
    "\n",
    "print(\"\\nâœ… TÃ¼m veri setleri hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š BÃ–LÃœM 3: VERÄ° SÃ–ZLÃœÄÃœ HAZIRLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 - Veri sÃ¶zlÃ¼ÄŸÃ¼ oluÅŸtur\n",
    "\n",
    "data_dictionary = pd.DataFrame({\n",
    "    'alan_adi': [\n",
    "        'yas', 'gelir', 'kredi_skoru', 'borc_tutari', 'kredi_tutari', \n",
    "        'calisma_suresi', 'bagimlÄ±_sayisi', 'ev_deger',\n",
    "        'egitim', 'istihdam', 'medeni_hal', 'konut_durumu', \n",
    "        'sehir_tipi', 'bolge', 'sektor', 'cinsiyet'\n",
    "    ],\n",
    "    'alan_aciklamasi': [\n",
    "        'MÃ¼ÅŸteri yaÅŸÄ± (yÄ±l)',\n",
    "        'AylÄ±k gelir tutarÄ± (TL)',\n",
    "        'Kredi risk skoru (300-850)',\n",
    "        'Mevcut borÃ§ tutarÄ± (TL)',\n",
    "        'Talep edilen kredi tutarÄ± (TL)',\n",
    "        'Mevcut iÅŸyerinde Ã§alÄ±ÅŸma sÃ¼resi (yÄ±l)',\n",
    "        'Bakmakla yÃ¼kÃ¼mlÃ¼ kiÅŸi sayÄ±sÄ±',\n",
    "        'Konut deÄŸeri (TL)',\n",
    "        'En yÃ¼ksek eÄŸitim seviyesi',\n",
    "        'Ä°stihdam durumu',\n",
    "        'Medeni durum',\n",
    "        'Konut sahiplik durumu',\n",
    "        'YerleÅŸim yeri tipi',\n",
    "        'CoÄŸrafi bÃ¶lge',\n",
    "        'Ã‡alÄ±ÅŸÄ±lan sektÃ¶r',\n",
    "        'Cinsiyet (E/K)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"ğŸ“š Veri SÃ¶zlÃ¼ÄŸÃ¼:\")\n",
    "print(\"=\"*80)\n",
    "print(data_dictionary.to_string(index=False))\n",
    "\n",
    "# Excel'e de kaydet (opsiyonel)\n",
    "data_dict_path = 'outputs/data_dictionary.xlsx'\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "data_dictionary.to_excel(data_dict_path, index=False)\n",
    "print(f\"\\nâœ… Veri sÃ¶zlÃ¼ÄŸÃ¼ kaydedildi: {data_dict_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ BÃ–LÃœM 4: PIPELINE KONFIGÃœRASYONU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 - Pipeline konfigÃ¼rasyonunu oluÅŸtur\n",
    "\n",
    "cfg = Config(\n",
    "    # Kolon tanÄ±mlarÄ±\n",
    "    id_col='app_id',\n",
    "    time_col='app_dt',\n",
    "    target_col='target',\n",
    "    \n",
    "    # Veri bÃ¶lme ayarlarÄ±\n",
    "    use_test_split=True,\n",
    "    test_size_row_frac=0.2,  # %20 test\n",
    "    oot_window_months=3,  # 3 aylÄ±k OOT penceresi\n",
    "    \n",
    "    # Veri sÃ¶zlÃ¼ÄŸÃ¼ ve kalibrasyon\n",
    "    data_dictionary_df=data_dictionary,  # Veri sÃ¶zlÃ¼ÄŸÃ¼ DataFrame\n",
    "    calibration_df=calibration_df,  # Kalibrasyon DataFrame\n",
    "    calibration_method='isotonic',  # Kalibrasyon metodu\n",
    "    \n",
    "    # Model ayarlarÄ±\n",
    "    cv_folds=5,  # Cross-validation fold sayÄ±sÄ±\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # TÃ¼m CPU'larÄ± kullan\n",
    "    \n",
    "    # Hyperparameter optimization\n",
    "    hpo_timeout=120,  # 2 dakika timeout\n",
    "    hpo_n_trials=20,  # 20 deneme\n",
    "    \n",
    "    # Feature engineering\n",
    "    rare_threshold=0.02,  # %2'den az gÃ¶rÃ¼len kategoriler\n",
    "    psi_threshold=0.20,  # PSI eÅŸiÄŸi\n",
    "    iv_min=0.02,  # Minimum IV deÄŸeri\n",
    "    corr_threshold=0.95,  # Korelasyon eÅŸiÄŸi\n",
    "    \n",
    "    # Ã‡Ä±ktÄ±lar\n",
    "    output_folder='outputs',\n",
    "    output_excel_path='master_model_report.xlsx',\n",
    "    log_file='pipeline.log',\n",
    "    write_artifacts=True,  # Model artifacts kaydet\n",
    "    write_csv=False,  # CSV yazma (Excel yeterli)\n",
    "    write_parquet=False  # Parquet yazma\n",
    ")\n",
    "\n",
    "print(\"âš™ï¸  KonfigÃ¼rasyon Ã–zeti:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Output klasÃ¶rÃ¼: {cfg.output_folder}\")\n",
    "print(f\"Excel raporu: {cfg.output_excel_path}\")\n",
    "print(f\"Veri sÃ¶zlÃ¼ÄŸÃ¼: {len(data_dictionary)} deÄŸiÅŸken tanÄ±mÄ±\")\n",
    "print(f\"Kalibrasyon: {len(calibration_df)} kayÄ±t\")\n",
    "print(f\"CV Folds: {cfg.cv_folds}\")\n",
    "print(f\"HPO Trials: {cfg.hpo_n_trials}\")\n",
    "print(f\"Random State: {cfg.random_state}\")\n",
    "print(\"\\nâœ… KonfigÃ¼rasyon hazÄ±r!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ BÃ–LÃœM 5: PÄ°PELINE Ã‡ALIÅTIRMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 - Pipeline'Ä± Ã§alÄ±ÅŸtÄ±r\n",
    "\n",
    "print(\"ğŸš€ Pipeline baÅŸlatÄ±lÄ±yor...\")\n",
    "print(\"=\"*80)\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Pipeline oluÅŸtur ve Ã§alÄ±ÅŸtÄ±r\n",
    "pipeline = RiskModelPipeline(cfg)\n",
    "pipeline.run(train_df)\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ… Pipeline tamamlandÄ±! (SÃ¼re: {duration:.1f} saniye)\")\n",
    "print(f\"\\nRun ID: {pipeline.cfg.run_id}\")\n",
    "print(f\"Best Model: {pipeline.best_model_name_}\")\n",
    "print(f\"Final Features: {len(pipeline.final_vars_)} deÄŸiÅŸken\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ BÃ–LÃœM 6: MODEL SONUÃ‡LARINI Ä°NCELEME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 - Model performansÄ±nÄ± incele\n",
    "\n",
    "print(\"ğŸ“ˆ Model Performans Ã–zeti:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if pipeline.models_summary_ is not None:\n",
    "    # En Ã¶nemli metrikleri gÃ¶ster\n",
    "    summary_cols = ['model', 'auc_traincv', 'auc_test', 'auc_oot', 'gini_oot', 'ks_oot']\n",
    "    available_cols = [col for col in summary_cols if col in pipeline.models_summary_.columns]\n",
    "    \n",
    "    print(\"\\nTÃ¼m Modellerin PerformansÄ±:\")\n",
    "    print(pipeline.models_summary_[available_cols].to_string())\n",
    "    \n",
    "    # En iyi model detayÄ±\n",
    "    best_model_row = pipeline.models_summary_[pipeline.models_summary_['model'] == pipeline.best_model_name_].iloc[0]\n",
    "    print(f\"\\nğŸ† En Ä°yi Model: {pipeline.best_model_name_}\")\n",
    "    print(f\"   - AUC (Train/CV): {best_model_row.get('auc_traincv', 'N/A')}\")\n",
    "    print(f\"   - AUC (Test): {best_model_row.get('auc_test', 'N/A')}\")\n",
    "    print(f\"   - AUC (OOT): {best_model_row.get('auc_oot', 'N/A')}\")\n",
    "    print(f\"   - Gini (OOT): {best_model_row.get('gini_oot', 'N/A')}\")\n",
    "    print(f\"   - KS (OOT): {best_model_row.get('ks_oot', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 - En iyi model deÄŸiÅŸkenlerini incele (veri sÃ¶zlÃ¼ÄŸÃ¼ ile)\n",
    "\n",
    "print(\"ğŸ“Š En Ä°yi Model DeÄŸiÅŸkenleri (Ä°lk 20):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if pipeline.best_model_vars_df_ is not None:\n",
    "    # GÃ¶sterilecek kolonlar\n",
    "    display_cols = ['variable', 'description', 'coef_or_importance', 'variable_group']\n",
    "    available_cols = [col for col in display_cols if col in pipeline.best_model_vars_df_.columns]\n",
    "    \n",
    "    # Ä°lk 20 deÄŸiÅŸkeni gÃ¶ster\n",
    "    print(pipeline.best_model_vars_df_[available_cols].head(20).to_string())\n",
    "    \n",
    "    # AÃ§Ä±klamalarÄ±n yÃ¼klendiÄŸini kontrol et\n",
    "    if 'description' in pipeline.best_model_vars_df_.columns:\n",
    "        has_desc = pipeline.best_model_vars_df_['description'].notna().sum()\n",
    "        print(f\"\\nâœ… Veri sÃ¶zlÃ¼ÄŸÃ¼nden {has_desc} deÄŸiÅŸken aÃ§Ä±klamasÄ± yÃ¼klendi\")\n",
    "else:\n",
    "    print(\"âŒ Model deÄŸiÅŸkenleri bulunamadÄ±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 - WOE raporunu incele (monotonic sÄ±ralÄ±)\n",
    "\n",
    "print(\"ğŸ“Š WOE Raporu Ã–rneÄŸi (Ä°lk 2 DeÄŸiÅŸken):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if pipeline.best_model_woe_df_ is not None:\n",
    "    # Ä°lk 2 deÄŸiÅŸken iÃ§in WOE binlerini gÃ¶ster\n",
    "    unique_vars = pipeline.best_model_woe_df_['variable'].unique()[:2]\n",
    "    \n",
    "    for var in unique_vars:\n",
    "        var_woe = pipeline.best_model_woe_df_[pipeline.best_model_woe_df_['variable'] == var]\n",
    "        \n",
    "        print(f\"\\nğŸ“Œ DeÄŸiÅŸken: {var}\")\n",
    "        \n",
    "        # AÃ§Ä±klama varsa gÃ¶ster\n",
    "        if 'variable_description' in var_woe.columns:\n",
    "            desc = var_woe['variable_description'].iloc[0]\n",
    "            if desc:\n",
    "                print(f\"   AÃ§Ä±klama: {desc}\")\n",
    "        \n",
    "        # WOE binleri\n",
    "        display_cols = ['group', 'bin_from', 'bin_to', 'count', 'event_rate', 'woe']\n",
    "        available_cols = [col for col in display_cols if col in var_woe.columns]\n",
    "        \n",
    "        print(\"\\n   WOE Binleri (event_rate'e gÃ¶re sÄ±ralÄ±):\")\n",
    "        print(var_woe[available_cols].to_string())\n",
    "        \n",
    "        # Monotonicity kontrolÃ¼\n",
    "        event_rates = var_woe['event_rate'].values\name",
    "        is_monotonic = all(event_rates[i] <= event_rates[i+1] for i in range(len(event_rates)-1))\n",
    "        print(f\"   Monotonic: {'âœ… Evet' if is_monotonic else 'âŒ HayÄ±r'}\")\n",
    "else:\n",
    "    print(\"âŒ WOE raporu bulunamadÄ±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ BÃ–LÃœM 7: SKORLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 - Model artifacts'larÄ± yÃ¼kle\n",
    "\n",
    "print(\"ğŸ“¦ Model Artifacts YÃ¼kleniyor...\")\n",
    "\n",
    "run_id = pipeline.cfg.run_id\n",
    "output_folder = pipeline.cfg.output_folder\n",
    "\n",
    "# Model dosyalarÄ±\n",
    "model_file = f\"{output_folder}/best_model_{run_id}.joblib\"\n",
    "features_file = f\"{output_folder}/final_vars_{run_id}.json\"\n",
    "woe_file = f\"{output_folder}/woe_mapping_{run_id}.json\"\n",
    "calibrator_file = f\"{output_folder}/calibrator_{run_id}.pkl\"\n",
    "\n",
    "# DosyalarÄ± kontrol et\n",
    "print(f\"\\nDosya KontrolÃ¼:\")\n",
    "print(f\"  Model: {'âœ…' if os.path.exists(model_file) else 'âŒ'}\")\n",
    "print(f\"  Features: {'âœ…' if os.path.exists(features_file) else 'âŒ'}\")\n",
    "print(f\"  WOE Mapping: {'âœ…' if os.path.exists(woe_file) else 'âŒ'}\")\n",
    "print(f\"  Calibrator: {'âœ…' if os.path.exists(calibrator_file) else 'âŒ'}\")\n",
    "\n",
    "# YÃ¼kle\n",
    "if all(os.path.exists(f) for f in [model_file, features_file, woe_file]):\n",
    "    model = joblib.load(model_file)\n",
    "    with open(features_file, 'r') as f:\n",
    "        final_features = json.load(f)\n",
    "    with open(woe_file, 'r') as f:\n",
    "        woe_mapping = json.load(f)\n",
    "    \n",
    "    # Calibrator (varsa)\n",
    "    calibrator = None\n",
    "    if os.path.exists(calibrator_file):\n",
    "        calibrator = joblib.load(calibrator_file)\n",
    "    \n",
    "    print(f\"\\nâœ… Artifacts yÃ¼klendi:\")\n",
    "    print(f\"  Model tipi: {type(model).__name__}\")\n",
    "    print(f\"  Feature sayÄ±sÄ±: {len(final_features)}\")\n",
    "    print(f\"  WOE deÄŸiÅŸken sayÄ±sÄ±: {len(woe_mapping)}\")\n",
    "    print(f\"  Calibrator: {'Var' if calibrator else 'Yok'}\")\n",
    "else:\n",
    "    print(\"âŒ BazÄ± dosyalar bulunamadÄ±!\")\n",
    "    model = pipeline.models_[pipeline.best_model_name_]\n",
    "    final_features = pipeline.final_vars_\n",
    "    woe_mapping = {}\n",
    "    for v, vw in pipeline.woe_map.items():\n",
    "        woe_mapping[v] = vw.__dict__\n",
    "    calibrator = pipeline.calibrator_ if hasattr(pipeline, 'calibrator_') else None\n",
    "    print(\"âœ… Pipeline'dan yÃ¼klendi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 - Skorlama yap\n",
    "\n",
    "print(\"ğŸ¯ Skorlama BaÅŸlatÄ±lÄ±yor...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Score data fonksiyonunu import et\n",
    "try:\n",
    "    from risk_pipeline.utils.scoring import score_data\n",
    "except:\n",
    "    from src.risk_pipeline.utils.scoring import score_data\n",
    "\n",
    "# Skorlama yap\n",
    "scoring_results = score_data(\n",
    "    scoring_df=scoring_df,\n",
    "    model=model,\n",
    "    final_features=final_features,\n",
    "    woe_mapping=woe_mapping,\n",
    "    calibrator=calibrator,\n",
    "    training_scores=None,  # PSI hesaplama iÃ§in (ÅŸimdilik None)\n",
    "    feature_mapping=None\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“Š Skorlama SonuÃ§larÄ±:\")\n",
    "print(f\"  Toplam skorlanan: {scoring_results['n_total']:,}\")\n",
    "print(f\"  Target'lÄ± kayÄ±t: {scoring_results['n_with_target']:,}\")\n",
    "print(f\"  Target'sÄ±z kayÄ±t: {scoring_results['n_without_target']:,}\")\n",
    "\n",
    "# Target'lÄ± kayÄ±tlar iÃ§in performans\n",
    "if 'with_target' in scoring_results and scoring_results['with_target']:\n",
    "    print(f\"\\nğŸ“ˆ Performans (Target'lÄ± KayÄ±tlar):\")\n",
    "    print(f\"  AUC: {scoring_results['with_target']['auc']:.4f}\")\n",
    "    print(f\"  Gini: {scoring_results['with_target']['gini']:.4f}\")\n",
    "    print(f\"  KS: {scoring_results['with_target']['ks']:.4f}\")\n",
    "\n",
    "# Skor daÄŸÄ±lÄ±mÄ±\n",
    "if 'scores' in scoring_results:\n",
    "    scores = np.array(scoring_results['scores'])\n",
    "    print(f\"\\nğŸ“Š Skor DaÄŸÄ±lÄ±mÄ±:\")\n",
    "    print(f\"  Min: {scores.min():.4f}\")\n",
    "    print(f\"  Q1: {np.percentile(scores, 25):.4f}\")\n",
    "    print(f\"  Median: {np.median(scores):.4f}\")\n",
    "    print(f\"  Q3: {np.percentile(scores, 75):.4f}\")\n",
    "    print(f\"  Max: {scores.max():.4f}\")\n",
    "    print(f\"  Mean: {scores.mean():.4f}\")\n",
    "    print(f\"  Std: {scores.std():.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Skorlama tamamlandÄ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š BÃ–LÃœM 8: EXCEL RAPORU Ä°NCELEME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 - Excel raporunu kontrol et\n",
    "\n",
    "excel_path = os.path.join(cfg.output_folder, cfg.output_excel_path)\n",
    "\n",
    "print(\"ğŸ“Š Excel Raporu KontrolÃ¼:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if os.path.exists(excel_path):\n",
    "    print(f\"âœ… Excel dosyasÄ± bulundu: {excel_path}\")\n",
    "    print(f\"   Boyut: {os.path.getsize(excel_path) / 1024:.1f} KB\")\n",
    "    \n",
    "    # Excel'deki sheet'leri listele\n",
    "    excel_file = pd.ExcelFile(excel_path)\n",
    "    sheets = excel_file.sheet_names\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Toplam {len(sheets)} sheet bulundu:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Sheet'leri kategorize et\n",
    "    main_sheets = ['final_vars', 'best_name', 'models_summary', 'best_model_vars_df', 'best_model_woe_df']\n",
    "    performance_sheets = ['ks_info_traincv', 'ks_info_test', 'ks_info_oot', 'gini_summary']\n",
    "    feature_sheets = ['top20_iv_df', 'top50_univariate', 'psi_summary', 'psi_dropped_features']\n",
    "    correlation_sheets = ['correlation_matrix', 'correlation_clusters', 'corr_dropped']\n",
    "    \n",
    "    print(\"\\nğŸ”¹ Ana Raporlar:\")\n",
    "    for sheet in main_sheets:\n",
    "        if sheet in sheets:\n",
    "            print(f\"   âœ… {sheet}\")\n",
    "    \n",
    "    print(\"\\nğŸ”¹ Performans RaporlarÄ±:\")\n",
    "    for sheet in performance_sheets:\n",
    "        if sheet in sheets:\n",
    "            print(f\"   âœ… {sheet}\")\n",
    "    \n",
    "    print(\"\\nğŸ”¹ Feature RaporlarÄ±:\")\n",
    "    for sheet in feature_sheets:\n",
    "        if sheet in sheets:\n",
    "            print(f\"   âœ… {sheet}\")\n",
    "    \n",
    "    print(\"\\nğŸ”¹ Korelasyon RaporlarÄ±:\")\n",
    "    for sheet in correlation_sheets:\n",
    "        if sheet in sheets:\n",
    "            print(f\"   âœ… {sheet}\")\n",
    "    \n",
    "    # DiÄŸer sheet'ler\n",
    "    other_sheets = [s for s in sheets if s not in main_sheets + performance_sheets + feature_sheets + correlation_sheets]\n",
    "    if other_sheets:\n",
    "        print(\"\\nğŸ”¹ DiÄŸer Raporlar:\")\n",
    "        for sheet in other_sheets:\n",
    "            print(f\"   âœ… {sheet}\")\n",
    "    \n",
    "    # Veri sÃ¶zlÃ¼ÄŸÃ¼ kontrolÃ¼\n",
    "    print(\"\\nğŸ“š Veri SÃ¶zlÃ¼ÄŸÃ¼ Entegrasyonu:\")\n",
    "    if 'best_model_vars_df' in sheets:\n",
    "        best_vars = pd.read_excel(excel_path, sheet_name='best_model_vars_df')\n",
    "        if 'description' in best_vars.columns:\n",
    "            has_desc = best_vars['description'].notna().sum()\n",
    "            print(f\"   âœ… {has_desc}/{len(best_vars)} deÄŸiÅŸken aÃ§Ä±klamasÄ± mevcut\")\n",
    "        else:\n",
    "            print(\"   âŒ AÃ§Ä±klama kolonu bulunamadÄ±\")\n",
    "    \n",
    "    # WOE monotonic kontrolÃ¼\n",
    "    print(\"\\nğŸ“ˆ WOE Monotonic SÄ±ralama:\")\n",
    "    if 'best_model_woe_df' in sheets:\n",
    "        woe_df = pd.read_excel(excel_path, sheet_name='best_model_woe_df')\n",
    "        if 'bin_from' in woe_df.columns and 'bin_to' in woe_df.columns:\n",
    "            print(\"   âœ… bin_from/bin_to kolonlarÄ± mevcut\")\n",
    "        if 'variable_description' in woe_df.columns:\n",
    "            print(\"   âœ… DeÄŸiÅŸken aÃ§Ä±klamalarÄ± mevcut\")\n",
    "        \n",
    "        # Monotonic kontrol\n",
    "        first_var = woe_df['variable'].iloc[0] if len(woe_df) > 0 else None\n",
    "        if first_var:\n",
    "            var_woe = woe_df[woe_df['variable'] == first_var]\n",
    "            event_rates = var_woe['event_rate'].values\n",
    "            is_mono = all(event_rates[i] <= event_rates[i+1] for i in range(len(event_rates)-1))\n",
    "            print(f\"   {'âœ…' if is_mono else 'âŒ'} Ä°lk deÄŸiÅŸken monotonic sÄ±ralÄ±\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ Excel dosyasÄ± bulunamadÄ±: {excel_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Excel raporu kontrolÃ¼ tamamlandÄ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ BÃ–LÃœM 9: Ã–ZET VE SONUÃ‡LAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 - Genel Ã¶zet\n",
    "\n",
    "print(\"ğŸ¯ PIPELINE Ã‡ALIÅTIRMA Ã–ZETÄ°\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š Veri:\")\n",
    "print(f\"  â€¢ EÄŸitim: {train_df.shape[0]:,} kayÄ±t\")\n",
    "print(f\"  â€¢ Kalibrasyon: {calibration_df.shape[0]:,} kayÄ±t\")\n",
    "print(f\"  â€¢ Skorlama: {scoring_df.shape[0]:,} kayÄ±t\")\n",
    "print(f\"  â€¢ Toplam deÄŸiÅŸken: {len(train_df.columns) - 3} (ID, tarih, target hariÃ§)\")\n",
    "\n",
    "print(\"\\nğŸ† Model:\")\n",
    "print(f\"  â€¢ En iyi model: {pipeline.best_model_name_}\")\n",
    "print(f\"  â€¢ Final deÄŸiÅŸken sayÄ±sÄ±: {len(pipeline.final_vars_)}\")\n",
    "print(f\"  â€¢ Kalibrasyon: {'UygulandÄ± âœ…' if pipeline.calibrator_ is not None else 'UygulanmadÄ± âŒ'}\")\n",
    "\n",
    "if pipeline.models_summary_ is not None and pipeline.best_model_name_:\n",
    "    best_row = pipeline.models_summary_[pipeline.models_summary_['model'] == pipeline.best_model_name_].iloc[0]\n",
    "    print(f\"\\nğŸ“ˆ Performans:\")\n",
    "    print(f\"  â€¢ AUC (OOT): {best_row.get('auc_oot', 'N/A')}\")\n",
    "    print(f\"  â€¢ Gini (OOT): {best_row.get('gini_oot', 'N/A')}\")\n",
    "    print(f\"  â€¢ KS (OOT): {best_row.get('ks_oot', 'N/A')}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Ã‡Ä±ktÄ±lar:\")\n",
    "print(f\"  â€¢ Excel raporu: {cfg.output_folder}/{cfg.output_excel_path}\")\n",
    "print(f\"  â€¢ Log dosyasÄ±: {cfg.output_folder}/{cfg.log_file}\")\n",
    "print(f\"  â€¢ Model artifacts: {cfg.output_folder}/best_model_{pipeline.cfg.run_id}.joblib\")\n",
    "print(f\"  â€¢ Run ID: {pipeline.cfg.run_id}\")\n",
    "\n",
    "print(\"\\nâœ¨ Ã–zellikler:\")\n",
    "print(f\"  â€¢ Veri sÃ¶zlÃ¼ÄŸÃ¼ entegrasyonu âœ…\")\n",
    "print(f\"  â€¢ WOE monotonic sÄ±ralama âœ…\")\n",
    "print(f\"  â€¢ DataFrame kalibrasyon desteÄŸi âœ…\")\n",
    "print(f\"  â€¢ GeliÅŸmiÅŸ Excel raporlama âœ…\")\n",
    "print(f\"  â€¢ Otomatik feature engineering âœ…\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ TÃœM Ä°ÅLEMLER BAÅARIYLA TAMAMLANDI!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ BÃ–LÃœM 10: SONUÃ‡LARI KAYDETME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 - Ã–nemli sonuÃ§larÄ± kaydet\n",
    "\n",
    "# SonuÃ§larÄ± bir dictionary'de topla\n",
    "results_summary = {\n",
    "    'run_id': pipeline.cfg.run_id,\n",
    "    'run_date': datetime.now().isoformat(),\n",
    "    'best_model': pipeline.best_model_name_,\n",
    "    'n_final_features': len(pipeline.final_vars_),\n",
    "    'final_features': pipeline.final_vars_,\n",
    "    'n_train': train_df.shape[0],\n",
    "    'n_calibration': calibration_df.shape[0],\n",
    "    'n_scoring': scoring_df.shape[0],\n",
    "    'calibration_applied': pipeline.calibrator_ is not None,\n",
    "    'output_folder': cfg.output_folder,\n",
    "    'excel_report': cfg.output_excel_path\n",
    "}\n",
    "\n",
    "# Performans metrikleri ekle\n",
    "if pipeline.models_summary_ is not None and pipeline.best_model_name_:\n",
    "    best_row = pipeline.models_summary_[pipeline.models_summary_['model'] == pipeline.best_model_name_].iloc[0]\n",
    "    results_summary['performance'] = {\n",
    "        'auc_traincv': float(best_row.get('auc_traincv', 0)),\n",
    "        'auc_test': float(best_row.get('auc_test', 0)),\n",
    "        'auc_oot': float(best_row.get('auc_oot', 0)),\n",
    "        'gini_oot': float(best_row.get('gini_oot', 0)),\n",
    "        'ks_oot': float(best_row.get('ks_oot', 0))\n",
    "    }\n",
    "\n",
    "# JSON olarak kaydet\n",
    "results_file = f\"{cfg.output_folder}/run_summary_{pipeline.cfg.run_id}.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"ğŸ’¾ SonuÃ§ Ã¶zeti kaydedildi: {results_file}\")\n",
    "\n",
    "# Skorlama sonuÃ§larÄ±nÄ± da kaydet\n",
    "if 'scoring_results' in locals():\n",
    "    scoring_file = f\"{cfg.output_folder}/scoring_results_{pipeline.cfg.run_id}.json\"\n",
    "    \n",
    "    # Numpy array'leri liste'ye Ã§evir\n",
    "    scoring_save = {\n",
    "        'n_total': scoring_results['n_total'],\n",
    "        'n_with_target': scoring_results['n_with_target'],\n",
    "        'n_without_target': scoring_results['n_without_target']\n",
    "    }\n",
    "    \n",
    "    if 'with_target' in scoring_results:\n",
    "        scoring_save['with_target_metrics'] = scoring_results['with_target']\n",
    "    \n",
    "    with open(scoring_file, 'w') as f:\n",
    "        json.dump(scoring_save, f, indent=2)\n",
    "    \n",
    "    print(f\"ğŸ’¾ Skorlama sonuÃ§larÄ± kaydedildi: {scoring_file}\")\n",
    "\n",
    "print(\"\\nâœ… TÃ¼m sonuÃ§lar baÅŸarÄ±yla kaydedildi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ TEBRÄ°KLER!\n",
    "\n",
    "End-to-end pipeline baÅŸarÄ±yla tamamlandÄ±. OluÅŸturulan dosyalar:\n",
    "\n",
    "1. **Excel Raporu**: `outputs/master_model_report.xlsx` - TÃ¼m raporlar tek dosyada\n",
    "2. **Model**: `outputs/best_model_[run_id].joblib`\n",
    "3. **WOE Mapping**: `outputs/woe_mapping_[run_id].json`\n",
    "4. **Final Features**: `outputs/final_vars_[run_id].json`\n",
    "5. **Calibrator**: `outputs/calibrator_[run_id].pkl`\n",
    "6. **Log DosyasÄ±**: `outputs/pipeline.log`\n",
    "7. **SonuÃ§ Ã–zeti**: `outputs/run_summary_[run_id].json`\n",
    "\n",
    "### Sonraki AdÄ±mlar:\n",
    "- DiÄŸer simulation notebook'larÄ±nÄ± deneyin\n",
    "- Excel raporunu detaylÄ± inceleyin\n",
    "- Model'i production'a deploy edin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}