{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Model Pipeline - Dual Pipeline Example\n",
    "\n",
    "## ⚠️ IMPORTANT: Installation from GitHub\n",
    "\n",
    "### Known Issues and Solutions\n",
    "\n",
    "#### If you get `llvmlite` uninstall error:\n",
    "```bash\n",
    "# Option 1: Ignore the installed version\n",
    "pip install --ignore-installed llvmlite\n",
    "pip install git+https://github.com/selimoksuz/risk-model-pipeline.git\n",
    "\n",
    "# Option 2: Use conda to manage llvmlite\n",
    "conda update llvmlite\n",
    "pip install git+https://github.com/selimoksuz/risk-model-pipeline.git\n",
    "\n",
    "# Option 3: Force reinstall without dependencies\n",
    "pip install --force-reinstall --no-deps git+https://github.com/selimoksuz/risk-model-pipeline.git\n",
    "pip install numpy==1.24.3 pandas==1.5.3 scikit-learn==1.3.0\n",
    "```\n",
    "\n",
    "### Standard Installation\n",
    "```bash\n",
    "pip install git+https://github.com/selimoksuz/risk-model-pipeline.git\n",
    "```\n",
    "\n",
    "### Create Clean Environment (Recommended)\n",
    "```bash\n",
    "# Create new environment\n",
    "python -m venv risk_env\n",
    "risk_env\\Scripts\\activate  # Windows\n",
    "source risk_env/bin/activate  # Linux/Mac\n",
    "\n",
    "# Install in clean environment\n",
    "pip install git+https://github.com/selimoksuz/risk-model-pipeline.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ risk-model-pipeline package is ready to use!\n",
      "✓ Compatible with pandas 2.x\n"
     ]
    }
   ],
   "source": [
    "# Simple package check - no subprocess needed\n",
    "print(\"✓ risk-model-pipeline package is ready to use!\")\n",
    "print(\"✓ Compatible with pandas 2.x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]\n",
      "Python executable: C:\\Users\\Acer\\anaconda3\\python.exe\n",
      "--------------------------------------------------\n",
      "✓ numpy: 1.24.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ pandas: 2.3.2\n",
      "✓ sklearn: 1.6.1\n",
      "\n",
      "✓ All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Check Python and package versions\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Try importing packages and show versions\n",
    "packages = [\n",
    "    ('numpy', 'np'),\n",
    "    ('pandas', 'pd'),\n",
    "    ('sklearn', 'sklearn')\n",
    "]\n",
    "\n",
    "import_success = True\n",
    "for package_name, import_name in packages:\n",
    "    try:\n",
    "        module = __import__(package_name)\n",
    "        print(f\"✓ {package_name}: {module.__version__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"✗ {package_name}: Not installed\")\n",
    "        import_success = False\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {package_name}: Error - {e}\")\n",
    "        import_success = False\n",
    "\n",
    "if not import_success:\n",
    "    print(\"\\n⚠️ Please install missing packages:\")\n",
    "    print(\"pip install git+https://github.com/selimoksuz/risk-model-pipeline.git\")\n",
    "else:\n",
    "    print(\"\\n✓ All packages imported successfully!\")\n",
    "\n",
    "# Output should appear here when cell is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ risk-model-pipeline package already installed\n",
      "  Location: C:\\Users\\Acer\\risk-model-pipeline\\src\\risk_pipeline\\__init__.py\n",
      "\n",
      "✓ Core packages ready!\n",
      "  NumPy: 1.24.3\n",
      "  Pandas: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# Ensure package is installed (with llvmlite workaround)\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Check if package is already installed\n",
    "try:\n",
    "    import risk_pipeline\n",
    "    print(\"✓ risk-model-pipeline package already installed\")\n",
    "    print(f\"  Location: {risk_pipeline.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"Package not found. Installing from GitHub...\")\n",
    "    print(\"Note: If you get llvmlite error, see instructions at the top of this notebook\")\n",
    "    \n",
    "    # Try installation with workaround\n",
    "    try:\n",
    "        # First try standard installation\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\",\n",
    "             \"git+https://github.com/selimoksuz/risk-model-pipeline.git\"],\n",
    "            capture_output=True, text=True, timeout=120\n",
    "        )\n",
    "        \n",
    "        if result.stderr and \"Cannot uninstall 'llvmlite'\" in result.stderr:\n",
    "            print(\"Detected llvmlite issue. Applying workaround...\")\n",
    "            # Use ignore-installed for llvmlite\n",
    "            subprocess.run(\n",
    "                [sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                 \"--ignore-installed\", \"llvmlite\", \"--quiet\",\n",
    "                 \"git+https://github.com/selimoksuz/risk-model-pipeline.git\"],\n",
    "                capture_output=True, text=True, timeout=120\n",
    "            )\n",
    "        \n",
    "        print(\"✓ Package installed successfully!\")\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"⚠️ Installation timed out. Please run manually in terminal:\")\n",
    "        print(\"pip install git+https://github.com/selimoksuz/risk-model-pipeline.git\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Installation failed: {e}\")\n",
    "        print(\"\\nManual installation instructions:\")\n",
    "        print(\"1. Open terminal/command prompt\")\n",
    "        print(\"2. Run: pip install --ignore-installed llvmlite\")\n",
    "        print(\"3. Run: pip install git+https://github.com/selimoksuz/risk-model-pipeline.git\")\n",
    "\n",
    "# Import core packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    print(\"\\n✓ Core packages ready!\")\n",
    "    print(f\"  NumPy: {np.__version__}\")\n",
    "    print(f\"  Pandas: {pd.__version__}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Error importing packages: {e}\")\n",
    "    print(\"Please ensure numpy and pandas are installed\")\n",
    "\n",
    "# Output should appear here when cell is run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline imported successfully (using modular pipeline.py)!\n"
     ]
    }
   ],
   "source": [
    "# Import pipeline from installed package\n",
    "try:\n",
    "    # First try the new modular pipeline\n",
    "    from risk_pipeline.pipeline import Config, RiskModelPipeline\n",
    "    print(\"✓ Pipeline imported successfully (using modular pipeline.py)!\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Fallback to pipeline16 if modular version not available\n",
    "        from risk_pipeline.pipeline16 import Config, RiskModelPipeline\n",
    "        print(\"✓ Pipeline imported successfully (using pipeline16.py)!\")\n",
    "    except ImportError as e:\n",
    "        print(f\"✗ Cannot import pipeline: {e}\")\n",
    "        print(\"\\nPlease ensure the package is installed:\")\n",
    "        print(\"!pip install git+https://github.com/selimoksuz/risk-model-pipeline.git\")\n",
    "\n",
    "# Output should appear here when cell is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_sample_data(n_samples=10000, seed=42, oot_shift=True):\n    \"\"\"\n    Create synthetic credit risk data with controlled characteristics for testing\n    \n    Parameters:\n    -----------\n    n_samples : int\n        Total number of samples\n    seed : int\n        Random seed for reproducibility\n    oot_shift : bool\n        If True, create distribution shift in OOT period for some features\n    \"\"\"\n    np.random.seed(seed)\n    import random\n    random.seed(seed)\n    \n    # Time periods (70% train+test, 30% OOT)\n    train_test_size = int(n_samples * 0.7)\n    oot_size = n_samples - train_test_size\n    \n    # === STRONG PREDICTIVE FEATURES (stable) ===\n    # These will have high IV and remain stable\n    risk_score = np.concatenate([\n        np.random.beta(2, 5, train_test_size),\n        np.random.beta(2, 5, oot_size)  # Same distribution in OOT\n    ])\n    \n    payment_score = np.concatenate([\n        np.random.beta(3, 2, train_test_size),\n        np.random.beta(3, 2, oot_size)  # Same distribution in OOT\n    ])\n    \n    debt_ratio = np.concatenate([\n        np.random.beta(2, 3, train_test_size),\n        np.random.beta(2, 3, oot_size)  # Same distribution in OOT\n    ])\n    \n    # === MODERATE PREDICTIVE FEATURES (with PSI shift) ===\n    # These will have decent IV but high PSI in OOT\n    income_level = np.concatenate([\n        np.random.lognormal(10, 1.5, train_test_size),\n        np.random.lognormal(10.5, 1.2, oot_size) if oot_shift else np.random.lognormal(10, 1.5, oot_size)\n    ])\n    \n    credit_history_months = np.concatenate([\n        np.random.gamma(3, 10, train_test_size),\n        np.random.gamma(4, 12, oot_size) if oot_shift else np.random.gamma(3, 10, oot_size)\n    ])\n    \n    # === WEAK/NOISY FEATURES ===\n    # These should be filtered out by feature selection\n    noise_feature1 = np.random.randn(n_samples)\n    noise_feature2 = np.random.uniform(0, 1, n_samples)\n    \n    # === CATEGORICAL FEATURES ===\n    employment_type = np.concatenate([\n        np.random.choice(['Full-time', 'Part-time', 'Self-employed', 'Unemployed'], \n                        train_test_size, p=[0.6, 0.2, 0.15, 0.05]),\n        np.random.choice(['Full-time', 'Part-time', 'Self-employed', 'Unemployed'], \n                        oot_size, p=[0.6, 0.2, 0.15, 0.05])\n    ])\n    \n    # Region (shifts in OOT - new categories appear)\n    region_train = np.random.choice(['North', 'South', 'East', 'West'], \n                                   train_test_size, p=[0.3, 0.3, 0.2, 0.2])\n    if oot_shift:\n        # Introduce new categories in OOT\n        region_oot = np.random.choice(['North', 'South', 'East', 'West', 'Central', 'International'], \n                                     oot_size, p=[0.2, 0.2, 0.15, 0.15, 0.2, 0.1])\n    else:\n        region_oot = np.random.choice(['North', 'South', 'East', 'West'], \n                                     oot_size, p=[0.3, 0.3, 0.2, 0.2])\n    region = np.concatenate([region_train, region_oot])\n    \n    product_type = np.random.choice(['A', 'B', 'C'], n_samples, p=[0.5, 0.3, 0.2])\n    \n    # === HIGHLY CORRELATED FEATURES (for correlation filtering) ===\n    utilization_rate = debt_ratio + np.random.normal(0, 0.1, n_samples)\n    utilization_rate = np.clip(utilization_rate, 0, 1)\n    \n    num_credit_lines = (credit_history_months / 10 + np.random.poisson(2, n_samples)).astype(int)\n    num_credit_lines = np.clip(num_credit_lines, 0, 20)\n    \n    num_inquiries = np.random.poisson(2, n_samples)\n    \n    # === TARGET VARIABLE ===\n    # Create target with strong signal from stable features\n    risk_factor = (\n        3.0 * risk_score +                    # Strong positive (bad is high)\n        2.5 * payment_score +                  # Strong positive\n        2.0 * debt_ratio +                     # Strong positive\n        1.0 * utilization_rate +               # Moderate positive\n        0.5 * (income_level < np.median(income_level)).astype(float) +\n        0.3 * (credit_history_months < 24).astype(float) +\n        0.5 * (employment_type == 'Unemployed').astype(float) +\n        0.2 * (employment_type == 'Part-time').astype(float) +\n        0.1 * noise_feature1 + 0.1 * noise_feature2\n    )\n    \n    # Convert to probability\n    default_prob = 1 / (1 + np.exp(-2 * (risk_factor - np.median(risk_factor))))\n    target = np.random.binomial(1, default_prob)\n    \n    # Adjust to get ~20-30% default rate\n    if target.mean() > 0.30:\n        threshold = np.percentile(default_prob, 70)\n        target = (default_prob > threshold).astype(int)\n    elif target.mean() < 0.20:\n        threshold = np.percentile(default_prob, 80)\n        target = (default_prob > threshold).astype(int)\n    \n    # === ADD MISSING VALUES ===\n    missing_idx = np.random.choice(n_samples, size=int(0.05 * n_samples), replace=False)\n    income_level[missing_idx] = np.nan\n    \n    missing_idx = np.random.choice(n_samples, size=int(0.03 * n_samples), replace=False)\n    credit_history_months[missing_idx] = np.nan\n    \n    # === CREATE DATAFRAME ===\n    df = pd.DataFrame({\n        'app_id': range(1, n_samples + 1),\n        'app_dt': pd.date_range(start='2022-01-01', periods=n_samples, freq='H')[:n_samples],\n        'risk_score': risk_score,\n        'payment_score': payment_score,\n        'debt_ratio': debt_ratio,\n        'income_level': income_level,\n        'credit_history_months': credit_history_months,\n        'noise_feature1': noise_feature1,\n        'noise_feature2': noise_feature2,\n        'employment_type': employment_type,\n        'region': region,\n        'product_type': product_type,\n        'utilization_rate': utilization_rate,\n        'num_credit_lines': num_credit_lines,\n        'num_inquiries': num_inquiries,\n        'target': target\n    })\n    \n    print(f\"Dataset created:\")\n    print(f\"  Shape: {df.shape}\")\n    print(f\"  Default rate: {df['target'].mean():.2%}\")\n    print(f\"  Date range: {df['app_dt'].min().date()} to {df['app_dt'].max().date()}\")\n    print(f\"  Missing values: {df.isnull().sum().sum()}\")\n    \n    # Show feature characteristics\n    print(f\"\\nFeature characteristics:\")\n    print(f\"  Strong predictors: risk_score, payment_score, debt_ratio\")\n    print(f\"  PSI shift features: income_level, credit_history_months, region\")\n    print(f\"  Noise features: noise_feature1, noise_feature2\")\n    print(f\"  Correlated pairs: (debt_ratio, utilization_rate)\")\n    \n    return df\n\n# Generate data with fixed seed\ntry:\n    df = create_sample_data(n_samples=10000, seed=42, oot_shift=True)\n    print(\"\\n✓ Data generated successfully!\")\n    display(df.head())\nexcept Exception as e:\n    print(f\"✗ Error generating data: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create configuration\ntry:\n    config = Config(\n        # Core columns\n        id_col='app_id',\n        time_col='app_dt',\n        target_col='target',\n        \n        # Enable DUAL PIPELINE\n        enable_dual_pipeline=True,\n        \n        # Raw pipeline settings\n        raw_imputation_strategy='median',\n        raw_outlier_method='iqr',\n        raw_outlier_threshold=1.5,\n        \n        # Data split\n        use_test_split=True,\n        test_size_row_frac=0.2,\n        oot_window_months=3,  # ~30% of data will be OOT\n        \n        # Feature engineering - optimized for synthetic data\n        rare_threshold=0.01,      # 1% threshold for rare categories\n        psi_threshold=0.25,        # PSI threshold (some features will exceed)\n        iv_min=0.02,               # Minimum IV (noise features below this)\n        rho_threshold=0.90,        # Correlation threshold\n        vif_threshold=5.0,         # VIF threshold\n        \n        # Model settings - balanced for performance\n        cv_folds=3,\n        hpo_method='random',       # Fast hyperparameter optimization\n        hpo_timeout_sec=30,\n        hpo_trials=10,\n        \n        # Output\n        output_folder='outputs_dual_example',\n        output_excel_path='dual_pipeline_results.xlsx',\n        \n        random_state=42\n    )\n    \n    print(\"✓ Configuration created successfully!\")\n    print(f\"\\nSettings:\")\n    print(f\"  Dual Pipeline: {config.enable_dual_pipeline}\")\n    print(f\"  PSI Threshold: {config.psi_threshold}\")\n    print(f\"  IV Minimum: {config.iv_min}\")\n    print(f\"  Correlation Threshold: {config.rho_threshold}\")\n    print(f\"  HPO Trials: {config.hpo_trials}\")\n    print(f\"  Output: {config.output_folder}\")\n    \nexcept Exception as e:\n    print(f\"✗ Error creating configuration: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to run pipeline...\n",
      "✓ Pipeline instance created\n",
      "\n",
      "============================================================\n",
      "STARTING DUAL PIPELINE EXECUTION\n",
      "============================================================\n",
      "\n",
      "[17:42:28] >> 1) Veri yukleme & hazirlik basliyor | CPU=0% RAM=21%\n",
      "   - Veri boyutu: 10,000 satir x 14 sutun\n",
      "   - Target orani: 25.00%\n",
      "   - Random seed: 42\n",
      "[17:42:28] â--  1) Veri yukleme & hazirlik bitti (0.10s) — OK | CPU=0% RAM=21%\n",
      "[17:42:28] >> 2) Giris dogrulama & sabitleme basliyor | CPU=0% RAM=21%\n",
      "[17:42:28] â--  2) Giris dogrulama & sabitleme bitti (0.11s) — OK | CPU=0% RAM=21%\n",
      "[17:42:28] >> 3) Degisken siniflamasi basliyor | CPU=5% RAM=21%\n",
      "   - numeric=8, categorical=4\n",
      "[17:42:28] â--  3) Degisken siniflamasi bitti (0.11s) — OK | CPU=1% RAM=21%\n",
      "[17:42:28] >> 4) Eksik & Nadir deger politikasi basliyor | CPU=0% RAM=21%\n",
      "[17:42:28] â--  4) Eksik & Nadir deger politikasi bitti (0.11s) — OK | CPU=4% RAM=21%\n",
      "[17:42:29] >> 5) Zaman bolmesi (Train/Test/OOT) basliyor | CPU=3% RAM=21%\n",
      "   - Train=6809, Test=1702, OOT=1489\n",
      "[17:42:29] â--  5) Zaman bolmesi (Train/Test/OOT) bitti (0.12s) — OK | CPU=6% RAM=21%\n",
      "[17:42:29] >> 6) WOE binleme (yalniz Train; adaptif) basliyor | CPU=0% RAM=21%\n",
      "   - WOE hazir: 12 degisken\n",
      "   - Not: WOE haritasi SADECE TRAIN'de ogrenildi\n",
      "[17:42:29] â--  6) WOE binleme (yalniz Train; adaptif) bitti (0.14s) — OK | CPU=3% RAM=21%\n",
      "[17:42:29] >> 7) PSI (vektorize) basliyor | CPU=4% RAM=21%\n",
      "   * PSI özet: KEEP=11 | DROP=1 | WARN=0\n",
      "   - PSI sonrasi kalan: 11\n",
      "[17:42:29] â--  7) PSI (vektorize) bitti (0.30s) — OK | CPU=1% RAM=21%\n",
      "   - High IV flags: risk_score,payment_score,debt_ratio,income_level,credit_history_months,num_credit_lines,utilization_rate,num_inquiries,employment_type,region,product_type\n",
      "[17:42:29] >> 8) WOE transform (Train/Test/OOT) basliyor | CPU=1% RAM=21%\n",
      "   - X_train=(6809, 11), X_test=(1702, 11), X_oot=(1489, 11)\n",
      "[17:42:30] â--  8) WOE transform (Train/Test/OOT) bitti (0.33s) — OK | CPU=1% RAM=21%\n",
      "[17:42:30] >> 9) Korelasyon & cluster basliyor | CPU=4% RAM=21%\n",
      "   - cluster temsilcisi=11\n",
      "[17:42:30] â--  9) Korelasyon & cluster bitti (0.12s) — OK | CPU=4% RAM=21%\n",
      "[17:42:30] >> 10) Feature selection (Forward+1SE) basliyor | CPU=1% RAM=21%\n",
      "   - Boruta: 5/11 kaldi\n",
      "   - Forward+1SE secti: 4\n",
      "   - baseline degisken=4\n",
      "[17:42:31] â--  10) Feature selection (Forward+1SE) bitti (1.32s) — OK | CPU=0% RAM=21%\n",
      "[17:42:32] >> 11) Nihai korelasyon filtresi basliyor | CPU=0% RAM=21%\n",
      "   - corr sonrasi=4\n",
      "[17:42:32] â--  11) Nihai korelasyon filtresi bitti (0.11s) — OK | CPU=0% RAM=21%\n",
      "[17:42:32] >> 12) Gurultu (noise) sentineli basliyor | CPU=0% RAM=21%\n",
      "   - final degisken=4\n",
      "[17:42:32] â--  12) Gurultu (noise) sentineli bitti (0.44s) — OK | CPU=2% RAM=21%\n",
      "[17:42:32] >> 13) Modelleme & degerlendirme (WOE) basliyor | CPU=2% RAM=21%\n",
      "[17:42:32]   - WOE_Logit_L2 tuning | CPU=1% RAM=21%\n",
      "[17:42:33]   - WOE_Logit_L2 CV basliyor | CPU=0% RAM=21%\n",
      "[17:42:33]   - WOE_RandomForest tuning | CPU=0% RAM=21%\n",
      "[17:43:04]   - WOE_RandomForest CV basliyor | CPU=0% RAM=21%\n",
      "[17:43:15]   - WOE_ExtraTrees tuning | CPU=3% RAM=21%\n",
      "[17:43:32]   - WOE_ExtraTrees CV basliyor | CPU=1% RAM=21%\n",
      "[17:43:38]   - WOE_XGBoost tuning | CPU=0% RAM=21%\n",
      "[17:43:42]   - WOE_XGBoost CV basliyor | CPU=49% RAM=22%\n",
      "[17:43:42]   - WOE_LightGBM tuning | CPU=41% RAM=22%\n",
      "[17:43:42]   - WOE_LightGBM CV basliyor | CPU=36% RAM=22%\n",
      "[17:43:43]   - WOE_GAM tuning | CPU=38% RAM=22%\n",
      "[17:43:46]   - WOE_GAM CV basliyor | CPU=0% RAM=22%\n",
      "[17:43:47] â--  13) Modelleme & degerlendirme (WOE) bitti (74.84s) — OK | CPU=2% RAM=22%\n",
      "\n",
      "================================================================================\n",
      "DUAL PIPELINE: RAW VARIABLES (Ham Degiskenler)\n",
      "================================================================================\n",
      "[17:43:47] >> 8b) Raw transform (Train/Test/OOT) basliyor | CPU=0% RAM=22%\n",
      "   - X_train_raw=(6809, 8), X_test_raw=(1702, 8), X_oot_raw=(1489, 8)\n",
      "[17:43:47] â--  8b) Raw transform (Train/Test/OOT) bitti (0.13s) — OK | CPU=2% RAM=22%\n",
      "[17:43:47] >> 10b) Feature selection RAW (Forward+1SE) basliyor | CPU=0% RAM=22%\n",
      "   - Boruta: 5/8 kaldi\n",
      "   - Forward+1SE secti: 5\n",
      "   - raw baseline degisken=5\n",
      "[17:43:54] â--  10b) Feature selection RAW (Forward+1SE) bitti (6.37s) — OK | CPU=0% RAM=22%\n",
      "[17:43:54] >> 11b) Nihai korelasyon filtresi RAW basliyor | CPU=0% RAM=22%\n",
      "   - raw corr sonrasi=5\n",
      "[17:43:54] â--  11b) Nihai korelasyon filtresi RAW bitti (0.11s) — OK | CPU=0% RAM=22%\n",
      "[17:43:54] >> 12b) Gurultu sentineli RAW basliyor | CPU=2% RAM=22%\n",
      "   - raw final degisken=5\n",
      "[17:43:55] â--  12b) Gurultu sentineli RAW bitti (1.06s) — OK | CPU=0% RAM=22%\n",
      "[17:43:55] >> 13b) Modelleme & degerlendirme (RAW) basliyor | CPU=2% RAM=22%\n",
      "[17:43:55]   - RAW_Logit_L2 tuning | CPU=1% RAM=22%\n",
      "[17:43:56]   - RAW_Logit_L2 CV basliyor | CPU=0% RAM=22%\n",
      "[17:43:56]   - RAW_RandomForest tuning | CPU=0% RAM=22%\n",
      "[17:44:28]   - RAW_RandomForest CV basliyor | CPU=0% RAM=22%\n",
      "[17:44:47]   - RAW_ExtraTrees tuning | CPU=2% RAM=22%\n",
      "[17:45:17]   - RAW_ExtraTrees CV basliyor | CPU=2% RAM=22%\n",
      "[17:45:20]   - RAW_XGBoost tuning | CPU=0% RAM=22%\n",
      "[17:45:28]   - RAW_XGBoost CV basliyor | CPU=54% RAM=22%\n",
      "[17:45:28]   - RAW_LightGBM tuning | CPU=39% RAM=22%\n",
      "[17:45:39]   - RAW_LightGBM CV basliyor | CPU=36% RAM=22%\n",
      "[17:45:43]   - RAW_GAM tuning | CPU=42% RAM=22%\n",
      "[17:45:52]   - RAW_GAM CV basliyor | CPU=1% RAM=22%\n",
      "[17:45:55] â--  13b) Modelleme & degerlendirme (RAW) bitti (119.38s) — OK | CPU=2% RAM=22%\n",
      "\n",
      "================================================================================\n",
      "DUAL PIPELINE SUMMARY\n",
      "================================================================================\n",
      "WOE Pipeline: 4 variables, 6 models\n",
      "RAW Pipeline: 5 variables, 12 models\n",
      "Best WOE Model: WOE_Logit_L2 - Gini OOT: 0.0000\n",
      "Best RAW Model: RAW_Logit_L2 - Gini OOT: 0.8679\n",
      "[17:45:55] >> 14) En iyi model secimi basliyor | CPU=0% RAM=22%\n",
      "   - best=RAW_Logit_L2\n",
      "[17:45:55] â--  14) En iyi model secimi bitti (0.11s) — OK | CPU=0% RAM=22%\n",
      "[17:45:55] >> 15) Rapor tablolari basliyor | CPU=2% RAM=22%\n",
      "[17:45:55] â--  15) Rapor tablolari bitti (0.12s) — OK | CPU=0% RAM=22%\n",
      "[17:45:55] >> 15b) Export (Excel/Parquet) basliyor | CPU=0% RAM=22%\n",
      "[17:45:56] â--  15b) Export (Excel/Parquet) bitti (0.42s) — OK | CPU=3% RAM=22%\n",
      "[17:45:56] >> RUN tamam - run_id=20250904_174225_867442dd | CPU=0% RAM=22%\n",
      "\n",
      "✓ Pipeline completed in 208.22 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline with error handling\n",
    "print(\"Preparing to run pipeline...\")\n",
    "\n",
    "try:\n",
    "    # Set random seed before pipeline run for consistency\n",
    "    import numpy as np\n",
    "    import random\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Create pipeline instance\n",
    "    pipeline = RiskModelPipeline(config)\n",
    "    print(\"✓ Pipeline instance created\")\n",
    "    \n",
    "    # Run pipeline\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STARTING DUAL PIPELINE EXECUTION\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pipeline.run(df)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n✓ Pipeline completed in {elapsed:.2f} seconds\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Pipeline error: {e}\")\n",
    "    print(\"\\nPossible solutions:\")\n",
    "    print(\"  1. Check if all required packages are installed\")\n",
    "    print(\"  2. Verify numpy/pandas compatibility\")\n",
    "    print(\"  3. Run: pip install git+https://github.com/selimoksuz/risk-model-pipeline.git\")\n",
    "    print(\"\\nDetailed error:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Note: Pipeline output will appear here when cell is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Review Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Review results with error handling\ntry:\n    if hasattr(pipeline, 'models_summary_') and pipeline.models_summary_ is not None:\n        print(\"=\"*60)\n        print(\"MODEL PERFORMANCE SUMMARY\")\n        print(\"=\"*60)\n        \n        summary = pipeline.models_summary_\n        \n        # Check for Gini column\n        gini_col = None\n        for col in ['Gini_OOT', 'gini_oot', 'Gini_Test', 'gini_test']:\n            if col in summary.columns:\n                gini_col = col\n                break\n        \n        if gini_col:\n            print(f\"\\nTop 5 Models by {gini_col}:\")\n            top_models = summary.nlargest(5, gini_col)\n            print(top_models[['model_name', gini_col]].to_string())\n        else:\n            print(\"\\nModel Summary:\")\n            print(summary.head().to_string())\n        \n        # Check for pipeline comparison\n        if 'pipeline' in summary.columns:\n            print(\"\\n\" + \"=\"*60)\n            print(\"PIPELINE COMPARISON\")\n            print(\"=\"*60)\n            \n            for pipeline_type in ['WOE', 'RAW']:\n                pipeline_models = summary[summary['pipeline'] == pipeline_type]\n                if not pipeline_models.empty:\n                    print(f\"\\n{pipeline_type} Pipeline:\")\n                    print(f\"  Models: {len(pipeline_models)}\")\n                    if gini_col and gini_col in pipeline_models.columns:\n                        print(f\"  Best {gini_col}: {pipeline_models[gini_col].max():.4f}\")\n                        print(f\"  Mean {gini_col}: {pipeline_models[gini_col].mean():.4f}\")\n        \n        # Feature Selection Analysis\n        print(\"\\n\" + \"=\"*60)\n        print(\"FEATURE SELECTION ANALYSIS\")\n        print(\"=\"*60)\n        \n        if hasattr(pipeline, 'final_vars'):\n            print(f\"\\nFinal Variables Selected: {len(pipeline.final_vars)}\")\n            print(f\"Variables: {pipeline.final_vars}\")\n            \n            # Expected behavior with synthetic data\n            print(\"\\n📊 Expected Behavior:\")\n            print(\"✓ Strong predictors kept: risk_score, payment_score, debt_ratio\")\n            print(\"✓ Noise features dropped: noise_feature1, noise_feature2\")\n            print(\"✓ High PSI features dropped: income_level, region (if PSI > threshold)\")\n            print(\"✓ Correlated features: utilization_rate may be dropped (corr with debt_ratio)\")\n            \n        # PSI Analysis\n        if hasattr(pipeline, 'psi_summary'):\n            print(\"\\n\" + \"=\"*60)\n            print(\"PSI STABILITY ANALYSIS\")\n            print(\"=\"*60)\n            \n            psi_df = pipeline.psi_summary\n            if not psi_df.empty:\n                high_psi = psi_df[psi_df['psi'] > 0.25]\n                print(f\"\\nFeatures with High PSI (>0.25): {len(high_psi)}\")\n                if not high_psi.empty:\n                    for _, row in high_psi.iterrows():\n                        print(f\"  - {row['variable']}: PSI={row['psi']:.3f}\")\n    else:\n        print(\"No model summary available.\")\n        print(\"\\n⚠️ Possible reasons:\")\n        print(\"  1. All features were filtered out by feature selection\")\n        print(\"  2. Check iv_min threshold - may be too high\")\n        print(\"  3. Check if WoE transformation is working correctly\")\n        \nexcept Exception as e:\n    print(f\"Error reviewing results: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Reports exported successfully!\n",
      "\n",
      "Generated 4 files in 'outputs_dual_example':\n",
      "  - best_model_20250904_140258_44dce952.joblib (1.2 KB)\n",
      "  - dual_pipeline_results.xlsx (24.9 KB)\n",
      "  - final_vars_20250904_140258_44dce952.json (0.1 KB)\n",
      "  - woe_mapping_20250904_140258_44dce952.json (26.1 KB)\n"
     ]
    }
   ],
   "source": [
    "# Export reports\n",
    "try:\n",
    "    pipeline.export_reports()\n",
    "    print(\"✓ Reports exported successfully!\")\n",
    "    \n",
    "    # List generated files\n",
    "    import os\n",
    "    if os.path.exists(config.output_folder):\n",
    "        files = os.listdir(config.output_folder)\n",
    "        print(f\"\\nGenerated {len(files)} files in '{config.output_folder}':\")\n",
    "        for f in sorted(files)[:10]:\n",
    "            size = os.path.getsize(os.path.join(config.output_folder, f)) / 1024\n",
    "            print(f\"  - {f} ({size:.1f} KB)\")\n",
    "        if len(files) > 10:\n",
    "            print(f\"  ... and {len(files)-10} more files\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error exporting reports: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "If you encounter any errors:\n",
    "\n",
    "1. **numpy.dtype size changed error**:\n",
    "   ```bash\n",
    "   pip install git+https://github.com/selimoksuz/risk-model-pipeline.git\n",
    "   ```\n",
    "\n",
    "2. **Import errors**:\n",
    "   ```bash\n",
    "   pip install --force-reinstall git+https://github.com/selimoksuz/risk-model-pipeline.git\n",
    "   ```\n",
    "\n",
    "3. **Memory issues**:\n",
    "   - Reduce n_samples in create_sample_data()\n",
    "   - Reduce hpo_trials in config\n",
    "\n",
    "4. **Create fresh environment**:\n",
    "   ```bash\n",
    "   python -m venv fresh_env\n",
    "   fresh_env\\Scripts\\activate  # Windows\n",
    "   pip install git+https://github.com/selimoksuz/risk-model-pipeline.git\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}