{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Pipeline Test - WOE vs Raw Variables\n",
    "\n",
    "This notebook tests the dual pipeline approach with both WOE transformation and raw variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from risk_pipeline.pipeline16 import Config, RiskModelPipeline\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Realistic Credit Risk Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_realistic_credit_data(n_samples=50000, seed=42):\n",
    "    \"\"\"Create realistic credit risk data with meaningful features (70-80% Gini target)\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Core risk features\n",
    "    risk_score = np.random.beta(2, 5, n_samples)  # Skewed risk distribution\n",
    "    payment_score = np.random.beta(3, 2, n_samples)  # Payment behavior\n",
    "    debt_burden = np.random.exponential(0.3, n_samples)\n",
    "    debt_burden = np.clip(debt_burden, 0, 1)\n",
    "    \n",
    "    # Income and employment\n",
    "    income_stability = np.random.beta(4, 2, n_samples)\n",
    "    employment_score = np.random.beta(5, 2, n_samples)\n",
    "    \n",
    "    # Credit history\n",
    "    credit_age = np.random.gamma(3, 2, n_samples) \n",
    "    credit_utilization = np.random.beta(2, 5, n_samples)\n",
    "    num_accounts = np.random.poisson(4, n_samples)\n",
    "    \n",
    "    # Behavioral features\n",
    "    inquiry_count = np.random.poisson(2, n_samples)\n",
    "    delinquency_flag = np.random.binomial(1, 0.15, n_samples)\n",
    "    \n",
    "    # Create realistic default probability (target ~75% Gini)\n",
    "    default_score = (\n",
    "        3.5 * risk_score +\n",
    "        3.0 * payment_score +\n",
    "        2.0 * debt_burden +\n",
    "        1.2 * (1 - income_stability) +\n",
    "        1.0 * (1 - employment_score) +\n",
    "        0.8 * credit_utilization +\n",
    "        0.6 * (inquiry_count / 10) +\n",
    "        1.5 * delinquency_flag +\n",
    "        -0.3 * np.log1p(credit_age) +\n",
    "        -0.2 * np.log1p(num_accounts) +\n",
    "        np.random.normal(0, 0.6, n_samples)  # Noise for realism\n",
    "    )\n",
    "    \n",
    "    # Convert to probability and create target\n",
    "    default_prob = 1 / (1 + np.exp(-2 * (default_score - np.median(default_score))))\n",
    "    target = np.random.binomial(1, default_prob)\n",
    "    \n",
    "    # Ensure reasonable default rate (10-20%)\n",
    "    if target.mean() > 0.2:\n",
    "        threshold = np.percentile(default_prob, 80)\n",
    "        target = (default_prob > threshold).astype(int)\n",
    "    elif target.mean() < 0.1:\n",
    "        threshold = np.percentile(default_prob, 90)\n",
    "        target = (default_prob > threshold).astype(int)\n",
    "    \n",
    "    # Additional noise features\n",
    "    noise_features = {}\n",
    "    for i in range(5):\n",
    "        noise_features[f'noise_{i+1}'] = np.random.randn(n_samples)\n",
    "    \n",
    "    # Geographic and demographic features\n",
    "    region = np.random.choice(['North', 'South', 'East', 'West'], n_samples)\n",
    "    channel = np.random.choice(['Online', 'Branch', 'Phone'], n_samples, p=[0.5, 0.3, 0.2])\n",
    "    \n",
    "    # Create temporal component for time-based split\n",
    "    days = np.sort(np.random.uniform(0, 365*2, n_samples))\n",
    "    app_dt = pd.to_datetime('2022-01-01') + pd.to_timedelta(days, unit='D')\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'app_id': range(1, n_samples + 1),\n",
    "        'app_dt': app_dt,\n",
    "        'target': target,\n",
    "        # Core features\n",
    "        'risk_score': risk_score,\n",
    "        'payment_score': payment_score,\n",
    "        'debt_burden': debt_burden,\n",
    "        'income_stability': income_stability,\n",
    "        'employment_score': employment_score,\n",
    "        'credit_age': credit_age,\n",
    "        'credit_utilization': credit_utilization,\n",
    "        'num_accounts': num_accounts,\n",
    "        'inquiry_count': inquiry_count,\n",
    "        'delinquency_flag': delinquency_flag,\n",
    "        # Categorical features\n",
    "        'region': region,\n",
    "        'channel': channel,\n",
    "        **noise_features\n",
    "    })\n",
    "    \n",
    "    # Add some missing values for realism\n",
    "    missing_cols = ['employment_score', 'credit_age', 'income_stability']\n",
    "    for col in missing_cols:\n",
    "        missing_idx = np.random.choice(n_samples, size=int(0.05 * n_samples), replace=False)\n",
    "        df.loc[missing_idx, col] = np.nan\n",
    "    \n",
    "    print(f\"Dataset created: {len(df):,} samples\")\n",
    "    print(f\"Default rate: {df['target'].mean():.2%}\")\n",
    "    print(f\"Date range: {df['app_dt'].min().date()} to {df['app_dt'].max().date()}\")\n",
    "    print(f\"Features: {len(df.columns) - 3} (excluding id, date, target)\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum():,}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "df = create_realistic_credit_data(n_samples=30000, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data dictionary with Turkish descriptions\n",
    "data_dict = pd.DataFrame([\n",
    "    {'alan_adi': 'risk_score', 'alan_aciklamasi': 'Genel risk skoru (0-1 arasi)'},\n",
    "    {'alan_adi': 'payment_score', 'alan_aciklamasi': 'Odeme davranisi skoru'},\n",
    "    {'alan_adi': 'debt_burden', 'alan_aciklamasi': 'Borc yuku orani'},\n",
    "    {'alan_adi': 'income_stability', 'alan_aciklamasi': 'Gelir stabilitesi'},\n",
    "    {'alan_adi': 'employment_score', 'alan_aciklamasi': 'Istihdam skoru'},\n",
    "    {'alan_adi': 'credit_age', 'alan_aciklamasi': 'Kredi gecmisi suresi (yil)'},\n",
    "    {'alan_adi': 'credit_utilization', 'alan_aciklamasi': 'Kredi kullanim orani'},\n",
    "    {'alan_adi': 'num_accounts', 'alan_aciklamasi': 'Hesap sayisi'},\n",
    "    {'alan_adi': 'inquiry_count', 'alan_aciklamasi': 'Kredi basvuru sayisi'},\n",
    "    {'alan_adi': 'delinquency_flag', 'alan_aciklamasi': 'Gecikme bayragi'},\n",
    "    {'alan_adi': 'region', 'alan_aciklamasi': 'Bolge'},\n",
    "    {'alan_adi': 'channel', 'alan_aciklamasi': 'Basvuru kanali'},\n",
    "])\n",
    "\n",
    "print(f\"Data dictionary created with {len(data_dict)} variable descriptions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration with Dual Pipeline Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration with dual pipeline\n",
    "config = Config(\n",
    "    # Data columns\n",
    "    id_col='app_id',\n",
    "    time_col='app_dt',\n",
    "    target_col='target',\n",
    "    \n",
    "    # DUAL PIPELINE SETTINGS\n",
    "    enable_dual_pipeline=True,  # Enable both WOE and RAW pipelines\n",
    "    raw_imputation_strategy='median',  # Imputation for raw pipeline\n",
    "    raw_outlier_method='iqr',  # Outlier method for raw pipeline\n",
    "    raw_outlier_threshold=1.5,  # IQR multiplier for outliers\n",
    "    \n",
    "    # Split configuration\n",
    "    use_test_split=True,\n",
    "    test_size_row_frac=0.2,\n",
    "    oot_window_months=3,\n",
    "    \n",
    "    # Feature thresholds (less aggressive for better performance)\n",
    "    rare_threshold=0.005,  # Reduced from 0.02\n",
    "    psi_threshold=0.30,    # Increased from 0.20\n",
    "    iv_min=0.01,          # Reduced from 0.02\n",
    "    rho_threshold=0.98,   # Increased from 0.80\n",
    "    \n",
    "    # Model settings (faster for testing)\n",
    "    cv_folds=3,\n",
    "    hpo_timeout_sec=60,   # Reduced for faster testing\n",
    "    hpo_trials=10,        # Reduced for faster testing\n",
    "    \n",
    "    # Data dictionary\n",
    "    data_dictionary_df=data_dict,\n",
    "    \n",
    "    # Output\n",
    "    output_folder='outputs_dual',\n",
    "    output_excel_path='dual_pipeline_report.xlsx',\n",
    "    \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Configuration created with dual pipeline enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Dual Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and run pipeline\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING DUAL PIPELINE EXECUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = RiskModelPipeline(config)\n",
    "\n",
    "# Run pipeline\n",
    "pipeline.run(df)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nTotal execution time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Pipeline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model summary if available\n",
    "if pipeline.models_summary_ is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sort by Gini OOT\n",
    "    summary = pipeline.models_summary_.sort_values('gini_oot', ascending=False)\n",
    "    \n",
    "    # Display top models\n",
    "    display_cols = ['model_name', 'pipeline', 'gini_cv', 'gini_test', 'gini_oot', 'n_features']\n",
    "    available_cols = [col for col in display_cols if col in summary.columns]\n",
    "    \n",
    "    print(\"\\nTop 10 Models:\")\n",
    "    print(summary[available_cols].head(10).to_string())\n",
    "    \n",
    "    # Compare WOE vs RAW\n",
    "    if 'pipeline' in summary.columns:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"WOE vs RAW COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        woe_models = summary[summary['pipeline'] == 'WOE']\n",
    "        raw_models = summary[summary['pipeline'] == 'RAW']\n",
    "        \n",
    "        if not woe_models.empty:\n",
    "            print(f\"\\nWOE Pipeline:\")\n",
    "            print(f\"  - Models: {len(woe_models)}\")\n",
    "            print(f\"  - Best Gini OOT: {woe_models['gini_oot'].max():.4f}\")\n",
    "            print(f\"  - Avg Gini OOT: {woe_models['gini_oot'].mean():.4f}\")\n",
    "        \n",
    "        if not raw_models.empty:\n",
    "            print(f\"\\nRAW Pipeline:\")\n",
    "            print(f\"  - Models: {len(raw_models)}\")\n",
    "            print(f\"  - Best Gini OOT: {raw_models['gini_oot'].max():.4f}\")\n",
    "            print(f\"  - Avg Gini OOT: {raw_models['gini_oot'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export and Review Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export reports\n",
    "pipeline.export_reports()\n",
    "print(f\"\\nReports exported to: {config.output_folder}/\")\n",
    "print(f\"Excel report: {config.output_excel_path}\")\n",
    "\n",
    "# List output files\n",
    "import os\n",
    "if os.path.exists(config.output_folder):\n",
    "    files = os.listdir(config.output_folder)\n",
    "    print(f\"\\nGenerated files ({len(files)}):\")\n",
    "    for f in sorted(files):\n",
    "        size = os.path.getsize(os.path.join(config.output_folder, f)) / 1024\n",
    "        print(f\"  - {f} ({size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate baseline Gini for comparison\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Get OOT data\n",
    "oot_mask = pipeline.oot_idx_\n",
    "if oot_mask is not None and len(oot_mask) > 0:\n",
    "    X_oot = df.iloc[oot_mask]\n",
    "    y_oot = X_oot['target'].values\n",
    "    \n",
    "    # Simple baseline using risk_score\n",
    "    if 'risk_score' in X_oot.columns:\n",
    "        baseline_scores = X_oot['risk_score'].fillna(X_oot['risk_score'].median())\n",
    "        baseline_auc = roc_auc_score(y_oot, baseline_scores)\n",
    "        baseline_gini = 2 * baseline_auc - 1\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"BASELINE COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Baseline Gini (risk_score only): {baseline_gini:.4f}\")\n",
    "        \n",
    "        if pipeline.models_summary_ is not None and not pipeline.models_summary_.empty:\n",
    "            best_gini = pipeline.models_summary_['gini_oot'].max()\n",
    "            improvement = best_gini - baseline_gini\n",
    "            print(f\"Best Pipeline Gini: {best_gini:.4f}\")\n",
    "            print(f\"Improvement: {improvement:+.4f} ({improvement/baseline_gini*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDual Pipeline Test Complete!\")\n",
    "print(\"Check the outputs_dual folder for detailed reports.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}