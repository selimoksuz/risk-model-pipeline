{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Risk Model Pipeline Test\n",
    "## Full Functionality Test with GitHub Package Installation\n",
    "\n",
    "This notebook:\n",
    "1. Installs the package directly from GitHub (development branch)\n",
    "2. Creates synthetic test data\n",
    "3. Tests ALL pipeline functionalities\n",
    "4. Validates outputs and generates comprehensive reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Package from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install package directly from GitHub development branch\n",
    "!pip install --upgrade git+https://github.com/selimoksuz/risk-model-pipeline.git@development\n",
    "\n",
    "# Verify installation\n",
    "import risk_pipeline\n",
    "print(f\"‚úÖ Package installed successfully!\")\n",
    "print(f\"Version info: {risk_pipeline.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import All Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "# Import all modules from risk_pipeline\n",
    "from risk_pipeline.core.config import Config\n",
    "from risk_pipeline.core.data_processor import DataProcessor\n",
    "from risk_pipeline.core.splitter import DataSplitter\n",
    "from risk_pipeline.core.feature_engineer import FeatureEngineer\n",
    "from risk_pipeline.core.feature_selector import FeatureSelector\n",
    "from risk_pipeline.core.woe_transformer import WOETransformer\n",
    "from risk_pipeline.core.model_builder import ModelBuilder\n",
    "from risk_pipeline.core.evaluator import Evaluator\n",
    "from risk_pipeline.core.reporter import Reporter\n",
    "from risk_pipeline.core.psi_calculator import PSICalculator\n",
    "from risk_pipeline.core.calibration_analyzer import CalibrationAnalyzer\n",
    "from risk_pipeline.core.risk_band_optimizer import RiskBandOptimizer\n",
    "from risk_pipeline.core.pipeline import RiskPipeline\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully!\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Synthetic Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Create main dataset\n",
    "n_samples = 10000\n",
    "n_features = 30\n",
    "\n",
    "# Generate classification data\n",
    "X, y = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=20,\n",
    "    n_redundant=5,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=3,\n",
    "    weights=[0.85, 0.15],  # Imbalanced (15% positive rate)\n",
    "    flip_y=0.02,  # Add 2% label noise\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = [f'feature_{i:02d}' for i in range(n_features)]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# Add some categorical features\n",
    "df['category_1'] = np.random.choice(['A', 'B', 'C', 'D'], size=n_samples)\n",
    "df['category_2'] = np.random.choice(['Low', 'Medium', 'High'], size=n_samples)\n",
    "df['region'] = np.random.choice(['North', 'South', 'East', 'West', 'Central'], size=n_samples)\n",
    "\n",
    "# Add some missing values\n",
    "missing_features = np.random.choice(feature_names[:10], 5, replace=False)\n",
    "for feat in missing_features:\n",
    "    missing_idx = np.random.choice(n_samples, int(n_samples * 0.05), replace=False)\n",
    "    df.loc[missing_idx, feat] = np.nan\n",
    "\n",
    "# Add ID column\n",
    "df['customer_id'] = [f'CUST_{i:06d}' for i in range(n_samples)]\n",
    "\n",
    "# Reorder columns\n",
    "df = df[['customer_id'] + feature_names + ['category_1', 'category_2', 'region', 'target']]\n",
    "\n",
    "print(f\"‚úÖ Synthetic dataset created!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"Target rate: {df['target'].mean():.2%}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Configuration Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Config class\n",
    "config = Config(\n",
    "    target_column='target',\n",
    "    id_column='customer_id',\n",
    "    test_size=0.2,\n",
    "    validation_size=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    cv_folds=5,\n",
    "    \n",
    "    # Feature engineering settings\n",
    "    create_polynomial=True,\n",
    "    polynomial_degree=2,\n",
    "    create_interactions=True,\n",
    "    \n",
    "    # Feature selection\n",
    "    selection_method='importance',\n",
    "    top_k_features=20,\n",
    "    \n",
    "    # WOE settings\n",
    "    max_bins=5,\n",
    "    min_samples_leaf=0.05,\n",
    "    \n",
    "    # Model settings\n",
    "    scoring_metric='roc_auc',\n",
    "    \n",
    "    # Output\n",
    "    output_folder='test_outputs',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Configuration created successfully!\")\n",
    "print(f\"\\nKey settings:\")\n",
    "print(f\"  Target column: {config.target_column}\")\n",
    "print(f\"  Test size: {config.test_size}\")\n",
    "print(f\"  Validation size: {config.validation_size}\")\n",
    "print(f\"  CV folds: {config.cv_folds}\")\n",
    "print(f\"  Output folder: {config.output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data processor\n",
    "processor = DataProcessor(config)\n",
    "\n",
    "# Process data\n",
    "df_processed = processor.validate_and_freeze(df.copy())\n",
    "\n",
    "print(\"‚úÖ Data processing completed!\")\n",
    "print(f\"Processed shape: {df_processed.shape}\")\n",
    "print(f\"\\nData types after processing:\")\n",
    "print(df_processed.dtypes.value_counts())\n",
    "\n",
    "# Check for any remaining issues\n",
    "if df_processed.isnull().sum().sum() > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Still have {df_processed.isnull().sum().sum()} missing values\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No missing values after processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Data Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize splitter\n",
    "splitter = DataSplitter(config)\n",
    "\n",
    "# Split data\n",
    "splits = splitter.split(df_processed)\n",
    "\n",
    "print(\"‚úÖ Data splitting completed!\")\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"  Train: {len(splits['train'])} samples ({len(splits['train'])/len(df_processed):.1%})\")\n",
    "print(f\"  Validation: {len(splits['validation'])} samples ({len(splits['validation'])/len(df_processed):.1%})\")\n",
    "print(f\"  Test: {len(splits['test'])} samples ({len(splits['test'])/len(df_processed):.1%})\")\n",
    "\n",
    "print(f\"\\nTarget rates:\")\n",
    "print(f\"  Train: {splits['train']['target'].mean():.2%}\")\n",
    "print(f\"  Validation: {splits['validation']['target'].mean():.2%}\")\n",
    "print(f\"  Test: {splits['test']['target'].mean():.2%}\")\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = splits['train'].drop(columns=['target', 'customer_id'])\n",
    "y_train = splits['train']['target']\n",
    "X_val = splits['validation'].drop(columns=['target', 'customer_id'])\n",
    "y_val = splits['validation']['target']\n",
    "X_test = splits['test'].drop(columns=['target', 'customer_id'])\n",
    "y_test = splits['test']['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Feature Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "engineer = FeatureEngineer(config)\n",
    "\n",
    "# Create features\n",
    "X_train_eng = engineer.create_features(X_train)\n",
    "X_val_eng = engineer.transform(X_val)\n",
    "X_test_eng = engineer.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Feature engineering completed!\")\n",
    "print(f\"\\nFeature counts:\")\n",
    "print(f\"  Original features: {X_train.shape[1]}\")\n",
    "print(f\"  After engineering: {X_train_eng.shape[1]}\")\n",
    "print(f\"  New features created: {X_train_eng.shape[1] - X_train.shape[1]}\")\n",
    "\n",
    "# Show sample of new features\n",
    "new_features = [col for col in X_train_eng.columns if col not in X_train.columns]\n",
    "if new_features:\n",
    "    print(f\"\\nSample new features: {new_features[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Feature Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature selector\n",
    "selector = FeatureSelector(config)\n",
    "\n",
    "# Select features\n",
    "selected_features = selector.select_features(X_train_eng, y_train)\n",
    "\n",
    "print(\"‚úÖ Feature selection completed!\")\n",
    "print(f\"\\nSelected {len(selected_features)} features from {X_train_eng.shape[1]}\")\n",
    "\n",
    "# Apply selection\n",
    "X_train_selected = X_train_eng[selected_features]\n",
    "X_val_selected = X_val_eng[selected_features]\n",
    "X_test_selected = X_test_eng[selected_features]\n",
    "\n",
    "# Show top features\n",
    "if hasattr(selector, 'feature_importance_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': selected_features[:10],\n",
    "        'importance': selector.feature_importance_[:10]\n",
    "    })\n",
    "    print(\"\\nTop 10 features by importance:\")\n",
    "    print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test WOE Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WOE transformer\n",
    "woe_transformer = WOETransformer(config)\n",
    "\n",
    "# Fit and transform\n",
    "X_train_woe = woe_transformer.fit_transform(X_train_selected, y_train)\n",
    "X_val_woe = woe_transformer.transform(X_val_selected)\n",
    "X_test_woe = woe_transformer.transform(X_test_selected)\n",
    "\n",
    "print(\"‚úÖ WOE transformation completed!\")\n",
    "print(f\"\\nTransformed data shape: {X_train_woe.shape}\")\n",
    "\n",
    "# Show WOE mapping for a sample variable\n",
    "if woe_transformer.woe_mapping_:\n",
    "    sample_var = list(woe_transformer.woe_mapping_.keys())[0]\n",
    "    print(f\"\\nWOE mapping for '{sample_var}':\")\n",
    "    print(woe_transformer.woe_mapping_[sample_var])\n",
    "    \n",
    "# Show IV values\n",
    "if hasattr(woe_transformer, 'iv_values_'):\n",
    "    iv_df = pd.DataFrame({\n",
    "        'feature': list(woe_transformer.iv_values_.keys())[:10],\n",
    "        'IV': list(woe_transformer.iv_values_.values())[:10]\n",
    "    }).sort_values('IV', ascending=False)\n",
    "    print(\"\\nTop 10 features by Information Value:\")\n",
    "    print(iv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model builder\n",
    "model_builder = ModelBuilder(config)\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'logistic_regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'random_forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=RANDOM_STATE),\n",
    "    'xgboost': xgb.XGBClassifier(n_estimators=100, max_depth=5, random_state=RANDOM_STATE, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Train models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    trained_model = model_builder.train(model, X_train_woe, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = model_builder.evaluate(trained_model, X_train_woe, y_train)\n",
    "    val_score = model_builder.evaluate(trained_model, X_val_woe, y_val)\n",
    "    test_score = model_builder.evaluate(trained_model, X_test_woe, y_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': trained_model,\n",
    "        'train_score': train_score,\n",
    "        'val_score': val_score,\n",
    "        'test_score': test_score\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train AUC: {train_score:.4f}\")\n",
    "    print(f\"  Val AUC: {val_score:.4f}\")\n",
    "    print(f\"  Test AUC: {test_score:.4f}\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['val_score'])\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f\"\\n‚úÖ Best model: {best_model_name} (Val AUC: {results[best_model_name]['val_score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = Evaluator(config)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_proba = best_model.predict_proba(X_test_woe)[:, 1]\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "metrics = evaluator.evaluate_model(y_test, y_pred, y_pred_proba)\n",
    "\n",
    "print(\"‚úÖ Model evaluation completed!\")\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Generate plots\n",
    "fig = evaluator.plot_performance(y_test, y_pred_proba)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Performance plots generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test PSI Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PSI calculator\n",
    "psi_calculator = PSICalculator()\n",
    "\n",
    "# Calculate score PSI\n",
    "y_train_pred = best_model.predict_proba(X_train_woe)[:, 1]\n",
    "score_psi = psi_calculator.calculate(y_train_pred, y_pred_proba)\n",
    "\n",
    "print(\"‚úÖ PSI Analysis completed!\")\n",
    "print(f\"\\nScore PSI (Train vs Test): {score_psi:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if score_psi < 0.1:\n",
    "    print(\"  ‚úÖ Model is stable (PSI < 0.1)\")\n",
    "elif score_psi < 0.25:\n",
    "    print(\"  ‚ö†Ô∏è Minor shift detected (0.1 <= PSI < 0.25)\")\n",
    "else:\n",
    "    print(\"  ‚ùå Significant shift detected (PSI >= 0.25)\")\n",
    "\n",
    "# Feature PSI\n",
    "print(\"\\nFeature PSI (Top 5 features):\")\n",
    "for col in selected_features[:5]:\n",
    "    feature_psi = psi_calculator.calculate(X_train_woe[col], X_test_woe[col])\n",
    "    status = \"‚úÖ\" if feature_psi < 0.1 else \"‚ö†Ô∏è\" if feature_psi < 0.25 else \"‚ùå\"\n",
    "    print(f\"  {col}: {feature_psi:.4f} {status}\")\n",
    "\n",
    "# Segment PSI\n",
    "segment_psi = psi_calculator.calculate_segment_psi(\n",
    "    X_train_woe, X_test_woe, \n",
    "    y_train_pred, y_pred_proba,\n",
    "    segment_column=None  # Will use score-based segments\n",
    ")\n",
    "print(\"\\nSegment-based PSI:\")\n",
    "print(segment_psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test Calibration Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize calibration analyzer\n",
    "calibration_analyzer = CalibrationAnalyzer()\n",
    "\n",
    "# Analyze calibration\n",
    "cal_results = calibration_analyzer.analyze_calibration(y_test, y_pred_proba)\n",
    "\n",
    "print(\"‚úÖ Calibration analysis completed!\")\n",
    "print(\"\\nCalibration Metrics:\")\n",
    "print(f\"  Expected Calibration Error (ECE): {cal_results['ece']:.4f}\")\n",
    "print(f\"  Maximum Calibration Error (MCE): {cal_results['mce']:.4f}\")\n",
    "print(f\"  Brier Score: {cal_results['brier_score']:.4f}\")\n",
    "\n",
    "# Statistical tests\n",
    "if 'hosmer_lemeshow' in cal_results:\n",
    "    print(f\"\\nHosmer-Lemeshow Test:\")\n",
    "    print(f\"  Statistic: {cal_results['hosmer_lemeshow']['statistic']:.4f}\")\n",
    "    print(f\"  P-value: {cal_results['hosmer_lemeshow']['p_value']:.4f}\")\n",
    "    if cal_results['hosmer_lemeshow']['p_value'] > 0.05:\n",
    "        print(\"  ‚úÖ Model is well calibrated (p > 0.05)\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è Calibration issues detected (p <= 0.05)\")\n",
    "\n",
    "# Plot calibration\n",
    "fig = calibration_analyzer.plot_calibration(y_test, y_pred_proba)\n",
    "plt.show()\n",
    "\n",
    "# Calibration by bins\n",
    "print(\"\\nCalibration by bins:\")\n",
    "print(cal_results['bins'][['bin', 'count', 'mean_predicted', 'mean_actual', 'calibration_error']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Test Risk Band Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize risk band optimizer\n",
    "risk_band_optimizer = RiskBandOptimizer()\n",
    "\n",
    "# Create risk bands\n",
    "risk_bands = risk_band_optimizer.optimize_bands(\n",
    "    y_true=y_test,\n",
    "    y_scores=y_pred_proba,\n",
    "    n_bands=5,\n",
    "    method='quantile'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Risk band optimization completed!\")\n",
    "print(\"\\nRisk Bands:\")\n",
    "print(risk_bands[['band', 'min_score', 'max_score', 'count', 'bad_rate', 'volume_pct', 'cumulative_bad_rate']])\n",
    "\n",
    "# Check monotonicity\n",
    "is_monotonic = all(risk_bands['bad_rate'].iloc[i] <= risk_bands['bad_rate'].iloc[i+1] \n",
    "                   for i in range(len(risk_bands)-1))\n",
    "print(f\"\\n{'‚úÖ' if is_monotonic else '‚ùå'} Risk bands are {'monotonic' if is_monotonic else 'not monotonic'}\")\n",
    "\n",
    "# Plot risk bands\n",
    "fig = risk_band_optimizer.plot_bands(risk_bands)\n",
    "plt.show()\n",
    "\n",
    "# Test different methods\n",
    "print(\"\\nTesting different binning methods:\")\n",
    "for method in ['quantile', 'equal_width', 'kmeans']:\n",
    "    bands = risk_band_optimizer.optimize_bands(y_test, y_pred_proba, n_bands=5, method=method)\n",
    "    gini = risk_band_optimizer.calculate_gini(bands)\n",
    "    print(f\"  {method}: Gini = {gini:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Test Reporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize reporter\n",
    "reporter = Reporter(config)\n",
    "\n",
    "# Prepare report data\n",
    "report_data = {\n",
    "    'model_name': best_model_name,\n",
    "    'model': best_model,\n",
    "    'metrics': metrics,\n",
    "    'feature_importance': pd.DataFrame({\n",
    "        'feature': selected_features[:20],\n",
    "        'importance': np.random.random(20)  # Placeholder\n",
    "    }),\n",
    "    'psi_results': {\n",
    "        'score_psi': score_psi,\n",
    "        'segment_psi': segment_psi\n",
    "    },\n",
    "    'calibration_results': cal_results,\n",
    "    'risk_bands': risk_bands,\n",
    "    'X_test': X_test_woe,\n",
    "    'y_test': y_test,\n",
    "    'y_pred': y_pred_proba\n",
    "}\n",
    "\n",
    "# Generate report\n",
    "report_path = reporter.generate_report(report_data)\n",
    "print(f\"‚úÖ Report generated: {report_path}\")\n",
    "\n",
    "# Generate summary\n",
    "summary = reporter.generate_summary(report_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Test Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fresh dataset for pipeline test\n",
    "X_pipeline, y_pipeline = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=25,\n",
    "    n_informative=18,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    weights=[0.8, 0.2],\n",
    "    random_state=RANDOM_STATE+1\n",
    ")\n",
    "\n",
    "df_pipeline = pd.DataFrame(X_pipeline, columns=[f'var_{i:02d}' for i in range(X_pipeline.shape[1])])\n",
    "df_pipeline['target'] = y_pipeline\n",
    "\n",
    "# Initialize complete pipeline\n",
    "pipeline = RiskPipeline(config)\n",
    "\n",
    "print(\"Testing complete pipeline...\\n\")\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(df_pipeline)\n",
    "\n",
    "# Get predictions\n",
    "predictions = pipeline.predict(df_pipeline)\n",
    "probabilities = pipeline.predict_proba(df_pipeline)\n",
    "\n",
    "print(\"\\n‚úÖ Complete pipeline test successful!\")\n",
    "print(f\"\\nPipeline components:\")\n",
    "if hasattr(pipeline, 'components_'):\n",
    "    for comp_name in pipeline.components_:\n",
    "        print(f\"  - {comp_name}\")\n",
    "\n",
    "# Evaluate pipeline\n",
    "pipeline_score = roc_auc_score(y_pipeline, probabilities[:, 1])\n",
    "print(f\"\\nPipeline AUC Score: {pipeline_score:.4f}\")\n",
    "\n",
    "# Save pipeline\n",
    "pipeline_path = os.path.join(config.output_folder, 'complete_pipeline.pkl')\n",
    "joblib.dump(pipeline, pipeline_path)\n",
    "print(f\"\\n‚úÖ Pipeline saved to: {pipeline_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Test Model Deployment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_new_data(data, pipeline_path):\n",
    "    \"\"\"\n",
    "    Score new data using saved pipeline\n",
    "    \"\"\"\n",
    "    # Load pipeline\n",
    "    pipeline = joblib.load(pipeline_path)\n",
    "    \n",
    "    # Score\n",
    "    scores = pipeline.predict_proba(data)[:, 1]\n",
    "    predictions = pipeline.predict(data)\n",
    "    \n",
    "    # Create results\n",
    "    results = pd.DataFrame({\n",
    "        'score': scores,\n",
    "        'prediction': predictions,\n",
    "        'risk_level': pd.cut(scores, bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                            labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test scoring function\n",
    "new_data = df_pipeline.sample(10, random_state=RANDOM_STATE)\n",
    "scoring_results = score_new_data(new_data, pipeline_path)\n",
    "\n",
    "print(\"‚úÖ Scoring function test completed!\")\n",
    "print(\"\\nSample scoring results:\")\n",
    "print(scoring_results)\n",
    "\n",
    "# Validate scores\n",
    "direct_scores = pipeline.predict_proba(new_data)[:, 1]\n",
    "assert np.allclose(scoring_results['score'].values, direct_scores), \"Score mismatch!\"\n",
    "print(\"\\n‚úÖ Score validation passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark different sample sizes\n",
    "sample_sizes = [1000, 5000, 10000]\n",
    "benchmark_results = []\n",
    "\n",
    "for n_samples in sample_sizes:\n",
    "    # Create data\n",
    "    X_bench, y_bench = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=20,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    df_bench = pd.DataFrame(X_bench, columns=[f'f_{i}' for i in range(X_bench.shape[1])])\n",
    "    df_bench['target'] = y_bench\n",
    "    \n",
    "    # Time pipeline\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pipeline_bench = RiskPipeline(config)\n",
    "    pipeline_bench.fit(df_bench)\n",
    "    _ = pipeline_bench.predict_proba(df_bench)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    benchmark_results.append({\n",
    "        'n_samples': n_samples,\n",
    "        'time_seconds': elapsed_time,\n",
    "        'samples_per_second': n_samples / elapsed_time\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ {n_samples:,} samples: {elapsed_time:.2f} seconds ({n_samples/elapsed_time:.0f} samples/sec)\")\n",
    "\n",
    "# Display results\n",
    "benchmark_df = pd.DataFrame(benchmark_results)\n",
    "print(\"\\nBenchmark Summary:\")\n",
    "print(benchmark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Error Handling and Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing error handling and edge cases...\\n\")\n",
    "\n",
    "# Test 1: Empty DataFrame\n",
    "try:\n",
    "    empty_df = pd.DataFrame()\n",
    "    pipeline.fit(empty_df)\n",
    "    print(\"‚ùå Should have raised error for empty DataFrame\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úÖ Empty DataFrame handled: {type(e).__name__}\")\n",
    "\n",
    "# Test 2: Missing target column\n",
    "try:\n",
    "    no_target_df = df_pipeline.drop(columns=['target'])\n",
    "    pipeline.fit(no_target_df)\n",
    "    print(\"‚ùå Should have raised error for missing target\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úÖ Missing target handled: {type(e).__name__}\")\n",
    "\n",
    "# Test 3: All missing values\n",
    "try:\n",
    "    all_nan_df = df_pipeline.copy()\n",
    "    all_nan_df.iloc[:, :-1] = np.nan\n",
    "    test_pipeline = RiskPipeline(config)\n",
    "    test_pipeline.fit(all_nan_df)\n",
    "    print(\"‚úÖ All NaN values handled successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è All NaN handling: {type(e).__name__}\")\n",
    "\n",
    "# Test 4: Single class target\n",
    "try:\n",
    "    single_class_df = df_pipeline.copy()\n",
    "    single_class_df['target'] = 0\n",
    "    test_pipeline = RiskPipeline(config)\n",
    "    test_pipeline.fit(single_class_df)\n",
    "    print(\"‚ùå Should have raised error for single class\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úÖ Single class handled: {type(e).__name__}\")\n",
    "\n",
    "# Test 5: Extreme imbalance\n",
    "try:\n",
    "    imbalanced_df = df_pipeline.copy()\n",
    "    imbalanced_df.loc[imbalanced_df.index[:-10], 'target'] = 0\n",
    "    imbalanced_df.loc[imbalanced_df.index[-10:], 'target'] = 1\n",
    "    test_pipeline = RiskPipeline(config)\n",
    "    test_pipeline.fit(imbalanced_df)\n",
    "    print(f\"‚úÖ Extreme imbalance handled (target rate: {imbalanced_df['target'].mean():.1%})\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Extreme imbalance: {type(e).__name__}\")\n",
    "\n",
    "print(\"\\n‚úÖ Error handling tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Final Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPLETE PIPELINE TEST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüì¶ Package: risk-model-pipeline (development branch)\")\n",
    "print(f\"‚è∞ Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\n‚úÖ MODULES TESTED:\")\n",
    "tested_modules = [\n",
    "    \"Config\",\n",
    "    \"DataProcessor\",\n",
    "    \"DataSplitter\",\n",
    "    \"FeatureEngineer\",\n",
    "    \"FeatureSelector\",\n",
    "    \"WOETransformer\",\n",
    "    \"ModelBuilder\",\n",
    "    \"Evaluator\",\n",
    "    \"PSICalculator\",\n",
    "    \"CalibrationAnalyzer\",\n",
    "    \"RiskBandOptimizer\",\n",
    "    \"Reporter\",\n",
    "    \"RiskPipeline\"\n",
    "]\n",
    "for module in tested_modules:\n",
    "    print(f\"  ‚úì {module}\")\n",
    "\n",
    "print(\"\\nüìä TEST RESULTS:\")\n",
    "print(f\"  Best Model: {best_model_name}\")\n",
    "print(f\"  Test AUC: {results[best_model_name]['test_score']:.4f}\")\n",
    "print(f\"  PSI: {score_psi:.4f}\")\n",
    "print(f\"  ECE: {cal_results['ece']:.4f}\")\n",
    "print(f\"  Risk Bands: {len(risk_bands)} bands\")\n",
    "\n",
    "print(\"\\nüíæ ARTIFACTS CREATED:\")\n",
    "if os.path.exists(config.output_folder):\n",
    "    files = os.listdir(config.output_folder)\n",
    "    for file in files[:5]:  # Show first 5 files\n",
    "        print(f\"  - {file}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"  ... and {len(files)-5} more files\")\n",
    "\n",
    "print(\"\\nüéØ PERFORMANCE:\")\n",
    "print(f\"  Average processing speed: {benchmark_df['samples_per_second'].mean():.0f} samples/sec\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ ALL TESTS PASSED SUCCESSFULLY! üéâ\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nThe risk-model-pipeline package is fully functional and ready for use!\")\n",
    "print(\"Install with: pip install git+https://github.com/selimoksuz/risk-model-pipeline.git@development\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}