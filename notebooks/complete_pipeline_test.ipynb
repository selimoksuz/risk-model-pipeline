{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Risk Model Pipeline Test\n",
    "## Full Functionality Test with GitHub Package Installation\n",
    "\n",
    "This notebook:\n",
    "1. Installs the package directly from GitHub (development branch)\n",
    "2. Creates synthetic test data\n",
    "3. Tests ALL pipeline functionalities\n",
    "4. Validates outputs and generates comprehensive reports\n",
    "\n",
    "**Instructions:**\n",
    "- Run cells sequentially from top to bottom\n",
    "- Restart kernel if you encounter import errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Package from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install package directly from GitHub development branch\n",
    "!pip install --upgrade git+https://github.com/selimoksuz/risk-model-pipeline.git@development\n",
    "\n",
    "# Verify installation\n",
    "import risk_pipeline\n",
    "print(f\"‚úÖ Package installed successfully!\")\n",
    "print(f\"Package location: {risk_pipeline.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, classification_report,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, precision_recall_curve\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Try importing XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGBOOST = True\n",
    "except ImportError:\n",
    "    HAS_XGBOOST = False\n",
    "    print(\"‚ö†Ô∏è XGBoost not installed\")\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ Standard libraries imported successfully!\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Risk Pipeline Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all modules from risk_pipeline with error handling\n",
    "modules_status = {}\n",
    "\n",
    "# Core modules\n",
    "try:\n",
    "    from risk_pipeline.core.config import Config\n",
    "    modules_status['Config'] = '‚úÖ'\n",
    "except ImportError as e:\n",
    "    modules_status['Config'] = f'‚ùå {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.data_processor import DataProcessor\n",
    "    modules_status['DataProcessor'] = '‚úÖ'\n",
    "except ImportError as e:\n",
    "    modules_status['DataProcessor'] = f'‚ùå {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.splitter import DataSplitter\n",
    "    modules_status['DataSplitter'] = '‚úÖ'\n",
    "except ImportError as e:\n",
    "    modules_status['DataSplitter'] = f'‚ùå {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.feature_engineer import FeatureEngineer\n",
    "    modules_status['FeatureEngineer'] = '‚úÖ'\n",
    "except ImportError as e:\n",
    "    modules_status['FeatureEngineer'] = f'‚ùå {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.feature_selector import FeatureSelector\n",
    "    modules_status['FeatureSelector'] = '‚úÖ'\n",
    "except ImportError as e:\n",
    "    modules_status['FeatureSelector'] = f'‚ùå {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.woe_transformer import WOETransformer\n",
    "    modules_status['WOETransformer'] = '‚úÖ'\n",
    "except ImportError as e:\n",
    "    modules_status['WOETransformer'] = f'‚ùå {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.model_builder import ModelBuilder\n",
    "    modules_status['ModelBuilder'] = '‚úÖ'\n",
    "except ImportError as e:\n",
    "    modules_status['ModelBuilder'] = f'‚ùå {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.model_trainer import ModelTrainer\n",
    "    modules_status['ModelTrainer'] = '‚úÖ'\n",
    "except ImportError as e:\n",
    "    modules_status['ModelTrainer'] = f'‚ùå {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.reporter import Reporter\n",
    "    modules_status['Reporter'] = '‚úÖ'\n",
    "except ImportError as e:\n",
    "    modules_status['Reporter'] = f'‚ùå {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.report_generator import ReportGenerator\n",
    "    modules_status['ReportGenerator'] = '‚úÖ'\n",
    "except ImportError as e:\n",
    "    modules_status['ReportGenerator'] = f'‚ùå {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.psi_calculator import PSICalculator\n",
    "    modules_status['PSICalculator'] = '‚úÖ'\n",
    "except ImportError as e:\n",
    "    modules_status['PSICalculator'] = f'‚ùå {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.calibration_analyzer import CalibrationAnalyzer\n",
    "    modules_status['CalibrationAnalyzer'] = '‚úÖ'\n",
    "except ImportError as e:\n",
    "    modules_status['CalibrationAnalyzer'] = f'‚ùå {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.risk_band_optimizer import RiskBandOptimizer\n",
    "    modules_status['RiskBandOptimizer'] = '‚úÖ'\n",
    "except ImportError as e:\n",
    "    modules_status['RiskBandOptimizer'] = f'‚ùå {e}'\n",
    "\n",
    "# Pipeline classes\n",
    "PIPELINE_CLASS = None\n",
    "try:\n",
    "    from risk_pipeline.pipeline import RiskModelPipeline\n",
    "    PIPELINE_CLASS = RiskModelPipeline\n",
    "    modules_status['RiskModelPipeline'] = '‚úÖ'\n",
    "except ImportError:\n",
    "    try:\n",
    "        from risk_pipeline.complete_pipeline import CompletePipeline\n",
    "        PIPELINE_CLASS = CompletePipeline\n",
    "        modules_status['CompletePipeline'] = '‚úÖ'\n",
    "    except ImportError:\n",
    "        modules_status['Pipeline'] = '‚ùå No pipeline class available'\n",
    "\n",
    "# Display import status\n",
    "print(\"Module Import Status:\")\n",
    "print(\"=\"*40)\n",
    "for module, status in modules_status.items():\n",
    "    print(f\"{module}: {status}\")\n",
    "\n",
    "# Count successful imports\n",
    "success_count = sum(1 for s in modules_status.values() if '‚úÖ' in str(s))\n",
    "print(f\"\\nSuccessfully imported: {success_count}/{len(modules_status)} modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Synthetic Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset\n",
    "n_samples = 10000\n",
    "n_features = 30\n",
    "\n",
    "# Generate classification data\n",
    "X, y = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=20,\n",
    "    n_redundant=5,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=3,\n",
    "    weights=[0.85, 0.15],  # Imbalanced (15% positive rate)\n",
    "    flip_y=0.02,  # Add 2% label noise\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = [f'feature_{i:02d}' for i in range(n_features)]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# Add categorical features\n",
    "df['category_1'] = np.random.choice(['A', 'B', 'C', 'D'], size=n_samples)\n",
    "df['category_2'] = np.random.choice(['Low', 'Medium', 'High'], size=n_samples, p=[0.5, 0.3, 0.2])\n",
    "df['region'] = np.random.choice(['North', 'South', 'East', 'West', 'Central'], size=n_samples)\n",
    "\n",
    "# Add some missing values\n",
    "missing_features = np.random.choice(feature_names[:10], 5, replace=False)\n",
    "for feat in missing_features:\n",
    "    missing_idx = np.random.choice(n_samples, int(n_samples * 0.05), replace=False)\n",
    "    df.loc[missing_idx, feat] = np.nan\n",
    "\n",
    "# Add ID column\n",
    "df['customer_id'] = [f'CUST_{i:06d}' for i in range(n_samples)]\n",
    "\n",
    "# Reorder columns\n",
    "df = df[['customer_id'] + feature_names + ['category_1', 'category_2', 'region', 'target']]\n",
    "\n",
    "print(f\"‚úÖ Synthetic dataset created!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"Target rate: {df['target'].mean():.2%}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "missing_summary = df.isnull().sum()\n",
    "print(missing_summary[missing_summary > 0])\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration\n",
    "config = Config(\n",
    "    target_column='target',\n",
    "    id_column='customer_id',\n",
    "    test_size=0.2,\n",
    "    validation_size=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    cv_folds=5,\n",
    "    \n",
    "    # Feature engineering\n",
    "    create_polynomial=False,  # Disable for faster testing\n",
    "    polynomial_degree=2,\n",
    "    create_interactions=False,\n",
    "    \n",
    "    # Feature selection\n",
    "    selection_method='importance',\n",
    "    top_k_features=20,\n",
    "    \n",
    "    # WOE settings\n",
    "    max_bins=5,\n",
    "    min_samples_leaf=0.05,\n",
    "    \n",
    "    # Model settings\n",
    "    scoring_metric='roc_auc',\n",
    "    \n",
    "    # Output\n",
    "    output_folder='test_outputs',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Configuration created!\")\n",
    "print(f\"\\nKey settings:\")\n",
    "print(f\"  Target: {config.target_column}\")\n",
    "print(f\"  Test size: {config.test_size}\")\n",
    "print(f\"  Validation size: {config.validation_size}\")\n",
    "print(f\"  Top K features: {config.top_k_features}\")\n",
    "print(f\"  Output folder: {config.output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataProcessor\n",
    "if 'DataProcessor' in globals():\n",
    "    processor = DataProcessor(config)\n",
    "    df_processed = processor.validate_and_freeze(df.copy())\n",
    "    \n",
    "    print(\"‚úÖ Data processing completed!\")\n",
    "    print(f\"Processed shape: {df_processed.shape}\")\n",
    "    print(f\"\\nColumns after processing: {df_processed.shape[1]}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_after = df_processed.isnull().sum().sum()\n",
    "    print(f\"Missing values after processing: {missing_after}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è DataProcessor not available\")\n",
    "    df_processed = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataSplitter\n",
    "if 'DataSplitter' in globals():\n",
    "    splitter = DataSplitter(config)\n",
    "    splits = splitter.split(df_processed)\n",
    "    \n",
    "    print(\"‚úÖ Data splitting completed!\")\n",
    "    print(f\"\\nSplit sizes:\")\n",
    "    print(f\"  Train: {len(splits['train'])} ({len(splits['train'])/len(df_processed):.1%})\")\n",
    "    print(f\"  Validation: {len(splits['validation'])} ({len(splits['validation'])/len(df_processed):.1%})\")\n",
    "    print(f\"  Test: {len(splits['test'])} ({len(splits['test'])/len(df_processed):.1%})\")\n",
    "    \n",
    "    print(f\"\\nTarget rates:\")\n",
    "    print(f\"  Train: {splits['train']['target'].mean():.2%}\")\n",
    "    print(f\"  Validation: {splits['validation']['target'].mean():.2%}\")\n",
    "    print(f\"  Test: {splits['test']['target'].mean():.2%}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train = splits['train'].drop(columns=['target', 'customer_id'])\n",
    "    y_train = splits['train']['target']\n",
    "    X_val = splits['validation'].drop(columns=['target', 'customer_id'])\n",
    "    y_val = splits['validation']['target']\n",
    "    X_test = splits['test'].drop(columns=['target', 'customer_id'])\n",
    "    y_test = splits['test']['target']\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è DataSplitter not available, using sklearn\")\n",
    "    # Manual split\n",
    "    X = df_processed.drop(columns=['target', 'customer_id'])\n",
    "    y = df_processed['target']\n",
    "    \n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.125, random_state=RANDOM_STATE, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FeatureEngineer\n",
    "if 'FeatureEngineer' in globals():\n",
    "    engineer = FeatureEngineer(config)\n",
    "    \n",
    "    X_train_eng = engineer.create_features(X_train)\n",
    "    X_val_eng = engineer.transform(X_val)\n",
    "    X_test_eng = engineer.transform(X_test)\n",
    "    \n",
    "    print(\"‚úÖ Feature engineering completed!\")\n",
    "    print(f\"\\nFeature counts:\")\n",
    "    print(f\"  Original: {X_train.shape[1]}\")\n",
    "    print(f\"  After engineering: {X_train_eng.shape[1]}\")\n",
    "    print(f\"  New features: {X_train_eng.shape[1] - X_train.shape[1]}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è FeatureEngineer not available\")\n",
    "    X_train_eng = X_train\n",
    "    X_val_eng = X_val\n",
    "    X_test_eng = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FeatureSelector\n",
    "if 'FeatureSelector' in globals():\n",
    "    selector = FeatureSelector(config)\n",
    "    selected_features = selector.select_features(X_train_eng, y_train)\n",
    "    \n",
    "    print(\"‚úÖ Feature selection completed!\")\n",
    "    print(f\"\\nSelected {len(selected_features)} features from {X_train_eng.shape[1]}\")\n",
    "    \n",
    "    # Apply selection\n",
    "    X_train_selected = X_train_eng[selected_features]\n",
    "    X_val_selected = X_val_eng[selected_features]\n",
    "    X_test_selected = X_test_eng[selected_features]\n",
    "    \n",
    "    print(f\"\\nTop 10 selected features:\")\n",
    "    print(selected_features[:10].tolist() if hasattr(selected_features, 'tolist') else selected_features[:10])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è FeatureSelector not available\")\n",
    "    # Select all features\n",
    "    selected_features = X_train_eng.columns.tolist()\n",
    "    X_train_selected = X_train_eng\n",
    "    X_val_selected = X_val_eng\n",
    "    X_test_selected = X_test_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test WOE Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test WOETransformer\n",
    "if 'WOETransformer' in globals():\n",
    "    woe_transformer = WOETransformer(config)\n",
    "    \n",
    "    X_train_woe = woe_transformer.fit_transform(X_train_selected, y_train)\n",
    "    X_val_woe = woe_transformer.transform(X_val_selected)\n",
    "    X_test_woe = woe_transformer.transform(X_test_selected)\n",
    "    \n",
    "    print(\"‚úÖ WOE transformation completed!\")\n",
    "    print(f\"\\nTransformed shape: {X_train_woe.shape}\")\n",
    "    \n",
    "    # Show sample WOE values\n",
    "    if hasattr(woe_transformer, 'woe_mapping_') and woe_transformer.woe_mapping_:\n",
    "        sample_var = list(woe_transformer.woe_mapping_.keys())[0]\n",
    "        print(f\"\\nSample WOE mapping for '{sample_var}':\")\n",
    "        print(woe_transformer.woe_mapping_[sample_var].head())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WOETransformer not available\")\n",
    "    X_train_woe = X_train_selected\n",
    "    X_val_woe = X_val_selected\n",
    "    X_test_woe = X_test_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "models = {\n",
    "    'logistic_regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'decision_tree': DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=5),\n",
    "    'random_forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "if HAS_XGBOOST:\n",
    "    models['xgboost'] = xgb.XGBClassifier(\n",
    "        n_estimators=100, max_depth=5, \n",
    "        random_state=RANDOM_STATE, eval_metric='logloss'\n",
    "    )\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "best_model = None\n",
    "best_score = 0\n",
    "best_model_name = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_woe, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict_proba(X_train_woe)[:, 1]\n",
    "    y_pred_val = model.predict_proba(X_val_woe)[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test_woe)[:, 1]\n",
    "    \n",
    "    # Calculate scores\n",
    "    train_score = roc_auc_score(y_train, y_pred_train)\n",
    "    val_score = roc_auc_score(y_val, y_pred_val)\n",
    "    test_score = roc_auc_score(y_test, y_pred_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_score': train_score,\n",
    "        'val_score': val_score,\n",
    "        'test_score': test_score,\n",
    "        'y_pred_test': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train AUC: {train_score:.4f}\")\n",
    "    print(f\"  Val AUC: {val_score:.4f}\")\n",
    "    print(f\"  Test AUC: {test_score:.4f}\")\n",
    "    \n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Best Model: {best_model_name} (Val AUC: {best_score:.4f})\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model predictions\n",
    "y_pred_proba = results[best_model_name]['y_pred_test']\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'auc': roc_auc_score(y_test, y_pred_proba),\n",
    "    'gini': 2 * roc_auc_score(y_test, y_pred_proba) - 1,\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'f1': f1_score(y_test, y_pred)\n",
    "}\n",
    "\n",
    "print(\"Performance Metrics:\")\n",
    "print(\"=\"*40)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric:10s}: {value:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Pred 0', 'Pred 1']))\n",
    "\n",
    "# Model comparison\n",
    "print(\"\\nModel Comparison:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train AUC': [r['train_score'] for r in results.values()],\n",
    "    'Val AUC': [r['val_score'] for r in results.values()],\n",
    "    'Test AUC': [r['test_score'] for r in results.values()],\n",
    "    'Overfit': [r['train_score'] - r['test_score'] for r in results.values()]\n",
    "})\n",
    "print(comparison_df.sort_values('Val AUC', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# ROC Curve\n",
    "ax = axes[0, 0]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "ax.plot(fpr, tpr, label=f'AUC = {metrics[\"auc\"]:.3f}')\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Score Distribution\n",
    "ax = axes[0, 1]\n",
    "ax.hist(y_pred_proba[y_test == 0], bins=30, alpha=0.5, label='Class 0', color='blue')\n",
    "ax.hist(y_pred_proba[y_test == 1], bins=30, alpha=0.5, label='Class 1', color='red')\n",
    "ax.set_xlabel('Predicted Probability')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Score Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "ax = axes[1, 0]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Precision-Recall Curve\n",
    "ax = axes[1, 1]\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "ax.plot(recall, precision)\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision-Recall Curve')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Performance plots generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Test PSI Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PSICalculator\n",
    "if 'PSICalculator' in globals():\n",
    "    psi_calculator = PSICalculator()\n",
    "    \n",
    "    # Calculate score PSI\n",
    "    y_train_pred = best_model.predict_proba(X_train_woe)[:, 1]\n",
    "    score_psi = psi_calculator.calculate(y_train_pred, y_pred_proba)\n",
    "    \n",
    "    print(\"‚úÖ PSI Analysis completed!\")\n",
    "    print(f\"\\nScore PSI (Train vs Test): {score_psi:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if score_psi < 0.1:\n",
    "        print(\"  ‚úÖ Model is stable (PSI < 0.1)\")\n",
    "    elif score_psi < 0.25:\n",
    "        print(\"  ‚ö†Ô∏è Minor shift detected (0.1 <= PSI < 0.25)\")\n",
    "    else:\n",
    "        print(\"  ‚ùå Significant shift detected (PSI >= 0.25)\")\n",
    "    \n",
    "    # Feature PSI for top features\n",
    "    print(\"\\nFeature PSI (Top 5 features):\")\n",
    "    for i, col in enumerate(X_train_woe.columns[:5]):\n",
    "        feature_psi = psi_calculator.calculate(X_train_woe[col], X_test_woe[col])\n",
    "        status = \"‚úÖ\" if feature_psi < 0.1 else \"‚ö†Ô∏è\" if feature_psi < 0.25 else \"‚ùå\"\n",
    "        print(f\"  {col}: {feature_psi:.4f} {status}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è PSICalculator not available\")\n",
    "    score_psi = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Test Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CalibrationAnalyzer\n",
    "if 'CalibrationAnalyzer' in globals():\n",
    "    calibration_analyzer = CalibrationAnalyzer()\n",
    "    \n",
    "    # Analyze calibration\n",
    "    cal_results = calibration_analyzer.analyze_calibration(y_test, y_pred_proba)\n",
    "    \n",
    "    print(\"‚úÖ Calibration analysis completed!\")\n",
    "    print(\"\\nCalibration Metrics:\")\n",
    "    print(f\"  Expected Calibration Error (ECE): {cal_results['ece']:.4f}\")\n",
    "    print(f\"  Maximum Calibration Error (MCE): {cal_results['mce']:.4f}\")\n",
    "    print(f\"  Brier Score: {cal_results['brier_score']:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if cal_results['ece'] < 0.05:\n",
    "        print(\"\\n‚úÖ Model is well calibrated (ECE < 0.05)\")\n",
    "    elif cal_results['ece'] < 0.1:\n",
    "        print(\"\\n‚ö†Ô∏è Model has minor calibration issues (0.05 <= ECE < 0.1)\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Model needs calibration (ECE >= 0.1)\")\n",
    "    \n",
    "    # Calibration plot\n",
    "    if hasattr(calibration_analyzer, 'plot_calibration'):\n",
    "        try:\n",
    "            fig = calibration_analyzer.plot_calibration(y_test, y_pred_proba)\n",
    "            plt.show()\n",
    "        except:\n",
    "            print(\"Could not generate calibration plot\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CalibrationAnalyzer not available\")\n",
    "    cal_results = {'ece': 0.0, 'mce': 0.0, 'brier_score': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Test Risk Band Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RiskBandOptimizer\n",
    "if 'RiskBandOptimizer' in globals():\n",
    "    risk_band_optimizer = RiskBandOptimizer()\n",
    "    \n",
    "    # Create risk bands\n",
    "    risk_bands = risk_band_optimizer.optimize_bands(\n",
    "        y_true=y_test,\n",
    "        y_scores=y_pred_proba,\n",
    "        n_bands=5,\n",
    "        method='quantile'\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Risk band optimization completed!\")\n",
    "    print(\"\\nRisk Bands:\")\n",
    "    print(risk_bands[['band', 'min_score', 'max_score', 'count', 'bad_rate', 'volume_pct']])\n",
    "    \n",
    "    # Check monotonicity\n",
    "    is_monotonic = all(risk_bands['bad_rate'].iloc[i] <= risk_bands['bad_rate'].iloc[i+1] \n",
    "                      for i in range(len(risk_bands)-1))\n",
    "    print(f\"\\nRisk bands are {'‚úÖ monotonic' if is_monotonic else '‚ùå not monotonic'}\")\n",
    "    \n",
    "    # Visualize risk bands\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Bad rate by band\n",
    "    ax = axes[0]\n",
    "    ax.bar(risk_bands['band'], risk_bands['bad_rate'], color='coral')\n",
    "    ax.set_xlabel('Risk Band')\n",
    "    ax.set_ylabel('Bad Rate')\n",
    "    ax.set_title('Bad Rate by Risk Band')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Volume distribution\n",
    "    ax = axes[1]\n",
    "    ax.bar(risk_bands['band'], risk_bands['volume_pct'], color='skyblue')\n",
    "    ax.set_xlabel('Risk Band')\n",
    "    ax.set_ylabel('Volume %')\n",
    "    ax.set_title('Volume Distribution')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è RiskBandOptimizer not available\")\n",
    "    risk_bands = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Test Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete pipeline if available\n",
    "if PIPELINE_CLASS:\n",
    "    print(f\"Testing {PIPELINE_CLASS.__name__}...\\n\")\n",
    "    \n",
    "    # Create fresh dataset\n",
    "    X_pipe, y_pipe = make_classification(\n",
    "        n_samples=5000, n_features=25, n_informative=18,\n",
    "        n_redundant=5, n_classes=2, weights=[0.8, 0.2],\n",
    "        random_state=RANDOM_STATE+1\n",
    "    )\n",
    "    \n",
    "    df_pipeline = pd.DataFrame(X_pipe, columns=[f'var_{i:02d}' for i in range(X_pipe.shape[1])])\n",
    "    df_pipeline['target'] = y_pipe\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = PIPELINE_CLASS(config)\n",
    "    \n",
    "    try:\n",
    "        # Fit pipeline\n",
    "        pipeline.fit(df_pipeline)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = pipeline.predict(df_pipeline)\n",
    "        probabilities = pipeline.predict_proba(df_pipeline)\n",
    "        \n",
    "        # Evaluate\n",
    "        pipeline_score = roc_auc_score(y_pipe, probabilities[:, 1])\n",
    "        \n",
    "        print(f\"‚úÖ Pipeline test successful!\")\n",
    "        print(f\"Pipeline AUC: {pipeline_score:.4f}\")\n",
    "        \n",
    "        # Save pipeline\n",
    "        os.makedirs(config.output_folder, exist_ok=True)\n",
    "        pipeline_path = os.path.join(config.output_folder, 'complete_pipeline.pkl')\n",
    "        joblib.dump(pipeline, pipeline_path)\n",
    "        print(f\"Pipeline saved to: {pipeline_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Pipeline test failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No pipeline class available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPLETE PIPELINE TEST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüì¶ Package: risk-model-pipeline (development branch)\")\n",
    "print(f\"‚è∞ Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\n‚úÖ MODULES TESTED:\")\n",
    "for module, status in modules_status.items():\n",
    "    if '‚úÖ' in str(status):\n",
    "        print(f\"  ‚úì {module}\")\n",
    "\n",
    "print(\"\\n‚ùå MODULES NOT AVAILABLE:\")\n",
    "for module, status in modules_status.items():\n",
    "    if '‚ùå' in str(status):\n",
    "        print(f\"  ‚úó {module}\")\n",
    "\n",
    "print(\"\\nüìä BEST MODEL RESULTS:\")\n",
    "print(f\"  Model: {best_model_name}\")\n",
    "print(f\"  Test AUC: {results[best_model_name]['test_score']:.4f}\")\n",
    "print(f\"  Gini: {metrics['gini']:.4f}\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "if 'PSICalculator' in globals():\n",
    "    print(f\"\\nüìà STABILITY METRICS:\")\n",
    "    print(f\"  PSI: {score_psi:.4f}\")\n",
    "\n",
    "if 'CalibrationAnalyzer' in globals():\n",
    "    print(f\"  ECE: {cal_results['ece']:.4f}\")\n",
    "\n",
    "if 'RiskBandOptimizer' in globals() and not risk_bands.empty:\n",
    "    print(f\"  Risk Bands: {len(risk_bands)} bands\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ TEST COMPLETED SUCCESSFULLY! üéâ\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}