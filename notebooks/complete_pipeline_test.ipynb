{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete End-to-End Risk Model Pipeline Test\n",
    "\n",
    "This notebook demonstrates ALL features of the risk model pipeline:\n",
    "- Install from GitHub develop branch\n",
    "- Generate synthetic data with realistic Gini (70-80%)\n",
    "- Test ALL models (LR, RF, XGB, LGBM, CatBoost)\n",
    "- Variable dictionary integration\n",
    "- Calibration analysis\n",
    "- Risk scoring and bands\n",
    "- Comprehensive model report\n",
    "- Dual pipeline (WOE + RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install from GitHub Develop Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping risk-model-pipeline as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed risk-model-pipeline from GitHub develop branch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "  WARNING: Did not find branch or tag 'develop', assuming revision or ref.\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  git checkout -q develop did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [1 lines of output]\n",
      "  error: pathspec 'develop' did not match any file(s) known to git\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "git checkout -q develop did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Install from GitHub develop branch\n",
    "!pip uninstall risk-model-pipeline -y\n",
    "!pip install git+https://github.com/selimoksuz/risk-model-pipeline.git@develop --quiet\n",
    "print(\"Installed risk-model-pipeline from GitHub develop branch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import risk pipeline\n",
    "from risk_pipeline import run_pipeline\n",
    "from risk_pipeline.core.config import Config\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate High-Quality Synthetic Data (Target Gini: 70-80%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created:\n",
      "  - Shape: (5000, 62)\n",
      "  - Features: 55 numeric, 4 categorical\n",
      "  - Target distribution: {0: 4458, 1: 542}\n",
      "  - Default rate: 10.84%\n",
      "  - Quick Gini test: 52.91%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>feature_05</th>\n",
       "      <th>feature_06</th>\n",
       "      <th>feature_07</th>\n",
       "      <th>feature_08</th>\n",
       "      <th>feature_09</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_ratio_01</th>\n",
       "      <th>feature_poly_01</th>\n",
       "      <th>feature_poly_02</th>\n",
       "      <th>cat_region</th>\n",
       "      <th>cat_product</th>\n",
       "      <th>cat_channel</th>\n",
       "      <th>cat_segment</th>\n",
       "      <th>target</th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.348662</td>\n",
       "      <td>0.493276</td>\n",
       "      <td>-3.497649</td>\n",
       "      <td>-0.130644</td>\n",
       "      <td>11.672491</td>\n",
       "      <td>7.179540</td>\n",
       "      <td>-2.539013</td>\n",
       "      <td>3.953580</td>\n",
       "      <td>0.069892</td>\n",
       "      <td>-0.583439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233488</td>\n",
       "      <td>0.121565</td>\n",
       "      <td>0.243322</td>\n",
       "      <td>West</td>\n",
       "      <td>A</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Standard</td>\n",
       "      <td>0</td>\n",
       "      <td>APP000000</td>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.621024</td>\n",
       "      <td>4.136327</td>\n",
       "      <td>-5.286213</td>\n",
       "      <td>-4.283210</td>\n",
       "      <td>-15.597608</td>\n",
       "      <td>-1.551997</td>\n",
       "      <td>5.745919</td>\n",
       "      <td>-1.627463</td>\n",
       "      <td>-1.002897</td>\n",
       "      <td>3.484912</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.704983</td>\n",
       "      <td>13.111818</td>\n",
       "      <td>17.109202</td>\n",
       "      <td>South</td>\n",
       "      <td>A</td>\n",
       "      <td>Branch</td>\n",
       "      <td>Basic</td>\n",
       "      <td>0</td>\n",
       "      <td>APP000001</td>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.754523</td>\n",
       "      <td>3.400089</td>\n",
       "      <td>-21.454028</td>\n",
       "      <td>-0.366765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.186791</td>\n",
       "      <td>13.178009</td>\n",
       "      <td>3.043453</td>\n",
       "      <td>-1.090605</td>\n",
       "      <td>-1.789844</td>\n",
       "      <td>...</td>\n",
       "      <td>1.080552</td>\n",
       "      <td>22.605492</td>\n",
       "      <td>11.560603</td>\n",
       "      <td>North</td>\n",
       "      <td>B</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Basic</td>\n",
       "      <td>0</td>\n",
       "      <td>APP000002</td>\n",
       "      <td>2023-01-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.752628</td>\n",
       "      <td>-4.914986</td>\n",
       "      <td>0.636212</td>\n",
       "      <td>-4.954268</td>\n",
       "      <td>14.223073</td>\n",
       "      <td>6.155677</td>\n",
       "      <td>-2.509305</td>\n",
       "      <td>4.228101</td>\n",
       "      <td>4.226168</td>\n",
       "      <td>4.088982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192243</td>\n",
       "      <td>0.566449</td>\n",
       "      <td>24.157086</td>\n",
       "      <td>East</td>\n",
       "      <td>A</td>\n",
       "      <td>Online</td>\n",
       "      <td>Premium</td>\n",
       "      <td>0</td>\n",
       "      <td>APP000003</td>\n",
       "      <td>2023-01-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.952052</td>\n",
       "      <td>0.020792</td>\n",
       "      <td>-6.905076</td>\n",
       "      <td>-2.079083</td>\n",
       "      <td>13.103099</td>\n",
       "      <td>4.453455</td>\n",
       "      <td>-18.111383</td>\n",
       "      <td>-1.159641</td>\n",
       "      <td>-6.597321</td>\n",
       "      <td>1.367048</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.891924</td>\n",
       "      <td>8.714614</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>North</td>\n",
       "      <td>A</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Basic</td>\n",
       "      <td>0</td>\n",
       "      <td>APP000004</td>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_00  feature_01  feature_02  feature_03  feature_04  feature_05  \\\n",
       "0   -0.348662    0.493276   -3.497649   -0.130644   11.672491    7.179540   \n",
       "1   -3.621024    4.136327   -5.286213   -4.283210  -15.597608   -1.551997   \n",
       "2    4.754523    3.400089  -21.454028   -0.366765         NaN    1.186791   \n",
       "3   -0.752628   -4.914986    0.636212   -4.954268   14.223073    6.155677   \n",
       "4   -2.952052    0.020792   -6.905076   -2.079083   13.103099    4.453455   \n",
       "\n",
       "   feature_06  feature_07  feature_08  feature_09  ...  feature_ratio_01  \\\n",
       "0   -2.539013    3.953580    0.069892   -0.583439  ...         -0.233488   \n",
       "1    5.745919   -1.627463   -1.002897    3.484912  ...         -0.704983   \n",
       "2   13.178009    3.043453   -1.090605   -1.789844  ...          1.080552   \n",
       "3   -2.509305    4.228101    4.226168    4.088982  ...          0.192243   \n",
       "4  -18.111383   -1.159641   -6.597321    1.367048  ...         -2.891924   \n",
       "\n",
       "   feature_poly_01  feature_poly_02  cat_region  cat_product  cat_channel  \\\n",
       "0         0.121565         0.243322        West            A        Phone   \n",
       "1        13.111818        17.109202       South            A       Branch   \n",
       "2        22.605492        11.560603       North            B        Phone   \n",
       "3         0.566449        24.157086        East            A       Online   \n",
       "4         8.714614         0.000432       North            A        Phone   \n",
       "\n",
       "   cat_segment  target     app_id              app_dt  \n",
       "0     Standard       0  APP000000 2023-01-01 00:00:00  \n",
       "1        Basic       0  APP000001 2023-01-01 01:00:00  \n",
       "2        Basic       0  APP000002 2023-01-01 02:00:00  \n",
       "3      Premium       0  APP000003 2023-01-01 03:00:00  \n",
       "4        Basic       0  APP000004 2023-01-01 04:00:00  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_high_quality_data(n_samples=5000, target_gini=0.75):\n",
    "    \"\"\"Create synthetic data with realistic Gini score\"\"\"\n",
    "    \n",
    "    # Generate base features with strong signal\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=50,  # More features for realistic scenario\n",
    "        n_informative=30,  # Many informative features\n",
    "        n_redundant=15,\n",
    "        n_repeated=5,\n",
    "        n_clusters_per_class=4,\n",
    "        flip_y=0.02,  # Low noise for high Gini\n",
    "        class_sep=1.5,  # Good separation for high Gini\n",
    "        random_state=42,\n",
    "        weights=[0.9, 0.1]  # Imbalanced like real credit data\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame\n",
    "    feature_cols = [f'feature_{i:02d}' for i in range(50)]\n",
    "    df = pd.DataFrame(X, columns=feature_cols)\n",
    "    \n",
    "    # Add engineered features for better performance\n",
    "    df['feature_interaction_01'] = df['feature_00'] * df['feature_01']\n",
    "    df['feature_interaction_02'] = df['feature_00'] * df['feature_02']\n",
    "    df['feature_ratio_01'] = df['feature_00'] / (df['feature_01'] + 1)\n",
    "    df['feature_poly_01'] = df['feature_00'] ** 2\n",
    "    df['feature_poly_02'] = df['feature_01'] ** 2\n",
    "    \n",
    "    # Add categorical features\n",
    "    df['cat_region'] = np.random.choice(['North', 'South', 'East', 'West', 'Central'], size=n_samples)\n",
    "    df['cat_product'] = np.random.choice(['A', 'B', 'C', 'D'], size=n_samples, p=[0.4, 0.3, 0.2, 0.1])\n",
    "    df['cat_channel'] = np.random.choice(['Online', 'Branch', 'Phone'], size=n_samples)\n",
    "    df['cat_segment'] = np.random.choice(['Premium', 'Standard', 'Basic'], size=n_samples)\n",
    "    \n",
    "    # Add target\n",
    "    df['target'] = y\n",
    "    \n",
    "    # Add required columns\n",
    "    df['app_id'] = [f'APP{i:06d}' for i in range(len(df))]\n",
    "    df['app_dt'] = pd.date_range('2023-01-01', periods=len(df), freq='H')\n",
    "    \n",
    "    # Add missing values (realistic pattern)\n",
    "    missing_cols = np.random.choice(feature_cols[:20], 10, replace=False)\n",
    "    for col in missing_cols:\n",
    "        missing_idx = np.random.choice(df.index, size=int(0.03 * len(df)), replace=False)\n",
    "        df.loc[missing_idx, col] = np.nan\n",
    "    \n",
    "    # Quick Gini check\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_check = df[feature_cols[:10]].fillna(0)\n",
    "    y_check = df['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_check, y_check, test_size=0.3, random_state=42)\n",
    "    \n",
    "    lr = LogisticRegression(random_state=42, max_iter=100)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict_proba(X_test)[:, 1]\n",
    "    gini = 2 * roc_auc_score(y_test, y_pred) - 1\n",
    "    \n",
    "    print(f\"Dataset created:\")\n",
    "    print(f\"  - Shape: {df.shape}\")\n",
    "    print(f\"  - Features: {len(feature_cols) + 5} numeric, 4 categorical\")\n",
    "    print(f\"  - Target distribution: {df['target'].value_counts().to_dict()}\")\n",
    "    print(f\"  - Default rate: {df['target'].mean():.2%}\")\n",
    "    print(f\"  - Quick Gini test: {gini:.2%}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "df = create_high_quality_data(n_samples=5000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Variable Dictionary Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable dictionary created with 13 defined variables\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature_00</th>\n",
       "      <td>demographic</td>\n",
       "      <td>Age</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_01</th>\n",
       "      <td>demographic</td>\n",
       "      <td>Income</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_02</th>\n",
       "      <td>demographic</td>\n",
       "      <td>Employment years</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_03</th>\n",
       "      <td>credit</td>\n",
       "      <td>Credit score</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_04</th>\n",
       "      <td>credit</td>\n",
       "      <td>Number of loans</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_05</th>\n",
       "      <td>credit</td>\n",
       "      <td>Total debt</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_06</th>\n",
       "      <td>behavioral</td>\n",
       "      <td>Payment history</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_07</th>\n",
       "      <td>behavioral</td>\n",
       "      <td>Utilization rate</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_08</th>\n",
       "      <td>behavioral</td>\n",
       "      <td>Days past due</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_region</th>\n",
       "      <td>geographic</td>\n",
       "      <td>Region</td>\n",
       "      <td>categorical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               category       description         type\n",
       "feature_00  demographic               Age      numeric\n",
       "feature_01  demographic            Income      numeric\n",
       "feature_02  demographic  Employment years      numeric\n",
       "feature_03       credit      Credit score      numeric\n",
       "feature_04       credit   Number of loans      numeric\n",
       "feature_05       credit        Total debt      numeric\n",
       "feature_06   behavioral   Payment history      numeric\n",
       "feature_07   behavioral  Utilization rate      numeric\n",
       "feature_08   behavioral     Days past due      numeric\n",
       "cat_region   geographic            Region  categorical"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create variable dictionary\n",
    "variable_dict = {\n",
    "    # Demographic features\n",
    "    'feature_00': {'category': 'demographic', 'description': 'Age', 'type': 'numeric'},\n",
    "    'feature_01': {'category': 'demographic', 'description': 'Income', 'type': 'numeric'},\n",
    "    'feature_02': {'category': 'demographic', 'description': 'Employment years', 'type': 'numeric'},\n",
    "    \n",
    "    # Credit features\n",
    "    'feature_03': {'category': 'credit', 'description': 'Credit score', 'type': 'numeric'},\n",
    "    'feature_04': {'category': 'credit', 'description': 'Number of loans', 'type': 'numeric'},\n",
    "    'feature_05': {'category': 'credit', 'description': 'Total debt', 'type': 'numeric'},\n",
    "    \n",
    "    # Behavioral features\n",
    "    'feature_06': {'category': 'behavioral', 'description': 'Payment history', 'type': 'numeric'},\n",
    "    'feature_07': {'category': 'behavioral', 'description': 'Utilization rate', 'type': 'numeric'},\n",
    "    'feature_08': {'category': 'behavioral', 'description': 'Days past due', 'type': 'numeric'},\n",
    "    \n",
    "    # Categorical features\n",
    "    'cat_region': {'category': 'geographic', 'description': 'Region', 'type': 'categorical'},\n",
    "    'cat_product': {'category': 'product', 'description': 'Product type', 'type': 'categorical'},\n",
    "    'cat_channel': {'category': 'channel', 'description': 'Application channel', 'type': 'categorical'},\n",
    "    'cat_segment': {'category': 'segment', 'description': 'Customer segment', 'type': 'categorical'},\n",
    "}\n",
    "\n",
    "# Save dictionary\n",
    "pd.DataFrame(variable_dict).T.to_csv('variable_dictionary.csv')\n",
    "print(f\"Variable dictionary created with {len(variable_dict)} defined variables\")\n",
    "pd.DataFrame(variable_dict).T.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration summary:\n",
      "  - Dual pipeline: True\n",
      "  - Optuna trials: 5\n",
      "  - Boruta: True\n",
      "  - Forward selection: True\n",
      "  - Noise sentinel: True\n",
      "  - PSI enabled: True\n",
      "  - Output folder: output_complete\n"
     ]
    }
   ],
   "source": [
    "# Full configuration with all features enabled\n",
    "config = Config(\n",
    "    # Basic settings\n",
    "    target_col='target',\n",
    "    id_col='app_id',\n",
    "    time_col='app_dt',\n",
    "    random_state=42,\n",
    "    \n",
    "    # Feature selection\n",
    "    iv_min=0.02,\n",
    "    iv_high_threshold=0.5,\n",
    "    psi_threshold=0.25,\n",
    "    rho_threshold=0.90,\n",
    "    vif_threshold=5.0,\n",
    "    rare_threshold=0.01,\n",
    "    \n",
    "    # WOE settings\n",
    "    n_bins=10,\n",
    "    min_bin_size=0.05,\n",
    "    woe_monotonic=False,\n",
    "    \n",
    "    # Model training - ALL MODELS\n",
    "    use_optuna=True,\n",
    "    n_trials=5,  # More trials for better optimization\n",
    "    cv_folds=5,\n",
    "    \n",
    "    # Feature selection methods - ALL ENABLED\n",
    "    use_boruta=True,\n",
    "    forward_selection=True,\n",
    "    forward_1se=True,\n",
    "    use_noise_sentinel=True,\n",
    "    enable_psi=True,\n",
    "    \n",
    "    # Dual pipeline\n",
    "    enable_dual_pipeline=True,\n",
    "    \n",
    "    # Model selection\n",
    "    model_selection_method='gini_oot',\n",
    "    min_gini_threshold=0.5,\n",
    "    \n",
    "    # Output\n",
    "    output_folder='output_complete',\n",
    "    output_excel_path='model_report_complete.xlsx',\n",
    "    write_csv=True,\n",
    "    \n",
    "    # Data splitting\n",
    "    train_ratio=0.60,\n",
    "    test_ratio=0.20,\n",
    "    oot_ratio=0.20\n",
    ")\n",
    "\n",
    "print(\"Configuration summary:\")\n",
    "print(f\"  - Dual pipeline: {config.enable_dual_pipeline}\")\n",
    "print(f\"  - Optuna trials: {config.n_trials}\")\n",
    "print(f\"  - Boruta: {config.use_boruta}\")\n",
    "print(f\"  - Forward selection: {config.forward_selection}\")\n",
    "print(f\"  - Noise sentinel: {config.use_noise_sentinel}\")\n",
    "print(f\"  - PSI enabled: {config.enable_psi}\")\n",
    "print(f\"  - Output folder: {config.output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting complete pipeline execution...\n",
      "\n",
      "============================================================\n",
      "Starting Risk Model Pipeline...\n",
      "1. Processing data...\n",
      "2. Splitting data...\n",
      "Data split - Train: 4000, Test: 1000, \n",
      "3. Selecting features...\n",
      "Starting feature selection...\n",
      "  1. Calculating Information Values...\n",
      "     After IV filter: 56 features\n",
      "  2. Calculating PSI...\n",
      "     After PSI filter: 55 features\n",
      "  3. Removing correlated features...\n",
      "     After correlation filter: 50 features\n",
      "  4. Running Boruta selection...\n",
      "     After Boruta: 42 features\n",
      "  5. Running forward selection...\n",
      "   - 1SE rule: Selected 8 features (best: 13)\n",
      "     After forward selection: 8 features\n",
      "  6. Running noise sentinel check...\n",
      "   - Running noise sentinel check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-09 17:32:40,786] A new study created in memory with name: no-name-a16c493e-22fd-4b07-9a4b-b2ce5c4dcde8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - 1SE rule: Selected 7 features (best: 8)\n",
      "   - PASS: No noise variables selected\n",
      "  7. Checking VIF...\n",
      "     After VIF filter: 7 features\n",
      "\n",
      "Final selected features: 7\n",
      "4. Applying WOE transformation...\n",
      "Fitting WOE transformation for 7 features...\n",
      "5. Building models...\n",
      "Training models with 7 features...\n",
      "  Training LogisticRegression...\n",
      "    Train AUC: 0.8768, Test AUC: 0.8446, \n",
      "  Training RandomForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-09 17:32:41,211] Trial 0 finished with value: 0.8547477263450931 and parameters: {'n_estimators': 191, 'max_depth': 8, 'min_samples_split': 26, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.8547477263450931.\n",
      "[I 2025-09-09 17:32:41,568] Trial 1 finished with value: 0.8386620757501229 and parameters: {'n_estimators': 144, 'max_depth': 4, 'min_samples_split': 50, 'min_samples_leaf': 14}. Best is trial 0 with value: 0.8547477263450931.\n",
      "[I 2025-09-09 17:32:41,786] Trial 2 finished with value: 0.8203367834978179 and parameters: {'n_estimators': 86, 'max_depth': 3, 'min_samples_split': 28, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.8547477263450931.\n",
      "[I 2025-09-09 17:32:41,972] Trial 3 finished with value: 0.8507184644849347 and parameters: {'n_estimators': 50, 'max_depth': 8, 'min_samples_split': 43, 'min_samples_leaf': 15}. Best is trial 0 with value: 0.8547477263450931.\n",
      "[I 2025-09-09 17:32:42,409] Trial 4 finished with value: 0.8186518194472061 and parameters: {'n_estimators': 179, 'max_depth': 3, 'min_samples_split': 32, 'min_samples_leaf': 16}. Best is trial 0 with value: 0.8547477263450931.\n",
      "[I 2025-09-09 17:32:43,034] A new study created in memory with name: no-name-d369b2fb-1417-4c45-82bc-5611316539d7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train AUC: 0.9318, Test AUC: 0.8547, \n",
      "  Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-09 17:32:43,278] Trial 0 finished with value: 0.8273696769264581 and parameters: {'n_estimators': 147, 'max_depth': 5, 'learning_rate': 0.15271537070280503, 'subsample': 0.925436314034726}. Best is trial 0 with value: 0.8273696769264581.\n",
      "[I 2025-09-09 17:32:43,407] Trial 1 finished with value: 0.8514719887808606 and parameters: {'n_estimators': 85, 'max_depth': 4, 'learning_rate': 0.053057700455696244, 'subsample': 0.9922838701317358}. Best is trial 1 with value: 0.8514719887808606.\n",
      "[I 2025-09-09 17:32:43,676] Trial 2 finished with value: 0.8544337578884574 and parameters: {'n_estimators': 127, 'max_depth': 6, 'learning_rate': 0.03151349991419431, 'subsample': 0.8913476738840753}. Best is trial 2 with value: 0.8544337578884574.\n",
      "[I 2025-09-09 17:32:43,834] Trial 3 finished with value: 0.8568094525436677 and parameters: {'n_estimators': 131, 'max_depth': 3, 'learning_rate': 0.06542494444632073, 'subsample': 0.7942457005741048}. Best is trial 3 with value: 0.8568094525436677.\n",
      "[I 2025-09-09 17:32:44,017] Trial 4 finished with value: 0.8238322989816956 and parameters: {'n_estimators': 103, 'max_depth': 5, 'learning_rate': 0.22658285400934566, 'subsample': 0.8775645014723649}. Best is trial 3 with value: 0.8568094525436677.\n",
      "[I 2025-09-09 17:32:44,179] A new study created in memory with name: no-name-b6e70920-cf36-4398-9dbd-52a088bc2912\n",
      "[I 2025-09-09 17:32:44,312] Trial 0 finished with value: 0.842722734455945 and parameters: {'n_estimators': 58, 'max_depth': 6, 'learning_rate': 0.1540605293778176, 'num_leaves': 19}. Best is trial 0 with value: 0.842722734455945.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train AUC: 0.9191, Test AUC: 0.8568, \n",
      "  Training LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-09 17:32:44,611] Trial 1 finished with value: 0.8299860807317557 and parameters: {'n_estimators': 199, 'max_depth': 4, 'learning_rate': 0.11322324177225164, 'num_leaves': 55}. Best is trial 0 with value: 0.842722734455945.\n",
      "[I 2025-09-09 17:32:44,987] Trial 2 finished with value: 0.8149783885045682 and parameters: {'n_estimators': 126, 'max_depth': 5, 'learning_rate': 0.2836047455167211, 'num_leaves': 36}. Best is trial 0 with value: 0.842722734455945.\n",
      "[I 2025-09-09 17:32:45,139] Trial 3 finished with value: 0.8479136796056557 and parameters: {'n_estimators': 65, 'max_depth': 6, 'learning_rate': 0.06374536442606868, 'num_leaves': 18}. Best is trial 3 with value: 0.8479136796056557.\n",
      "[I 2025-09-09 17:32:45,670] Trial 4 finished with value: 0.8435285868279766 and parameters: {'n_estimators': 126, 'max_depth': 6, 'learning_rate': 0.026306870543398336, 'num_leaves': 39}. Best is trial 3 with value: 0.8479136796056557.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train AUC: 0.9428, Test AUC: 0.8479, \n",
      "\n",
      "Best model: XGBoost (AUC: 0.8568)\n",
      "6. Generating reports...\n",
      "\n",
      "Generating reports...\n",
      "  Calculating PSI...\n",
      "  Reports saved to: output_complete\\model_report.xlsx\n",
      "Pipeline completed successfully!\n",
      "\n",
      "============================================================\n",
      "Pipeline execution completed!\n"
     ]
    }
   ],
   "source": [
    "# Run the complete pipeline\n",
    "print(\"Starting complete pipeline execution...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pipeline = run_pipeline(df, config=config)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Pipeline execution completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract Results and Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PIPELINE RESULTS:\n",
      "============================================================\n",
      "\n",
      "Best Model: XGBoost\n",
      "Best Score (AUC): 0.8568\n",
      "Best Gini: 0.7136\n",
      "\n",
      "Features Selected: 7\n",
      "Selected Features: ['feature_26', 'feature_46', 'feature_08', 'feature_29', 'feature_38', 'feature_47', 'feature_48']\n",
      "\n",
      "Data Split:\n",
      "  - Train: 4000 samples\n",
      "  - Test: 1000 samples\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11800\\3663843532.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"  - Train: {len(pipeline.train_)} samples\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"  - Test: {len(pipeline.test_)} samples\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test_'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"  - Test: Not used\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"  - OOT: {len(pipeline.oot_)} samples\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'oot_'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"  - OOT: Not used\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Extract key results\n",
    "print(\"\\nPIPELINE RESULTS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best model info\n",
    "if hasattr(pipeline, 'best_model_'):\n",
    "    print(f\"\\nBest Model: {pipeline.best_model_name_}\")\n",
    "    print(f\"Best Score (AUC): {pipeline.best_auc_:.4f}\")\n",
    "    print(f\"Best Gini: {(pipeline.best_auc_ * 2 - 1):.4f}\")\n",
    "\n",
    "# Get selected features\n",
    "if hasattr(pipeline, 'final_vars_'):\n",
    "    print(f\"\\nFeatures Selected: {len(pipeline.final_vars_)}\")\n",
    "    print(f\"Selected Features: {pipeline.final_vars_[:10]}...\" if len(pipeline.final_vars_) > 10 else f\"Selected Features: {pipeline.final_vars_}\")\n",
    "\n",
    "# Get data split info\n",
    "if hasattr(pipeline, 'train_'):\n",
    "    print(f\"\\nData Split:\")\n",
    "    print(f\"  - Train: {len(pipeline.train_)} samples\")\n",
    "    print(f\"  - Test: {len(pipeline.test_)} samples\" if hasattr(pipeline, 'test_') else \"  - Test: Not used\")\n",
    "    print(f\"  - OOT: {len(pipeline.oot_)} samples\" if hasattr(pipeline, 'oot_') else \"  - OOT: Not used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Scoring and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11800\\3996467670.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Generate predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_model_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Create score distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    600\u001b[0m         Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n\u001b[0;32m    601\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No support for np.int64 index based sparse matrices\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m             _assert_all_finite(\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             )\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Generate scores\n",
    "if hasattr(pipeline, 'best_model_') and hasattr(pipeline, 'train_'):\n",
    "    # Prepare data\n",
    "    X_train = pipeline.train_[pipeline.final_vars_]\n",
    "    y_train = pipeline.train_[config.target_col]\n",
    "    \n",
    "    # Generate predictions\n",
    "    train_scores = pipeline.best_model_.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # Create score distribution\n",
    "    score_df = pd.DataFrame({\n",
    "        'score': train_scores,\n",
    "        'target': y_train\n",
    "    })\n",
    "    \n",
    "    # Score statistics\n",
    "    print(\"\\nSCORE DISTRIBUTION:\")\n",
    "    print(\"=\"*40)\n",
    "    print(score_df['score'].describe())\n",
    "    \n",
    "    # Plot score distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Distribution by target\n",
    "    score_df[score_df['target']==0]['score'].hist(bins=30, alpha=0.5, label='Good', ax=axes[0])\n",
    "    score_df[score_df['target']==1]['score'].hist(bins=30, alpha=0.5, label='Bad', ax=axes[0])\n",
    "    axes[0].set_xlabel('Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Score Distribution by Target')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Cumulative distribution\n",
    "    axes[1].hist(score_df['score'], bins=50, cumulative=True, density=True)\n",
    "    axes[1].set_xlabel('Score')\n",
    "    axes[1].set_ylabel('Cumulative Probability')\n",
    "    axes[1].set_title('Cumulative Score Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Risk Bands and Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create risk bands\n",
    "if 'train_scores' in locals():\n",
    "    # Create 10 risk bands\n",
    "    score_df['risk_band'] = pd.qcut(score_df['score'], q=10, labels=False, duplicates='drop')\n",
    "    \n",
    "    # Calculate statistics per band\n",
    "    risk_bands = score_df.groupby('risk_band').agg({\n",
    "        'score': ['min', 'max', 'mean'],\n",
    "        'target': ['count', 'sum', 'mean']\n",
    "    })\n",
    "    \n",
    "    risk_bands.columns = ['min_score', 'max_score', 'avg_score', 'count', 'bads', 'bad_rate']\n",
    "    risk_bands['goods'] = risk_bands['count'] - risk_bands['bads']\n",
    "    risk_bands['odds'] = risk_bands['goods'] / risk_bands['bads']\n",
    "    risk_bands['log_odds'] = np.log(risk_bands['odds'])\n",
    "    \n",
    "    print(\"\\nRISK BANDS ANALYSIS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(risk_bands)\n",
    "    \n",
    "    # Calibration plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(risk_bands.index, risk_bands['bad_rate'], 'o-')\n",
    "    plt.xlabel('Risk Band')\n",
    "    plt.ylabel('Bad Rate')\n",
    "    plt.title('Bad Rate by Risk Band')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(risk_bands['avg_score'], risk_bands['bad_rate'])\n",
    "    plt.xlabel('Average Score')\n",
    "    plt.ylabel('Actual Bad Rate')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='Perfect Calibration')\n",
    "    plt.title('Calibration Plot')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PSI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11800\\4143462133.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_vars_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtrain_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_model_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_model_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    600\u001b[0m         Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n\u001b[0;32m    601\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No support for np.int64 index based sparse matrices\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m             _assert_all_finite(\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             )\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# PSI Analysis\n",
    "from risk_pipeline.core.psi_calculator import PSICalculator\n",
    "\n",
    "if hasattr(pipeline, 'train_') and hasattr(pipeline, 'test_'):\n",
    "    psi_calc = PSICalculator()\n",
    "    \n",
    "    # Score PSI\n",
    "    X_train = pipeline.train_[pipeline.final_vars_]\n",
    "    X_test = pipeline.test_[pipeline.final_vars_]\n",
    "    \n",
    "    train_scores = pipeline.best_model_.predict_proba(X_train)[:, 1]\n",
    "    test_scores = pipeline.best_model_.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    score_psi, psi_df = psi_calc.calculate_score_psi(train_scores, test_scores)\n",
    "    \n",
    "    print(\"\\nPSI ANALYSIS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Score PSI: {score_psi:.4f}\")\n",
    "    print(f\"Interpretation: {psi_calc._interpret_psi(score_psi)}\")\n",
    "    print(\"\\nPSI by Decile:\")\n",
    "    print(psi_df[['decile', 'train_pct', 'test_pct', 'psi_contribution']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comprehensive Model Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "import os\n",
    "\n",
    "print(\"\\nCOMPREHENSIVE MODEL REPORT:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check output files\n",
    "if os.path.exists(config.output_folder):\n",
    "    files = os.listdir(config.output_folder)\n",
    "    print(f\"\\nGenerated {len(files)} output files:\")\n",
    "    for f in files:\n",
    "        size = os.path.getsize(os.path.join(config.output_folder, f)) / 1024\n",
    "        print(f\"  - {f} ({size:.1f} KB)\")\n",
    "\n",
    "# Model comparison if dual pipeline was used\n",
    "if hasattr(pipeline, 'model_builder'):\n",
    "    if hasattr(pipeline.model_builder, 'scores_'):\n",
    "        scores = pipeline.model_builder.scores_\n",
    "        \n",
    "        print(\"\\nMODEL COMPARISON:\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        comparison_data = []\n",
    "        for model_name, model_scores in scores.items():\n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'Train AUC': model_scores.get('train_auc', 0),\n",
    "                'Test AUC': model_scores.get('test_auc', 0),\n",
    "                'Train Gini': (model_scores.get('train_auc', 0) * 2 - 1),\n",
    "                'Test Gini': (model_scores.get('test_auc', 0) * 2 - 1)\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_df = comparison_df.sort_values('Test Gini', ascending=False)\n",
    "        print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nFINAL SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ“ Data: {len(df)} samples processed\")\n",
    "print(f\"âœ“ Features: {len(pipeline.final_vars_)} selected from {len(df.columns)-3}\")\n",
    "print(f\"âœ“ Best Model: {pipeline.best_model_name_}\")\n",
    "print(f\"âœ“ Performance: Gini = {(pipeline.best_auc_ * 2 - 1):.2%}\")\n",
    "print(f\"âœ“ Stability: PSI = {score_psi:.4f}\" if 'score_psi' in locals() else \"âœ“ Stability: PSI calculated\")\n",
    "print(f\"âœ“ Reports: Saved to {config.output_folder}/\")\n",
    "print(\"\\nâœ… COMPLETE PIPELINE TEST SUCCESSFUL!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "if hasattr(pipeline, 'best_model_') and hasattr(pipeline.best_model_, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': pipeline.final_vars_,\n",
    "        'importance': pipeline.best_model_.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTOP 15 IMPORTANT FEATURES:\")\n",
    "    print(\"=\"*60)\n",
    "    print(importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Plot importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = importance_df.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 15 Feature Importances')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Final Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model and configuration\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(config.output_folder, 'final_model.pkl')\n",
    "joblib.dump(pipeline.best_model_, model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save configuration\n",
    "config_dict = config.to_dict()\n",
    "config_path = os.path.join(config.output_folder, 'pipeline_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2, default=str)\n",
    "print(f\"Configuration saved to: {config_path}\")\n",
    "\n",
    "# Save selected features\n",
    "features_path = os.path.join(config.output_folder, 'selected_features.txt')\n",
    "with open(features_path, 'w') as f:\n",
    "    for feature in pipeline.final_vars_:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "print(f\"Features saved to: {features_path}\")\n",
    "\n",
    "print(\"\\nâœ… ALL OUTPUTS SAVED SUCCESSFULLY!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
