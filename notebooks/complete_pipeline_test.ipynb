{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Risk Model Pipeline Test\n",
    "## Full Functionality Test with GitHub Package Installation\n",
    "\n",
    "This notebook:\n",
    "1. Installs the package directly from GitHub (development branch)\n",
    "2. Creates synthetic test data\n",
    "3. Tests ALL pipeline functionalities\n",
    "4. Validates outputs and generates comprehensive reports\n",
    "\n",
    "**Instructions:**\n",
    "- Run cells sequentially from top to bottom\n",
    "- Restart kernel if you encounter import errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Package from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/selimoksuz/risk-model-pipeline.git 'C:\\Users\\Acer\\AppData\\Local\\Temp\\pip-req-build-64xe7gfu'\n",
      "  Running command git checkout -b development --track origin/development\n",
      "  Branch 'development' set up to track remote branch 'development' from 'origin'.\n",
      "  Switched to a new branch 'development'\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/selimoksuz/risk-model-pipeline.git@development\n",
      "  Cloning https://github.com/selimoksuz/risk-model-pipeline.git (to revision development) to c:\\users\\acer\\appdata\\local\\temp\\pip-req-build-64xe7gfu\n",
      "  Resolved https://github.com/selimoksuz/risk-model-pipeline.git to commit 4c89fc36063eb101cc1a75dde72da1938cb802ab\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: xlsxwriter>=3.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from risk-pipeline==0.3.0) (3.2.5)\n",
      "Requirement already satisfied: openpyxl>=3.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from risk-pipeline==0.3.0) (3.1.5)\n",
      "Requirement already satisfied: scikit-learn<1.3.0,>=1.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from risk-pipeline==0.3.0) (1.2.2)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.3.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from risk-pipeline==0.3.0) (1.5.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from risk-pipeline==0.3.0) (1.5.2)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.20.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from risk-pipeline==0.3.0) (1.24.4)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\acer\\anaconda3\\lib\\site-packages (from openpyxl>=3.0.0->risk-pipeline==0.3.0) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pandas<2.0.0,>=1.3.0->risk-pipeline==0.3.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pandas<2.0.0,>=1.3.0->risk-pipeline==0.3.0) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from scikit-learn<1.3.0,>=1.0.0->risk-pipeline==0.3.0) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from scikit-learn<1.3.0,>=1.0.0->risk-pipeline==0.3.0) (1.13.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas<2.0.0,>=1.3.0->risk-pipeline==0.3.0) (1.17.0)\n",
      "✅ Package installed successfully!\n",
      "Package location: C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\risk_pipeline\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "# Install package directly from GitHub development branch\n",
    "!pip install --upgrade git+https://github.com/selimoksuz/risk-model-pipeline.git@development\n",
    "\n",
    "# Verify installation\n",
    "import risk_pipeline\n",
    "print(f\"✅ Package installed successfully!\")\n",
    "print(f\"Package location: {risk_pipeline.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Standard libraries imported successfully!\n",
      "Timestamp: 2025-09-15 11:25:44\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, classification_report,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, precision_recall_curve\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Try importing XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGBOOST = True\n",
    "except ImportError:\n",
    "    HAS_XGBOOST = False\n",
    "    print(\"⚠️ XGBoost not installed\")\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"✅ Standard libraries imported successfully!\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Create synthetic dataset\nn_samples = 10000\nn_features = 30\n\n# Generate classification data\nX, y = make_classification(\n    n_samples=n_samples,\n    n_features=n_features,\n    n_informative=20,\n    n_redundant=5,\n    n_repeated=0,\n    n_classes=2,\n    n_clusters_per_class=3,\n    weights=[0.85, 0.15],  # Imbalanced (15% positive rate)\n    flip_y=0.02,  # Add 2% label noise\n    random_state=RANDOM_STATE\n)\n\n# Create DataFrame\nfeature_names = [f'feature_{i:02d}' for i in range(n_features)]\ndf = pd.DataFrame(X, columns=feature_names)\ndf['target'] = y\n\n# Add categorical features\ndf['category_1'] = np.random.choice(['A', 'B', 'C', 'D'], size=n_samples)\ndf['category_2'] = np.random.choice(['Low', 'Medium', 'High'], size=n_samples, p=[0.5, 0.3, 0.2])\ndf['region'] = np.random.choice(['North', 'South', 'East', 'West', 'Central'], size=n_samples)\n\n# Add time column for OOT splitting (distribute over 12 months)\nbase_date = pd.Timestamp('2023-01-01')\ndays_range = 365  # 1 year of data\ndf['application_date'] = [base_date + pd.Timedelta(days=np.random.randint(0, days_range)) \n                          for _ in range(n_samples)]\ndf = df.sort_values('application_date').reset_index(drop=True)\n\n# Add some missing values\nmissing_features = np.random.choice(feature_names[:10], 5, replace=False)\nfor feat in missing_features:\n    missing_idx = np.random.choice(n_samples, int(n_samples * 0.05), replace=False)\n    df.loc[missing_idx, feat] = np.nan\n\n# Add ID column\ndf['customer_id'] = [f'CUST_{i:06d}' for i in range(n_samples)]\n\n# Reorder columns\ndf = df[['customer_id', 'application_date'] + feature_names + ['category_1', 'category_2', 'region', 'target']]\n\nprint(f\"✅ Synthetic dataset created!\")\nprint(f\"Shape: {df.shape}\")\nprint(f\"\\nDate range: {df['application_date'].min()} to {df['application_date'].max()}\")\nprint(f\"Total days: {(df['application_date'].max() - df['application_date'].min()).days}\")\nprint(f\"\\nTarget distribution:\")\nprint(df['target'].value_counts())\nprint(f\"Target rate: {df['target'].mean():.2%}\")\nprint(f\"\\nMissing values:\")\nmissing_summary = df.isnull().sum()\nprint(missing_summary[missing_summary > 0])\nprint(f\"\\nData types:\")\nprint(df.dtypes.value_counts())\nprint(f\"\\nFirst 5 rows:\")\ndf.head()"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module Import Status:\n",
      "========================================\n",
      "Config: ✅\n",
      "DataProcessor: ✅\n",
      "DataSplitter: ✅\n",
      "FeatureEngineer: ✅\n",
      "FeatureSelector: ✅\n",
      "WOETransformer: ✅\n",
      "ModelBuilder: ✅\n",
      "ModelTrainer: ✅\n",
      "Reporter: ✅\n",
      "ReportGenerator: ✅\n",
      "PSICalculator: ✅\n",
      "CalibrationAnalyzer: ✅\n",
      "RiskBandOptimizer: ✅\n",
      "RiskModelPipeline: ✅\n",
      "\n",
      "Successfully imported: 14/14 modules\n"
     ]
    }
   ],
   "source": [
    "# Import all modules from risk_pipeline with error handling\n",
    "modules_status = {}\n",
    "\n",
    "# Core modules\n",
    "try:\n",
    "    from risk_pipeline.core.config import Config\n",
    "    modules_status['Config'] = '✅'\n",
    "except ImportError as e:\n",
    "    modules_status['Config'] = f'❌ {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.data_processor import DataProcessor\n",
    "    modules_status['DataProcessor'] = '✅'\n",
    "except ImportError as e:\n",
    "    modules_status['DataProcessor'] = f'❌ {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.splitter import DataSplitter\n",
    "    modules_status['DataSplitter'] = '✅'\n",
    "except ImportError as e:\n",
    "    modules_status['DataSplitter'] = f'❌ {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.feature_engineer import FeatureEngineer\n",
    "    modules_status['FeatureEngineer'] = '✅'\n",
    "except ImportError as e:\n",
    "    modules_status['FeatureEngineer'] = f'❌ {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.feature_selector import FeatureSelector\n",
    "    modules_status['FeatureSelector'] = '✅'\n",
    "except ImportError as e:\n",
    "    modules_status['FeatureSelector'] = f'❌ {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.woe_transformer import WOETransformer\n",
    "    modules_status['WOETransformer'] = '✅'\n",
    "except ImportError as e:\n",
    "    modules_status['WOETransformer'] = f'❌ {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.model_builder import ModelBuilder\n",
    "    modules_status['ModelBuilder'] = '✅'\n",
    "except ImportError as e:\n",
    "    modules_status['ModelBuilder'] = f'❌ {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.model_trainer import ModelTrainer\n",
    "    modules_status['ModelTrainer'] = '✅'\n",
    "except ImportError as e:\n",
    "    modules_status['ModelTrainer'] = f'❌ {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.reporter import Reporter\n",
    "    modules_status['Reporter'] = '✅'\n",
    "except ImportError as e:\n",
    "    modules_status['Reporter'] = f'❌ {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.report_generator import ReportGenerator\n",
    "    modules_status['ReportGenerator'] = '✅'\n",
    "except ImportError as e:\n",
    "    modules_status['ReportGenerator'] = f'❌ {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.psi_calculator import PSICalculator\n",
    "    modules_status['PSICalculator'] = '✅'\n",
    "except ImportError as e:\n",
    "    modules_status['PSICalculator'] = f'❌ {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.calibration_analyzer import CalibrationAnalyzer\n",
    "    modules_status['CalibrationAnalyzer'] = '✅'\n",
    "except ImportError as e:\n",
    "    modules_status['CalibrationAnalyzer'] = f'❌ {e}'\n",
    "\n",
    "try:\n",
    "    from risk_pipeline.core.risk_band_optimizer import RiskBandOptimizer\n",
    "    modules_status['RiskBandOptimizer'] = '✅'\n",
    "except ImportError as e:\n",
    "    modules_status['RiskBandOptimizer'] = f'❌ {e}'\n",
    "\n",
    "# Pipeline classes\n",
    "PIPELINE_CLASS = None\n",
    "try:\n",
    "    from risk_pipeline.pipeline import RiskModelPipeline\n",
    "    PIPELINE_CLASS = RiskModelPipeline\n",
    "    modules_status['RiskModelPipeline'] = '✅'\n",
    "except ImportError:\n",
    "    try:\n",
    "        from risk_pipeline.complete_pipeline import CompletePipeline\n",
    "        PIPELINE_CLASS = CompletePipeline\n",
    "        modules_status['CompletePipeline'] = '✅'\n",
    "    except ImportError:\n",
    "        modules_status['Pipeline'] = '❌ No pipeline class available'\n",
    "\n",
    "# Display import status\n",
    "print(\"Module Import Status:\")\n",
    "print(\"=\"*40)\n",
    "for module, status in modules_status.items():\n",
    "    print(f\"{module}: {status}\")\n",
    "\n",
    "# Count successful imports\n",
    "success_count = sum(1 for s in modules_status.values() if '✅' in str(s))\n",
    "print(f\"\\nSuccessfully imported: {success_count}/{len(modules_status)} modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Create configuration\nconfig = Config(\n    target_col='target',  \n    id_col='customer_id',  \n    time_col='application_date',  # Time column for time-based OOT splitting\n    random_state=RANDOM_STATE,\n    cv_folds=5,\n    \n    # Data splitting - Time based OOT\n    use_test_split=True,\n    train_ratio=0.6,  # 60% train\n    test_ratio=0.2,   # 20% test  \n    oot_ratio=0.2,    # 20% OOT\n    oot_months=3,     # Last 3 months as OOT (if time_col exists)\n    min_oot_size=50,  # Minimum samples required for OOT\n    \n    # Feature selection\n    iv_threshold=0.02,\n    psi_threshold=0.25,\n    max_features=20,\n    min_features=3,\n    use_boruta=False,  # Disable for faster testing\n    forward_selection=False,\n    \n    # WOE settings\n    n_bins=5,\n    min_bin_size=0.05,\n    woe_monotonic=False,\n    \n    # Model settings\n    use_optuna=False,  # Disable hyperparameter optimization for faster testing\n    n_trials=10,\n    \n    # Output\n    output_folder='test_outputs',\n    write_csv=True\n)\n\nprint(\"✅ Configuration created!\")\nprint(f\"\\nKey settings:\")\nprint(f\"  Target column: {config.target_col}\")\nprint(f\"  ID column: {config.id_col}\")\nprint(f\"  Time column: {config.time_col}\")\nprint(f\"  Random state: {config.random_state}\")\nprint(f\"\\nSplitting settings:\")\nprint(f\"  Train ratio: {config.train_ratio}\")\nprint(f\"  Test ratio: {config.test_ratio}\")\nprint(f\"  OOT ratio: {config.oot_ratio}\")\nprint(f\"  OOT months: {config.oot_months} (last {config.oot_months} months)\")\nprint(f\"  Min OOT size: {config.min_oot_size}\")\nprint(f\"\\nFeature settings:\")\nprint(f\"  Max features: {config.max_features}\")\nprint(f\"  IV threshold: {config.iv_threshold}\")\nprint(f\"  PSI threshold: {config.psi_threshold}\")\nprint(f\"\\nOutput folder: {config.output_folder}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Synthetic dataset created!\n",
      "Shape: (10000, 35)\n",
      "\n",
      "Target distribution:\n",
      "0    8438\n",
      "1    1562\n",
      "Name: target, dtype: int64\n",
      "Target rate: 15.62%\n",
      "\n",
      "Missing values:\n",
      "feature_02    500\n",
      "feature_03    500\n",
      "feature_05    500\n",
      "feature_06    500\n",
      "feature_07    500\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "float64    30\n",
      "object      4\n",
      "int32       1\n",
      "dtype: int64\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>feature_05</th>\n",
       "      <th>feature_06</th>\n",
       "      <th>feature_07</th>\n",
       "      <th>feature_08</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>region</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUST_000000</td>\n",
       "      <td>-6.477781</td>\n",
       "      <td>-0.731372</td>\n",
       "      <td>-2.311460</td>\n",
       "      <td>-0.058832</td>\n",
       "      <td>-3.018677</td>\n",
       "      <td>0.074369</td>\n",
       "      <td>-0.861212</td>\n",
       "      <td>3.084822</td>\n",
       "      <td>0.399639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215469</td>\n",
       "      <td>-2.098355</td>\n",
       "      <td>0.890491</td>\n",
       "      <td>0.128130</td>\n",
       "      <td>0.248140</td>\n",
       "      <td>-0.677959</td>\n",
       "      <td>C</td>\n",
       "      <td>Low</td>\n",
       "      <td>West</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUST_000001</td>\n",
       "      <td>1.227635</td>\n",
       "      <td>-0.267962</td>\n",
       "      <td>-0.633130</td>\n",
       "      <td>2.561494</td>\n",
       "      <td>0.924121</td>\n",
       "      <td>-2.672802</td>\n",
       "      <td>1.811894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.809488</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.968186</td>\n",
       "      <td>-1.643806</td>\n",
       "      <td>-1.434728</td>\n",
       "      <td>-1.202694</td>\n",
       "      <td>-3.587611</td>\n",
       "      <td>-0.564109</td>\n",
       "      <td>D</td>\n",
       "      <td>Low</td>\n",
       "      <td>Central</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUST_000002</td>\n",
       "      <td>-4.838718</td>\n",
       "      <td>-1.522911</td>\n",
       "      <td>-0.663803</td>\n",
       "      <td>-0.160599</td>\n",
       "      <td>3.114718</td>\n",
       "      <td>-3.141340</td>\n",
       "      <td>-0.450571</td>\n",
       "      <td>-1.715498</td>\n",
       "      <td>-3.816446</td>\n",
       "      <td>...</td>\n",
       "      <td>5.456020</td>\n",
       "      <td>-0.136188</td>\n",
       "      <td>-0.643618</td>\n",
       "      <td>2.186524</td>\n",
       "      <td>-3.353286</td>\n",
       "      <td>0.930788</td>\n",
       "      <td>A</td>\n",
       "      <td>High</td>\n",
       "      <td>Central</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUST_000003</td>\n",
       "      <td>-0.985095</td>\n",
       "      <td>5.833187</td>\n",
       "      <td>-1.116619</td>\n",
       "      <td>-2.408938</td>\n",
       "      <td>4.254459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.080952</td>\n",
       "      <td>2.055296</td>\n",
       "      <td>-4.600582</td>\n",
       "      <td>...</td>\n",
       "      <td>5.616757</td>\n",
       "      <td>-0.167999</td>\n",
       "      <td>-1.054511</td>\n",
       "      <td>4.308316</td>\n",
       "      <td>0.042326</td>\n",
       "      <td>-1.834173</td>\n",
       "      <td>C</td>\n",
       "      <td>Low</td>\n",
       "      <td>West</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUST_000004</td>\n",
       "      <td>2.500893</td>\n",
       "      <td>2.024687</td>\n",
       "      <td>-1.556530</td>\n",
       "      <td>-3.134494</td>\n",
       "      <td>0.687153</td>\n",
       "      <td>-6.029785</td>\n",
       "      <td>4.187321</td>\n",
       "      <td>-1.743445</td>\n",
       "      <td>6.840227</td>\n",
       "      <td>...</td>\n",
       "      <td>4.779020</td>\n",
       "      <td>-0.214986</td>\n",
       "      <td>-2.706046</td>\n",
       "      <td>5.223239</td>\n",
       "      <td>-1.464699</td>\n",
       "      <td>1.415935</td>\n",
       "      <td>C</td>\n",
       "      <td>High</td>\n",
       "      <td>West</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  feature_00  feature_01  feature_02  feature_03  feature_04  \\\n",
       "0  CUST_000000   -6.477781   -0.731372   -2.311460   -0.058832   -3.018677   \n",
       "1  CUST_000001    1.227635   -0.267962   -0.633130    2.561494    0.924121   \n",
       "2  CUST_000002   -4.838718   -1.522911   -0.663803   -0.160599    3.114718   \n",
       "3  CUST_000003   -0.985095    5.833187   -1.116619   -2.408938    4.254459   \n",
       "4  CUST_000004    2.500893    2.024687   -1.556530   -3.134494    0.687153   \n",
       "\n",
       "   feature_05  feature_06  feature_07  feature_08  ...  feature_24  \\\n",
       "0    0.074369   -0.861212    3.084822    0.399639  ...   -0.215469   \n",
       "1   -2.672802    1.811894         NaN    4.809488  ...   -4.968186   \n",
       "2   -3.141340   -0.450571   -1.715498   -3.816446  ...    5.456020   \n",
       "3         NaN   -3.080952    2.055296   -4.600582  ...    5.616757   \n",
       "4   -6.029785    4.187321   -1.743445    6.840227  ...    4.779020   \n",
       "\n",
       "   feature_25  feature_26  feature_27  feature_28  feature_29  category_1  \\\n",
       "0   -2.098355    0.890491    0.128130    0.248140   -0.677959           C   \n",
       "1   -1.643806   -1.434728   -1.202694   -3.587611   -0.564109           D   \n",
       "2   -0.136188   -0.643618    2.186524   -3.353286    0.930788           A   \n",
       "3   -0.167999   -1.054511    4.308316    0.042326   -1.834173           C   \n",
       "4   -0.214986   -2.706046    5.223239   -1.464699    1.415935           C   \n",
       "\n",
       "   category_2   region  target  \n",
       "0         Low     West       0  \n",
       "1         Low  Central       0  \n",
       "2        High  Central       1  \n",
       "3         Low     West       0  \n",
       "4        High     West       0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create synthetic dataset\n",
    "n_samples = 10000\n",
    "n_features = 30\n",
    "\n",
    "# Generate classification data\n",
    "X, y = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=20,\n",
    "    n_redundant=5,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=3,\n",
    "    weights=[0.85, 0.15],  # Imbalanced (15% positive rate)\n",
    "    flip_y=0.02,  # Add 2% label noise\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = [f'feature_{i:02d}' for i in range(n_features)]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# Add categorical features\n",
    "df['category_1'] = np.random.choice(['A', 'B', 'C', 'D'], size=n_samples)\n",
    "df['category_2'] = np.random.choice(['Low', 'Medium', 'High'], size=n_samples, p=[0.5, 0.3, 0.2])\n",
    "df['region'] = np.random.choice(['North', 'South', 'East', 'West', 'Central'], size=n_samples)\n",
    "\n",
    "# Add some missing values\n",
    "missing_features = np.random.choice(feature_names[:10], 5, replace=False)\n",
    "for feat in missing_features:\n",
    "    missing_idx = np.random.choice(n_samples, int(n_samples * 0.05), replace=False)\n",
    "    df.loc[missing_idx, feat] = np.nan\n",
    "\n",
    "# Add ID column\n",
    "df['customer_id'] = [f'CUST_{i:06d}' for i in range(n_samples)]\n",
    "\n",
    "# Reorder columns\n",
    "df = df[['customer_id'] + feature_names + ['category_1', 'category_2', 'region', 'target']]\n",
    "\n",
    "print(f\"✅ Synthetic dataset created!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"Target rate: {df['target'].mean():.2%}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "missing_summary = df.isnull().sum()\n",
    "print(missing_summary[missing_summary > 0])\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Test DataSplitter\nif 'DataSplitter' in globals():\n    splitter = DataSplitter(config)\n    splits = splitter.split(df_processed)\n    \n    print(\"✅ Data splitting completed!\")\n    print(f\"\\nSplit sizes:\")\n    print(f\"  Train: {len(splits['train'])} ({len(splits['train'])/len(df_processed):.1%})\")\n    \n    if 'validation' in splits:\n        print(f\"  Validation: {len(splits['validation'])} ({len(splits['validation'])/len(df_processed):.1%})\")\n    \n    if 'test' in splits:\n        print(f\"  Test: {len(splits['test'])} ({len(splits['test'])/len(df_processed):.1%})\")\n    \n    if 'oot' in splits:\n        print(f\"  OOT: {len(splits['oot'])} ({len(splits['oot'])/len(df_processed):.1%})\")\n    \n    print(f\"\\nTarget rates:\")\n    print(f\"  Train: {splits['train']['target'].mean():.2%}\")\n    \n    if 'validation' in splits:\n        print(f\"  Validation: {splits['validation']['target'].mean():.2%}\")\n    \n    if 'test' in splits:\n        print(f\"  Test: {splits['test']['target'].mean():.2%}\")\n    \n    if 'oot' in splits:\n        print(f\"  OOT: {splits['oot']['target'].mean():.2%}\")\n    \n    # Prepare data\n    X_train = splits['train'].drop(columns=['target', 'customer_id'], errors='ignore')\n    y_train = splits['train']['target']\n    \n    # Use OOT as validation if available, otherwise use test\n    if 'oot' in splits:\n        X_val = splits['oot'].drop(columns=['target', 'customer_id'], errors='ignore')\n        y_val = splits['oot']['target']\n    elif 'validation' in splits:\n        X_val = splits['validation'].drop(columns=['target', 'customer_id'], errors='ignore')\n        y_val = splits['validation']['target']\n    else:\n        # Create validation from train\n        from sklearn.model_selection import train_test_split\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_train, y_train, test_size=0.2, random_state=RANDOM_STATE, stratify=y_train\n        )\n    \n    # Use test set if available\n    if 'test' in splits:\n        X_test = splits['test'].drop(columns=['target', 'customer_id'], errors='ignore')\n        y_test = splits['test']['target']\n    else:\n        # Use validation as test\n        X_test = X_val\n        y_test = y_val\nelse:\n    print(\"⚠️ DataSplitter not available, using sklearn\")\n    # Manual split\n    X = df_processed.drop(columns=['target', 'customer_id'], errors='ignore')\n    y = df_processed['target']\n    \n    X_temp, X_test, y_temp, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n    )\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_temp, y_temp, test_size=0.125, random_state=RANDOM_STATE, stratify=y_temp\n    )\n    \n    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration created!\n",
      "\n",
      "Key settings:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'target_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21176\\2559338598.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"✅ Configuration created!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nKey settings:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"  Target: {config.target_column}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"  Test size: {config.test_size}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"  Validation size: {config.validation_size}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Config' object has no attribute 'target_column'"
     ]
    }
   ],
   "source": [
    "# Create configuration\n",
    "config = Config(\n",
    "    target_column='target',\n",
    "    id_column='customer_id',\n",
    "    test_size=0.2,\n",
    "    validation_size=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    cv_folds=5,\n",
    "    \n",
    "    # Feature engineering\n",
    "    create_polynomial=False,  # Disable for faster testing\n",
    "    polynomial_degree=2,\n",
    "    create_interactions=False,\n",
    "    \n",
    "    # Feature selection\n",
    "    selection_method='importance',\n",
    "    top_k_features=20,\n",
    "    \n",
    "    # WOE settings\n",
    "    max_bins=5,\n",
    "    min_samples_leaf=0.05,\n",
    "    \n",
    "    # Model settings\n",
    "    scoring_metric='roc_auc',\n",
    "    \n",
    "    # Output\n",
    "    output_folder='test_outputs',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"✅ Configuration created!\")\n",
    "print(f\"\\nKey settings:\")\n",
    "print(f\"  Target: {config.target_column}\")\n",
    "print(f\"  Test size: {config.test_size}\")\n",
    "print(f\"  Validation size: {config.validation_size}\")\n",
    "print(f\"  Top K features: {config.top_k_features}\")\n",
    "print(f\"  Output folder: {config.output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Test WOETransformer\nif 'WOETransformer' in globals():\n    woe_transformer = WOETransformer(config)\n    \n    # Prepare DataFrames with target column for WOE fitting\n    train_df = X_train_selected.copy()\n    train_df['target'] = y_train\n    train_df['customer_id'] = range(len(train_df))\n    \n    val_df = X_val_selected.copy()\n    val_df['target'] = y_val\n    val_df['customer_id'] = range(len(val_df))\n    \n    test_df = X_test_selected.copy()\n    test_df['target'] = y_test\n    test_df['customer_id'] = range(len(test_df))\n    \n    # Fit and transform\n    woe_result = woe_transformer.fit_transform(\n        train=train_df,\n        test=test_df,\n        oot=val_df,\n        features=selected_features\n    )\n    \n    # Extract transformed data\n    X_train_woe = woe_result['train'].drop(columns=['target', 'customer_id'], errors='ignore')\n    \n    if 'test' in woe_result:\n        X_test_woe = woe_result['test'].drop(columns=['target', 'customer_id'], errors='ignore')\n    else:\n        X_test_woe = X_test_selected\n    \n    if 'oot' in woe_result:\n        X_val_woe = woe_result['oot'].drop(columns=['target', 'customer_id'], errors='ignore')\n    else:\n        X_val_woe = X_val_selected\n    \n    print(\"✅ WOE transformation completed!\")\n    print(f\"\\nTransformed shape: {X_train_woe.shape}\")\n    \n    # Show WOE mapping info\n    if 'mapping' in woe_result and woe_result['mapping']:\n        print(f\"WOE mappings created for {len(woe_result['mapping'])} features\")\nelse:\n    print(\"⚠️ WOETransformer not available\")\n    X_train_woe = X_train_selected\n    X_val_woe = X_val_selected\n    X_test_woe = X_test_selected"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataProcessor\n",
    "if 'DataProcessor' in globals():\n",
    "    processor = DataProcessor(config)\n",
    "    df_processed = processor.validate_and_freeze(df.copy())\n",
    "    \n",
    "    print(\"✅ Data processing completed!\")\n",
    "    print(f\"Processed shape: {df_processed.shape}\")\n",
    "    print(f\"\\nColumns after processing: {df_processed.shape[1]}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_after = df_processed.isnull().sum().sum()\n",
    "    print(f\"Missing values after processing: {missing_after}\")\n",
    "else:\n",
    "    print(\"⚠️ DataProcessor not available\")\n",
    "    df_processed = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataSplitter\n",
    "if 'DataSplitter' in globals():\n",
    "    splitter = DataSplitter(config)\n",
    "    splits = splitter.split(df_processed)\n",
    "    \n",
    "    print(\"✅ Data splitting completed!\")\n",
    "    print(f\"\\nSplit sizes:\")\n",
    "    print(f\"  Train: {len(splits['train'])} ({len(splits['train'])/len(df_processed):.1%})\")\n",
    "    print(f\"  Validation: {len(splits['validation'])} ({len(splits['validation'])/len(df_processed):.1%})\")\n",
    "    print(f\"  Test: {len(splits['test'])} ({len(splits['test'])/len(df_processed):.1%})\")\n",
    "    \n",
    "    print(f\"\\nTarget rates:\")\n",
    "    print(f\"  Train: {splits['train']['target'].mean():.2%}\")\n",
    "    print(f\"  Validation: {splits['validation']['target'].mean():.2%}\")\n",
    "    print(f\"  Test: {splits['test']['target'].mean():.2%}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train = splits['train'].drop(columns=['target', 'customer_id'])\n",
    "    y_train = splits['train']['target']\n",
    "    X_val = splits['validation'].drop(columns=['target', 'customer_id'])\n",
    "    y_val = splits['validation']['target']\n",
    "    X_test = splits['test'].drop(columns=['target', 'customer_id'])\n",
    "    y_test = splits['test']['target']\n",
    "else:\n",
    "    print(\"⚠️ DataSplitter not available, using sklearn\")\n",
    "    # Manual split\n",
    "    X = df_processed.drop(columns=['target', 'customer_id'])\n",
    "    y = df_processed['target']\n",
    "    \n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.125, random_state=RANDOM_STATE, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FeatureEngineer\n",
    "if 'FeatureEngineer' in globals():\n",
    "    engineer = FeatureEngineer(config)\n",
    "    \n",
    "    X_train_eng = engineer.create_features(X_train)\n",
    "    X_val_eng = engineer.transform(X_val)\n",
    "    X_test_eng = engineer.transform(X_test)\n",
    "    \n",
    "    print(\"✅ Feature engineering completed!\")\n",
    "    print(f\"\\nFeature counts:\")\n",
    "    print(f\"  Original: {X_train.shape[1]}\")\n",
    "    print(f\"  After engineering: {X_train_eng.shape[1]}\")\n",
    "    print(f\"  New features: {X_train_eng.shape[1] - X_train.shape[1]}\")\n",
    "else:\n",
    "    print(\"⚠️ FeatureEngineer not available\")\n",
    "    X_train_eng = X_train\n",
    "    X_val_eng = X_val\n",
    "    X_test_eng = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FeatureSelector\n",
    "if 'FeatureSelector' in globals():\n",
    "    selector = FeatureSelector(config)\n",
    "    selected_features = selector.select_features(X_train_eng, y_train)\n",
    "    \n",
    "    print(\"✅ Feature selection completed!\")\n",
    "    print(f\"\\nSelected {len(selected_features)} features from {X_train_eng.shape[1]}\")\n",
    "    \n",
    "    # Apply selection\n",
    "    X_train_selected = X_train_eng[selected_features]\n",
    "    X_val_selected = X_val_eng[selected_features]\n",
    "    X_test_selected = X_test_eng[selected_features]\n",
    "    \n",
    "    print(f\"\\nTop 10 selected features:\")\n",
    "    print(selected_features[:10].tolist() if hasattr(selected_features, 'tolist') else selected_features[:10])\n",
    "else:\n",
    "    print(\"⚠️ FeatureSelector not available\")\n",
    "    # Select all features\n",
    "    selected_features = X_train_eng.columns.tolist()\n",
    "    X_train_selected = X_train_eng\n",
    "    X_val_selected = X_val_eng\n",
    "    X_test_selected = X_test_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test WOE Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test WOETransformer\n",
    "if 'WOETransformer' in globals():\n",
    "    woe_transformer = WOETransformer(config)\n",
    "    \n",
    "    X_train_woe = woe_transformer.fit_transform(X_train_selected, y_train)\n",
    "    X_val_woe = woe_transformer.transform(X_val_selected)\n",
    "    X_test_woe = woe_transformer.transform(X_test_selected)\n",
    "    \n",
    "    print(\"✅ WOE transformation completed!\")\n",
    "    print(f\"\\nTransformed shape: {X_train_woe.shape}\")\n",
    "    \n",
    "    # Show sample WOE values\n",
    "    if hasattr(woe_transformer, 'woe_mapping_') and woe_transformer.woe_mapping_:\n",
    "        sample_var = list(woe_transformer.woe_mapping_.keys())[0]\n",
    "        print(f\"\\nSample WOE mapping for '{sample_var}':\")\n",
    "        print(woe_transformer.woe_mapping_[sample_var].head())\n",
    "else:\n",
    "    print(\"⚠️ WOETransformer not available\")\n",
    "    X_train_woe = X_train_selected\n",
    "    X_val_woe = X_val_selected\n",
    "    X_test_woe = X_test_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "models = {\n",
    "    'logistic_regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'decision_tree': DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=5),\n",
    "    'random_forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "if HAS_XGBOOST:\n",
    "    models['xgboost'] = xgb.XGBClassifier(\n",
    "        n_estimators=100, max_depth=5, \n",
    "        random_state=RANDOM_STATE, eval_metric='logloss'\n",
    "    )\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "best_model = None\n",
    "best_score = 0\n",
    "best_model_name = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_woe, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict_proba(X_train_woe)[:, 1]\n",
    "    y_pred_val = model.predict_proba(X_val_woe)[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test_woe)[:, 1]\n",
    "    \n",
    "    # Calculate scores\n",
    "    train_score = roc_auc_score(y_train, y_pred_train)\n",
    "    val_score = roc_auc_score(y_val, y_pred_val)\n",
    "    test_score = roc_auc_score(y_test, y_pred_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_score': train_score,\n",
    "        'val_score': val_score,\n",
    "        'test_score': test_score,\n",
    "        'y_pred_test': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train AUC: {train_score:.4f}\")\n",
    "    print(f\"  Val AUC: {val_score:.4f}\")\n",
    "    print(f\"  Test AUC: {test_score:.4f}\")\n",
    "    \n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Best Model: {best_model_name} (Val AUC: {best_score:.4f})\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model predictions\n",
    "y_pred_proba = results[best_model_name]['y_pred_test']\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'auc': roc_auc_score(y_test, y_pred_proba),\n",
    "    'gini': 2 * roc_auc_score(y_test, y_pred_proba) - 1,\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'f1': f1_score(y_test, y_pred)\n",
    "}\n",
    "\n",
    "print(\"Performance Metrics:\")\n",
    "print(\"=\"*40)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric:10s}: {value:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Pred 0', 'Pred 1']))\n",
    "\n",
    "# Model comparison\n",
    "print(\"\\nModel Comparison:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train AUC': [r['train_score'] for r in results.values()],\n",
    "    'Val AUC': [r['val_score'] for r in results.values()],\n",
    "    'Test AUC': [r['test_score'] for r in results.values()],\n",
    "    'Overfit': [r['train_score'] - r['test_score'] for r in results.values()]\n",
    "})\n",
    "print(comparison_df.sort_values('Val AUC', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# ROC Curve\n",
    "ax = axes[0, 0]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "ax.plot(fpr, tpr, label=f'AUC = {metrics[\"auc\"]:.3f}')\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Score Distribution\n",
    "ax = axes[0, 1]\n",
    "ax.hist(y_pred_proba[y_test == 0], bins=30, alpha=0.5, label='Class 0', color='blue')\n",
    "ax.hist(y_pred_proba[y_test == 1], bins=30, alpha=0.5, label='Class 1', color='red')\n",
    "ax.set_xlabel('Predicted Probability')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Score Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "ax = axes[1, 0]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Precision-Recall Curve\n",
    "ax = axes[1, 1]\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "ax.plot(recall, precision)\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision-Recall Curve')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Performance plots generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Test PSI Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PSICalculator\n",
    "if 'PSICalculator' in globals():\n",
    "    psi_calculator = PSICalculator()\n",
    "    \n",
    "    # Calculate score PSI\n",
    "    y_train_pred = best_model.predict_proba(X_train_woe)[:, 1]\n",
    "    score_psi = psi_calculator.calculate(y_train_pred, y_pred_proba)\n",
    "    \n",
    "    print(\"✅ PSI Analysis completed!\")\n",
    "    print(f\"\\nScore PSI (Train vs Test): {score_psi:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if score_psi < 0.1:\n",
    "        print(\"  ✅ Model is stable (PSI < 0.1)\")\n",
    "    elif score_psi < 0.25:\n",
    "        print(\"  ⚠️ Minor shift detected (0.1 <= PSI < 0.25)\")\n",
    "    else:\n",
    "        print(\"  ❌ Significant shift detected (PSI >= 0.25)\")\n",
    "    \n",
    "    # Feature PSI for top features\n",
    "    print(\"\\nFeature PSI (Top 5 features):\")\n",
    "    for i, col in enumerate(X_train_woe.columns[:5]):\n",
    "        feature_psi = psi_calculator.calculate(X_train_woe[col], X_test_woe[col])\n",
    "        status = \"✅\" if feature_psi < 0.1 else \"⚠️\" if feature_psi < 0.25 else \"❌\"\n",
    "        print(f\"  {col}: {feature_psi:.4f} {status}\")\n",
    "else:\n",
    "    print(\"⚠️ PSICalculator not available\")\n",
    "    score_psi = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Test Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CalibrationAnalyzer\n",
    "if 'CalibrationAnalyzer' in globals():\n",
    "    calibration_analyzer = CalibrationAnalyzer()\n",
    "    \n",
    "    # Analyze calibration\n",
    "    cal_results = calibration_analyzer.analyze_calibration(y_test, y_pred_proba)\n",
    "    \n",
    "    print(\"✅ Calibration analysis completed!\")\n",
    "    print(\"\\nCalibration Metrics:\")\n",
    "    print(f\"  Expected Calibration Error (ECE): {cal_results['ece']:.4f}\")\n",
    "    print(f\"  Maximum Calibration Error (MCE): {cal_results['mce']:.4f}\")\n",
    "    print(f\"  Brier Score: {cal_results['brier_score']:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if cal_results['ece'] < 0.05:\n",
    "        print(\"\\n✅ Model is well calibrated (ECE < 0.05)\")\n",
    "    elif cal_results['ece'] < 0.1:\n",
    "        print(\"\\n⚠️ Model has minor calibration issues (0.05 <= ECE < 0.1)\")\n",
    "    else:\n",
    "        print(\"\\n❌ Model needs calibration (ECE >= 0.1)\")\n",
    "    \n",
    "    # Calibration plot\n",
    "    if hasattr(calibration_analyzer, 'plot_calibration'):\n",
    "        try:\n",
    "            fig = calibration_analyzer.plot_calibration(y_test, y_pred_proba)\n",
    "            plt.show()\n",
    "        except:\n",
    "            print(\"Could not generate calibration plot\")\n",
    "else:\n",
    "    print(\"⚠️ CalibrationAnalyzer not available\")\n",
    "    cal_results = {'ece': 0.0, 'mce': 0.0, 'brier_score': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Test Risk Band Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RiskBandOptimizer\n",
    "if 'RiskBandOptimizer' in globals():\n",
    "    risk_band_optimizer = RiskBandOptimizer()\n",
    "    \n",
    "    # Create risk bands\n",
    "    risk_bands = risk_band_optimizer.optimize_bands(\n",
    "        y_true=y_test,\n",
    "        y_scores=y_pred_proba,\n",
    "        n_bands=5,\n",
    "        method='quantile'\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Risk band optimization completed!\")\n",
    "    print(\"\\nRisk Bands:\")\n",
    "    print(risk_bands[['band', 'min_score', 'max_score', 'count', 'bad_rate', 'volume_pct']])\n",
    "    \n",
    "    # Check monotonicity\n",
    "    is_monotonic = all(risk_bands['bad_rate'].iloc[i] <= risk_bands['bad_rate'].iloc[i+1] \n",
    "                      for i in range(len(risk_bands)-1))\n",
    "    print(f\"\\nRisk bands are {'✅ monotonic' if is_monotonic else '❌ not monotonic'}\")\n",
    "    \n",
    "    # Visualize risk bands\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Bad rate by band\n",
    "    ax = axes[0]\n",
    "    ax.bar(risk_bands['band'], risk_bands['bad_rate'], color='coral')\n",
    "    ax.set_xlabel('Risk Band')\n",
    "    ax.set_ylabel('Bad Rate')\n",
    "    ax.set_title('Bad Rate by Risk Band')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Volume distribution\n",
    "    ax = axes[1]\n",
    "    ax.bar(risk_bands['band'], risk_bands['volume_pct'], color='skyblue')\n",
    "    ax.set_xlabel('Risk Band')\n",
    "    ax.set_ylabel('Volume %')\n",
    "    ax.set_title('Volume Distribution')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ RiskBandOptimizer not available\")\n",
    "    risk_bands = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Test Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete pipeline if available\n",
    "if PIPELINE_CLASS:\n",
    "    print(f\"Testing {PIPELINE_CLASS.__name__}...\\n\")\n",
    "    \n",
    "    # Create fresh dataset\n",
    "    X_pipe, y_pipe = make_classification(\n",
    "        n_samples=5000, n_features=25, n_informative=18,\n",
    "        n_redundant=5, n_classes=2, weights=[0.8, 0.2],\n",
    "        random_state=RANDOM_STATE+1\n",
    "    )\n",
    "    \n",
    "    df_pipeline = pd.DataFrame(X_pipe, columns=[f'var_{i:02d}' for i in range(X_pipe.shape[1])])\n",
    "    df_pipeline['target'] = y_pipe\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = PIPELINE_CLASS(config)\n",
    "    \n",
    "    try:\n",
    "        # Fit pipeline\n",
    "        pipeline.fit(df_pipeline)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = pipeline.predict(df_pipeline)\n",
    "        probabilities = pipeline.predict_proba(df_pipeline)\n",
    "        \n",
    "        # Evaluate\n",
    "        pipeline_score = roc_auc_score(y_pipe, probabilities[:, 1])\n",
    "        \n",
    "        print(f\"✅ Pipeline test successful!\")\n",
    "        print(f\"Pipeline AUC: {pipeline_score:.4f}\")\n",
    "        \n",
    "        # Save pipeline\n",
    "        os.makedirs(config.output_folder, exist_ok=True)\n",
    "        pipeline_path = os.path.join(config.output_folder, 'complete_pipeline.pkl')\n",
    "        joblib.dump(pipeline, pipeline_path)\n",
    "        print(f\"Pipeline saved to: {pipeline_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Pipeline test failed: {e}\")\n",
    "else:\n",
    "    print(\"⚠️ No pipeline class available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPLETE PIPELINE TEST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n📦 Package: risk-model-pipeline (development branch)\")\n",
    "print(f\"⏰ Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\n✅ MODULES TESTED:\")\n",
    "for module, status in modules_status.items():\n",
    "    if '✅' in str(status):\n",
    "        print(f\"  ✓ {module}\")\n",
    "\n",
    "print(\"\\n❌ MODULES NOT AVAILABLE:\")\n",
    "for module, status in modules_status.items():\n",
    "    if '❌' in str(status):\n",
    "        print(f\"  ✗ {module}\")\n",
    "\n",
    "print(\"\\n📊 BEST MODEL RESULTS:\")\n",
    "print(f\"  Model: {best_model_name}\")\n",
    "print(f\"  Test AUC: {results[best_model_name]['test_score']:.4f}\")\n",
    "print(f\"  Gini: {metrics['gini']:.4f}\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "if 'PSICalculator' in globals():\n",
    "    print(f\"\\n📈 STABILITY METRICS:\")\n",
    "    print(f\"  PSI: {score_psi:.4f}\")\n",
    "\n",
    "if 'CalibrationAnalyzer' in globals():\n",
    "    print(f\"  ECE: {cal_results['ece']:.4f}\")\n",
    "\n",
    "if 'RiskBandOptimizer' in globals() and not risk_bands.empty:\n",
    "    print(f\"  Risk Bands: {len(risk_bands)} bands\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎉 TEST COMPLETED SUCCESSFULLY! 🎉\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}