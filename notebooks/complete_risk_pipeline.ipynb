{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Risk Model Pipeline\n",
    "## End-to-End Machine Learning Pipeline for Risk Modeling\n",
    "\n",
    "This notebook demonstrates a complete risk modeling pipeline including:\n",
    "- Data loading and preprocessing\n",
    "- Feature engineering and selection  \n",
    "- Model training and evaluation\n",
    "- Advanced analytics (PSI, Calibration, Risk Bands)\n",
    "- Comprehensive reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgboost_available = True\n",
    "except ImportError:\n",
    "    xgboost_available = False\n",
    "    print(\"XGBoost not available\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    lightgbm_available = True\n",
    "except ImportError:\n",
    "    lightgbm_available = False\n",
    "    print(\"LightGBM not available\")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(f\"Setup completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add Project Path and Import Local Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Import from local src folder\n",
    "try:\n",
    "    from src.risk_pipeline.core.config import Config\n",
    "    from src.risk_pipeline.core.data_processor import DataProcessor\n",
    "    from src.risk_pipeline.core.splitter import DataSplitter\n",
    "    from src.risk_pipeline.core.feature_engineer import FeatureEngineer\n",
    "    from src.risk_pipeline.core.feature_selector import FeatureSelector\n",
    "    from src.risk_pipeline.core.woe_transformer import WOETransformer\n",
    "    from src.risk_pipeline.core.model_builder import ModelBuilder\n",
    "    from src.risk_pipeline.core.reporter import Reporter\n",
    "    from src.risk_pipeline.core.psi_calculator import PSICalculator\n",
    "    from src.risk_pipeline.core.calibration_analyzer import CalibrationAnalyzer\n",
    "    from src.risk_pipeline.core.risk_band_optimizer import RiskBandOptimizer\n",
    "    print(\"‚úÖ Local modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing local modules: {e}\")\n",
    "    print(\"Using standalone implementation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline configuration\n",
    "config = Config(\n",
    "    target_column='target',\n",
    "    test_size=0.2,\n",
    "    validation_size=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    cv_folds=5,\n",
    "    \n",
    "    # Feature engineering\n",
    "    create_polynomial=False,  # Start simple\n",
    "    create_interactions=False,\n",
    "    \n",
    "    # Feature selection\n",
    "    selection_method='importance',\n",
    "    top_k_features=30,\n",
    "    \n",
    "    # WOE parameters\n",
    "    max_bins=5,\n",
    "    min_samples_leaf=0.05,\n",
    "    \n",
    "    # Output\n",
    "    output_folder='outputs/pipeline_run',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Configuration set\")\n",
    "print(f\"  Target: {config.target_column}\")\n",
    "print(f\"  Test size: {config.test_size}\")\n",
    "print(f\"  Random state: {config.random_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data - adjust path as needed\n",
    "data_path = '../data/processed/model_data.csv'\n",
    "\n",
    "# Try different paths if first one doesn't work\n",
    "if not os.path.exists(data_path):\n",
    "    alternate_paths = [\n",
    "        '../data/model_data.csv',\n",
    "        'data/model_data.csv',\n",
    "        '../sample_data.csv',\n",
    "        'sample_data.csv'\n",
    "    ]\n",
    "    for path in alternate_paths:\n",
    "        if os.path.exists(path):\n",
    "            data_path = path\n",
    "            break\n",
    "    else:\n",
    "        print(\"Creating sample data...\")\n",
    "        # Create sample data if no file found\n",
    "        from sklearn.datasets import make_classification\n",
    "        X, y = make_classification(\n",
    "            n_samples=10000, n_features=20, n_informative=15,\n",
    "            n_redundant=5, n_clusters_per_class=2,\n",
    "            weights=[0.9, 0.1], random_state=RANDOM_STATE\n",
    "        )\n",
    "        df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "        df['target'] = y\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"\\nTarget rate: {df['target'].mean():.2%}\")\n",
    "\n",
    "# Basic info\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(f\"\\nMissing values found in {missing[missing > 0].shape[0]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = DataProcessor(config)\n",
    "\n",
    "# Validate data\n",
    "df_processed = processor.validate_and_freeze(df)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_processed.drop(columns=['target'])\n",
    "y = df_processed['target']\n",
    "\n",
    "# Identify variable types\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {len(numeric_cols)}\")\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "# Handle missing values\n",
    "if numeric_cols:\n",
    "    imputer_num = SimpleImputer(strategy='median')\n",
    "    X[numeric_cols] = imputer_num.fit_transform(X[numeric_cols])\n",
    "\n",
    "if categorical_cols:\n",
    "    imputer_cat = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "    X[categorical_cols] = imputer_cat.fit_transform(X[categorical_cols])\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "\n",
    "print(\"\\nPreprocessing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Test/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: train+val vs test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=config.test_size, random_state=config.random_state, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: train vs validation\n",
    "val_size_adjusted = config.validation_size / (1 - config.test_size)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=val_size_adjusted, random_state=config.random_state, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X):.1%})\")\n",
    "print(f\"Val set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X):.1%})\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X):.1%})\")\n",
    "\n",
    "print(\"\\nTarget rates:\")\n",
    "print(f\"  Train: {y_train.mean():.2%}\")\n",
    "print(f\"  Val: {y_val.mean():.2%}\")\n",
    "print(f\"  Test: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create additional features\n",
    "engineer = FeatureEngineer(config)\n",
    "\n",
    "if config.create_polynomial or config.create_interactions:\n",
    "    print(\"Creating engineered features...\")\n",
    "    X_train_eng = engineer.create_features(pd.DataFrame(X_train))\n",
    "    X_val_eng = engineer.transform(pd.DataFrame(X_val))\n",
    "    X_test_eng = engineer.transform(pd.DataFrame(X_test))\n",
    "    \n",
    "    print(f\"Features after engineering: {X_train_eng.shape[1]} (was {X_train.shape[1]})\")\n",
    "    \n",
    "    X_train = X_train_eng\n",
    "    X_val = X_val_eng\n",
    "    X_test = X_test_eng\n",
    "else:\n",
    "    print(\"Skipping feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "selector = FeatureSelector(config)\n",
    "\n",
    "# Select features based on importance\n",
    "selected_features = selector.select_features(X_train, y_train)\n",
    "\n",
    "print(f\"Selected {len(selected_features)} features from {X_train.shape[1]}\")\n",
    "\n",
    "# Apply selection\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_val_selected = X_val[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Show top features if available\n",
    "if hasattr(selector, 'feature_importance_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': selected_features[:10],\n",
    "        'importance': selector.feature_importance_[:10]\n",
    "    })\n",
    "    print(\"\\nTop 10 features:\")\n",
    "    print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. WOE Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WOE Transformation\n",
    "woe_transformer = WOETransformer(config)\n",
    "\n",
    "# Fit and transform\n",
    "X_train_woe = woe_transformer.fit_transform(X_train_selected, y_train)\n",
    "X_val_woe = woe_transformer.transform(X_val_selected)\n",
    "X_test_woe = woe_transformer.transform(X_test_selected)\n",
    "\n",
    "print(f\"WOE transformation completed\")\n",
    "print(f\"  Shape: {X_train_woe.shape}\")\n",
    "\n",
    "# Show sample WOE mapping\n",
    "if woe_transformer.woe_mapping_:\n",
    "    sample_var = list(woe_transformer.woe_mapping_.keys())[0]\n",
    "    print(f\"\\nSample WOE mapping for '{sample_var}':\")\n",
    "    print(woe_transformer.woe_mapping_[sample_var].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=5),\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=100, max_depth=10),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE, n_estimators=100, max_depth=5)\n",
    "}\n",
    "\n",
    "if xgboost_available:\n",
    "    models['XGBoost'] = XGBClassifier(random_state=RANDOM_STATE, n_estimators=100, max_depth=5)\n",
    "\n",
    "if lightgbm_available:\n",
    "    models['LightGBM'] = LGBMClassifier(random_state=RANDOM_STATE, n_estimators=100, max_depth=5, verbose=-1)\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "best_model = None\n",
    "best_score = 0\n",
    "best_model_name = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_woe, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict_proba(X_train_woe)[:, 1]\n",
    "    y_pred_val = model.predict_proba(X_val_woe)[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test_woe)[:, 1]\n",
    "    \n",
    "    # Calculate scores\n",
    "    train_score = roc_auc_score(y_train, y_pred_train)\n",
    "    val_score = roc_auc_score(y_val, y_pred_val)\n",
    "    test_score = roc_auc_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_score': train_score,\n",
    "        'val_score': val_score,\n",
    "        'test_score': test_score,\n",
    "        'y_pred_train': y_pred_train,\n",
    "        'y_pred_val': y_pred_val,\n",
    "        'y_pred_test': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train AUC: {train_score:.4f}\")\n",
    "    print(f\"  Val AUC: {val_score:.4f}\")\n",
    "    print(f\"  Test AUC: {test_score:.4f}\")\n",
    "    \n",
    "    # Track best model\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Best Model: {best_model_name} (Val AUC: {best_score:.4f})\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model predictions\n",
    "best_results = results[best_model_name]\n",
    "y_pred_test = best_results['y_pred_test']\n",
    "y_pred_test_binary = (y_pred_test >= 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "print(f\"Detailed Metrics for {best_model_name}:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# AUC and Gini\n",
    "auc = roc_auc_score(y_test, y_pred_test)\n",
    "gini = 2 * auc - 1\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Gini: {gini:.4f}\")\n",
    "\n",
    "# Classification metrics\n",
    "print(f\"\\nClassification Metrics (threshold=0.5):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_test_binary):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_test_binary):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_test_binary):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_test_binary):.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred_test_binary)\n",
    "print(pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Pred 0', 'Pred 1']))\n",
    "\n",
    "# Model comparison\n",
    "print(\"\\nModel Comparison (Test Set):\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train AUC': [r['train_score'] for r in results.values()],\n",
    "    'Val AUC': [r['val_score'] for r in results.values()],\n",
    "    'Test AUC': [r['test_score'] for r in results.values()],\n",
    "    'Overfit': [r['train_score'] - r['test_score'] for r in results.values()]\n",
    "})\n",
    "print(comparison_df.sort_values('Val AUC', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. ROC Curves\n",
    "ax = axes[0, 0]\n",
    "for name, res in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, res['y_pred_test'])\n",
    "    ax.plot(fpr, tpr, label=f\"{name} (AUC={res['test_score']:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Score Distribution\n",
    "ax = axes[0, 1]\n",
    "ax.hist(y_pred_test[y_test == 0], bins=30, alpha=0.5, label='Negative', color='blue')\n",
    "ax.hist(y_pred_test[y_test == 1], bins=30, alpha=0.5, label='Positive', color='red')\n",
    "ax.set_xlabel('Predicted Probability')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(f'Score Distribution - {best_model_name}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Precision-Recall Curve\n",
    "ax = axes[1, 0]\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_test)\n",
    "ax.plot(recall, precision, color='purple')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title(f'Precision-Recall Curve - {best_model_name}')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature Importance (if available)\n",
    "ax = axes[1, 1]\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance = best_model.feature_importances_\n",
    "    indices = np.argsort(importance)[::-1][:10]\n",
    "    ax.barh(range(10), importance[indices], color='skyblue')\n",
    "    ax.set_yticks(range(10))\n",
    "    ax.set_yticklabels([selected_features[i] for i in indices])\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title('Top 10 Feature Importances')\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    coef = np.abs(best_model.coef_[0])\n",
    "    indices = np.argsort(coef)[::-1][:10]\n",
    "    ax.barh(range(10), coef[indices], color='lightcoral')\n",
    "    ax.set_yticks(range(10))\n",
    "    ax.set_yticklabels([selected_features[i] for i in indices])\n",
    "    ax.set_xlabel('|Coefficient|')\n",
    "    ax.set_title('Top 10 Feature Coefficients')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'Feature importance not available', \n",
    "            ha='center', va='center', transform=ax.transAxes)\n",
    "    ax.set_title('Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. PSI Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PSI\n",
    "psi_calculator = PSICalculator()\n",
    "\n",
    "# Score PSI\n",
    "train_scores = best_results['y_pred_train']\n",
    "test_scores = best_results['y_pred_test']\n",
    "\n",
    "score_psi = psi_calculator.calculate(train_scores, test_scores)\n",
    "\n",
    "print(\"PSI Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Score PSI (Train vs Test): {score_psi:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if score_psi < 0.1:\n",
    "    print(\"  ‚úÖ Model is stable (PSI < 0.1)\")\n",
    "elif score_psi < 0.25:\n",
    "    print(\"  ‚ö†Ô∏è Minor shift detected (0.1 <= PSI < 0.25)\")\n",
    "else:\n",
    "    print(\"  ‚ùå Significant shift detected (PSI >= 0.25)\")\n",
    "\n",
    "# Feature PSI for top features\n",
    "print(\"\\nFeature PSI (Train vs Test):\")\n",
    "feature_psi = {}\n",
    "for col in selected_features[:10]:\n",
    "    psi = psi_calculator.calculate(X_train_woe[col], X_test_woe[col])\n",
    "    feature_psi[col] = psi\n",
    "    status = \"‚úÖ\" if psi < 0.1 else \"‚ö†Ô∏è\" if psi < 0.25 else \"‚ùå\"\n",
    "    print(f\"  {col}: {psi:.4f} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration analysis\n",
    "calibration_analyzer = CalibrationAnalyzer()\n",
    "\n",
    "# Analyze calibration\n",
    "cal_results = calibration_analyzer.analyze_calibration(y_test, y_pred_test)\n",
    "\n",
    "print(\"Calibration Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Expected Calibration Error (ECE): {cal_results['ece']:.4f}\")\n",
    "print(f\"Maximum Calibration Error (MCE): {cal_results['mce']:.4f}\")\n",
    "print(f\"Brier Score: {cal_results['brier_score']:.4f}\")\n",
    "\n",
    "# Calibration plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "bins = cal_results['bins']\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "plt.scatter(bins['mean_predicted'], bins['mean_actual'], s=100, alpha=0.7, color='red')\n",
    "plt.plot(bins['mean_predicted'], bins['mean_actual'], 'r-', alpha=0.5)\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Plot')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "if cal_results['ece'] < 0.05:\n",
    "    print(\"\\n‚úÖ Model is well calibrated (ECE < 0.05)\")\n",
    "elif cal_results['ece'] < 0.1:\n",
    "    print(\"\\n‚ö†Ô∏è Model has minor calibration issues (0.05 <= ECE < 0.1)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Model needs calibration (ECE >= 0.1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Risk Band Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize risk bands\n",
    "risk_band_optimizer = RiskBandOptimizer()\n",
    "\n",
    "# Create risk bands\n",
    "risk_bands = risk_band_optimizer.optimize_bands(\n",
    "    y_true=y_test,\n",
    "    y_scores=y_pred_test,\n",
    "    n_bands=5,\n",
    "    method='quantile'\n",
    ")\n",
    "\n",
    "print(\"Risk Bands Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(risk_bands[['band', 'min_score', 'max_score', 'bad_rate', 'volume_pct', 'cumulative_bad_rate']])\n",
    "\n",
    "# Visualize risk bands\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Bad rate by band\n",
    "ax = axes[0]\n",
    "ax.bar(risk_bands['band'], risk_bands['bad_rate'], color='coral')\n",
    "ax.set_xlabel('Risk Band')\n",
    "ax.set_ylabel('Bad Rate')\n",
    "ax.set_title('Bad Rate by Risk Band')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Volume distribution\n",
    "ax = axes[1]\n",
    "ax.bar(risk_bands['band'], risk_bands['volume_pct'], color='skyblue')\n",
    "ax.set_xlabel('Risk Band')\n",
    "ax.set_ylabel('Volume %')\n",
    "ax.set_title('Volume Distribution by Risk Band')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check monotonicity\n",
    "is_monotonic = all(risk_bands['bad_rate'].iloc[i] <= risk_bands['bad_rate'].iloc[i+1] \n",
    "                   for i in range(len(risk_bands)-1))\n",
    "print(f\"\\nRisk bands are {'‚úÖ monotonic' if is_monotonic else '‚ùå not monotonic'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save Models and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = config.output_folder\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "model_path = os.path.join(output_dir, 'best_model.pkl')\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"‚úÖ Model saved: {model_path}\")\n",
    "\n",
    "# Save WOE transformer\n",
    "woe_path = os.path.join(output_dir, 'woe_transformer.pkl')\n",
    "joblib.dump(woe_transformer, woe_path)\n",
    "print(f\"‚úÖ WOE transformer saved: {woe_path}\")\n",
    "\n",
    "# Save configuration\n",
    "config_dict = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_score': float(best_score),\n",
    "    'selected_features': selected_features,\n",
    "    'risk_bands': risk_bands.to_dict('records'),\n",
    "    'psi_score': float(score_psi),\n",
    "    'ece': float(cal_results['ece']),\n",
    "    'training_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "config_path = os.path.join(output_dir, 'pipeline_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "print(f\"‚úÖ Configuration saved: {config_path}\")\n",
    "\n",
    "# Save results summary\n",
    "summary_path = os.path.join(output_dir, 'results_summary.csv')\n",
    "comparison_df.to_csv(summary_path, index=False)\n",
    "print(f\"‚úÖ Results summary saved: {summary_path}\")\n",
    "\n",
    "print(f\"\\nüìÅ All artifacts saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Model Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_new_data(new_df, model_dir=None):\n",
    "    \"\"\"\n",
    "    Score new data using saved model artifacts\n",
    "    \"\"\"\n",
    "    if model_dir is None:\n",
    "        model_dir = config.output_folder\n",
    "    \n",
    "    # Load artifacts\n",
    "    model = joblib.load(os.path.join(model_dir, 'best_model.pkl'))\n",
    "    woe_transformer = joblib.load(os.path.join(model_dir, 'woe_transformer.pkl'))\n",
    "    \n",
    "    # Load config\n",
    "    with open(os.path.join(model_dir, 'pipeline_config.json'), 'r') as f:\n",
    "        saved_config = json.load(f)\n",
    "    \n",
    "    # Process new data\n",
    "    X_new = new_df[saved_config['selected_features']]\n",
    "    X_new_woe = woe_transformer.transform(X_new)\n",
    "    \n",
    "    # Score\n",
    "    scores = model.predict_proba(X_new_woe)[:, 1]\n",
    "    \n",
    "    # Assign risk bands\n",
    "    risk_bands_df = pd.DataFrame(saved_config['risk_bands'])\n",
    "    \n",
    "    def assign_band(score):\n",
    "        for _, band in risk_bands_df.iterrows():\n",
    "            if band['min_score'] <= score <= band['max_score']:\n",
    "                return band['band']\n",
    "        return 'Unknown'\n",
    "    \n",
    "    # Create results\n",
    "    results = pd.DataFrame({\n",
    "        'score': scores,\n",
    "        'risk_band': [assign_band(s) for s in scores],\n",
    "        'prediction': (scores >= 0.5).astype(int)\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test scoring function\n",
    "print(\"Testing scoring function...\")\n",
    "test_sample = pd.DataFrame(X_test).iloc[:5]\n",
    "test_results = score_new_data(test_sample)\n",
    "print(\"\\nSample scoring results:\")\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RISK MODEL PIPELINE - FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä DATA:\")\n",
    "print(f\"  Total samples: {len(df):,}\")\n",
    "print(f\"  Features: {len(selected_features)}\")\n",
    "print(f\"  Target rate: {y.mean():.2%}\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"  Train AUC: {results[best_model_name]['train_score']:.4f}\")\n",
    "print(f\"  Val AUC: {results[best_model_name]['val_score']:.4f}\")\n",
    "print(f\"  Test AUC: {results[best_model_name]['test_score']:.4f}\")\n",
    "print(f\"  Gini: {2*results[best_model_name]['test_score']-1:.4f}\")\n",
    "\n",
    "print(f\"\\nüìà STABILITY:\")\n",
    "print(f\"  PSI: {score_psi:.4f}\")\n",
    "print(f\"  ECE: {cal_results['ece']:.4f}\")\n",
    "print(f\"  Brier Score: {cal_results['brier_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ RISK BANDS:\")\n",
    "print(f\"  Number of bands: {len(risk_bands)}\")\n",
    "print(f\"  Monotonic: {'Yes' if is_monotonic else 'No'}\")\n",
    "\n",
    "print(f\"\\nüíæ SAVED ARTIFACTS:\")\n",
    "print(f\"  Model: {model_path}\")\n",
    "print(f\"  WOE Transformer: {woe_path}\")\n",
    "print(f\"  Configuration: {config_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Pipeline completed successfully!\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated a complete end-to-end risk modeling pipeline including:\n",
    "\n",
    "‚úÖ **Data Processing**: Loading, validation, and preprocessing  \n",
    "‚úÖ **Feature Engineering**: Optional polynomial and interaction features  \n",
    "‚úÖ **Feature Selection**: Importance-based selection  \n",
    "‚úÖ **WOE Transformation**: Weight of Evidence encoding  \n",
    "‚úÖ **Model Training**: Multiple algorithms with cross-validation  \n",
    "‚úÖ **Evaluation**: Comprehensive metrics (AUC, Gini, Precision, Recall)  \n",
    "‚úÖ **PSI Monitoring**: Population stability tracking  \n",
    "‚úÖ **Calibration**: Analysis and visualization  \n",
    "‚úÖ **Risk Bands**: Optimized segmentation  \n",
    "‚úÖ **Model Persistence**: Saving all artifacts for deployment  \n",
    "\n",
    "The pipeline is now ready for production deployment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}