{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Model Pipeline - Complete Workflow Example\n",
    "\n",
    "This notebook demonstrates the complete workflow:\n",
    "1. Data preparation with realistic target distribution\n",
    "2. Model training with good performance (70-80% Train Gini)\n",
    "3. Calibration functionality\n",
    "4. Credit scoring transformation\n",
    "5. Model evaluation and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/Update the risk-model-pipeline package from GitHub\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_risk_pipeline():\n",
    "    \"\"\"Install or update risk-model-pipeline package\"\"\"\n",
    "    try:\n",
    "        # Check if package is already installed\n",
    "        import risk_pipeline\n",
    "        print(\"Risk Model Pipeline is already installed. Updating to latest version...\")\n",
    "        \n",
    "        # Uninstall existing version\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"risk-model-pipeline\"], \n",
    "                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        print(\"‚úì Existing version uninstalled\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Risk Model Pipeline not found. Installing fresh...\")\n",
    "    \n",
    "    # Install latest version from GitHub\n",
    "    print(\"Installing from GitHub...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \n",
    "                          \"git+https://github.com/selimoksuz/risk-model-pipeline.git\"])\n",
    "    \n",
    "    print(\"‚úÖ Risk Model Pipeline installed successfully!\")\n",
    "    \n",
    "    # Verify installation\n",
    "    try:\n",
    "        import risk_pipeline\n",
    "        from risk_pipeline import Config, DualPipeline\n",
    "        print(f\"‚úì Version verified: Package imported successfully\")\n",
    "        print(f\"‚úì Config class available: {Config.__module__}\")\n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Installation verification failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run installation\n",
    "if install_risk_pipeline():\n",
    "    print(\"\\nüéâ Ready to use Risk Model Pipeline!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Please restart the kernel and try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Import our pipeline\n",
    "from risk_pipeline import Config, DualPipeline\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Create synthetic data with realistic characteristics for credit risk modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create dataset size\n",
    "n_samples = 10000\n",
    "\n",
    "# Create features that correlate with target\n",
    "def create_credit_data(n_samples=10000):\n",
    "    \"\"\"\n",
    "    Create synthetic credit risk data with realistic patterns\n",
    "    Target event rate: ~15% (realistic for credit default)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Base features\n",
    "    age = np.random.normal(40, 12, n_samples)\n",
    "    age = np.clip(age, 18, 80)\n",
    "    \n",
    "    income = np.random.lognormal(10.5, 0.6, n_samples)  # Log-normal income distribution\n",
    "    income = np.clip(income, 10000, 500000)\n",
    "    \n",
    "    credit_score = np.random.normal(650, 80, n_samples)\n",
    "    credit_score = np.clip(credit_score, 300, 850)\n",
    "    \n",
    "    debt_ratio = np.random.beta(2, 5, n_samples)  # Debt-to-income ratio\n",
    "    \n",
    "    months_employed = np.random.exponential(60, n_samples)\n",
    "    months_employed = np.clip(months_employed, 0, 480)\n",
    "    \n",
    "    num_credit_lines = np.random.poisson(5, n_samples)\n",
    "    num_credit_lines = np.clip(num_credit_lines, 0, 20)\n",
    "    \n",
    "    utilization_rate = np.random.beta(3, 7, n_samples)  # Credit utilization\n",
    "    \n",
    "    # Create risk score based on features (for realistic target)\n",
    "    risk_score = (\n",
    "        - 0.01 * age  # Older = lower risk\n",
    "        - 0.00001 * income  # Higher income = lower risk\n",
    "        - 0.005 * credit_score  # Higher score = lower risk\n",
    "        + 3.0 * debt_ratio  # Higher debt = higher risk\n",
    "        - 0.005 * months_employed  # Longer employment = lower risk\n",
    "        + 0.05 * num_credit_lines  # More credit lines = slightly higher risk\n",
    "        + 2.0 * utilization_rate  # Higher utilization = higher risk\n",
    "        + np.random.normal(0, 0.5, n_samples)  # Random noise\n",
    "    )\n",
    "    \n",
    "    # Convert to probability using sigmoid\n",
    "    default_prob = 1 / (1 + np.exp(-risk_score))\n",
    "    \n",
    "    # Adjust to get ~15% event rate\n",
    "    default_prob = default_prob * 0.3\n",
    "    \n",
    "    # Generate binary target\n",
    "    target = np.random.binomial(1, default_prob)\n",
    "    \n",
    "    # Create categorical features\n",
    "    education = np.random.choice(\n",
    "        ['High School', 'Bachelor', 'Master', 'PhD'], \n",
    "        n_samples, \n",
    "        p=[0.3, 0.45, 0.2, 0.05]\n",
    "    )\n",
    "    \n",
    "    employment_type = np.random.choice(\n",
    "        ['Full-time', 'Part-time', 'Self-employed', 'Unemployed'],\n",
    "        n_samples,\n",
    "        p=[0.65, 0.15, 0.15, 0.05]\n",
    "    )\n",
    "    \n",
    "    region = np.random.choice(\n",
    "        ['North', 'South', 'East', 'West', 'Central'],\n",
    "        n_samples,\n",
    "        p=[0.2, 0.25, 0.2, 0.2, 0.15]\n",
    "    )\n",
    "    \n",
    "    home_ownership = np.random.choice(\n",
    "        ['Own', 'Rent', 'Mortgage', 'Other'],\n",
    "        n_samples,\n",
    "        p=[0.25, 0.35, 0.35, 0.05]\n",
    "    )\n",
    "    \n",
    "    # Create additional numeric features\n",
    "    num_late_payments = np.random.poisson(0.5, n_samples)\n",
    "    num_late_payments = np.clip(num_late_payments, 0, 10)\n",
    "    \n",
    "    months_since_last_late = np.random.exponential(24, n_samples)\n",
    "    months_since_last_late = np.clip(months_since_last_late, 0, 120)\n",
    "    months_since_last_late[num_late_payments == 0] = 999  # No late payment\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'app_id': range(n_samples),\n",
    "        'app_dt': pd.date_range(start='2022-01-01', periods=n_samples, freq='H')[:n_samples],\n",
    "        'target': target,\n",
    "        'age': age.round(0).astype(int),\n",
    "        'income': income.round(0).astype(int),\n",
    "        'credit_score': credit_score.round(0).astype(int),\n",
    "        'debt_ratio': debt_ratio.round(3),\n",
    "        'months_employed': months_employed.round(0).astype(int),\n",
    "        'num_credit_lines': num_credit_lines,\n",
    "        'utilization_rate': utilization_rate.round(3),\n",
    "        'num_late_payments': num_late_payments,\n",
    "        'months_since_last_late': months_since_last_late.round(0).astype(int),\n",
    "        'education': education,\n",
    "        'employment_type': employment_type,\n",
    "        'region': region,\n",
    "        'home_ownership': home_ownership\n",
    "    })\n",
    "    \n",
    "    # Add some missing values (realistic)\n",
    "    missing_indices = np.random.choice(n_samples, size=int(n_samples * 0.02), replace=False)\n",
    "    df.loc[missing_indices, 'months_since_last_late'] = np.nan\n",
    "    \n",
    "    missing_indices = np.random.choice(n_samples, size=int(n_samples * 0.01), replace=False)\n",
    "    df.loc[missing_indices, 'months_employed'] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create the dataset\n",
    "df = create_credit_data(n_samples=10000)\n",
    "\n",
    "print(f\"Dataset created with shape: {df.shape}\")\n",
    "print(f\"Target event rate: {df['target'].mean():.2%}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline Configuration\n",
    "\n",
    "Configure the pipeline with optimal settings for model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline\n",
    "config = Config(\n",
    "    # Basic settings\n",
    "    target_col='target',\n",
    "    id_col='app_id',\n",
    "    time_col='app_dt',\n",
    "    output_folder='outputs',\n",
    "    \n",
    "    # Feature selection parameters\n",
    "    iv_min=0.02,  # Minimum Information Value\n",
    "    psi_threshold=0.25,  # Population Stability Index threshold\n",
    "    rho_threshold=0.90,  # Correlation threshold\n",
    "    max_features=12,  # Maximum number of features\n",
    "    min_features=5,  # Minimum number of features\n",
    "    \n",
    "    # WOE settings\n",
    "    n_bins=10,\n",
    "    min_bin_size=0.05,\n",
    "    woe_monotonic=False,\n",
    "    \n",
    "    # HPO (Hyperparameter Optimization) settings\n",
    "    use_optuna=True,  # Enable Bayesian optimization with Optuna\n",
    "    n_trials=50,  # Number of HPO trials to run\n",
    "    optuna_timeout=300,  # Maximum 5 minutes for optimization\n",
    "    cv_folds=5,  # Cross-validation folds for HPO evaluation\n",
    "    \n",
    "    # Feature selection methods\n",
    "    use_boruta=True,  # Boruta feature selection\n",
    "    forward_1se=True,  # Forward selection with 1SE rule\n",
    "    use_noise_sentinel=True,  # Noise feature for stability check\n",
    "    \n",
    "    # Data splitting\n",
    "    use_test_split=True,\n",
    "    test_ratio=0.20,  # 20% for test\n",
    "    oot_ratio=0.20,  # 20% for out-of-time validation\n",
    "    \n",
    "    # Dual pipeline (WOE vs RAW)\n",
    "    enable_dual_pipeline=True,  # Compare WOE and RAW features\n",
    "    \n",
    "    # RAW pipeline settings\n",
    "    raw_outlier_method='clip',  # Handle outliers by clipping\n",
    "    raw_scaler_type='standard',  # Standardization\n",
    "    imputation_strategy='median',  # Median imputation for missing values\n",
    "    \n",
    "    # Model selection criteria\n",
    "    model_selection_method='balanced',  # Balance between performance and stability\n",
    "    model_stability_weight=0.3,  # Weight for stability in selection\n",
    "    min_gini_threshold=0.5,  # Minimum acceptable Gini\n",
    "    \n",
    "    # Random seed for reproducibility\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Pipeline configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Train the risk model using the dual pipeline approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the dual pipeline\n",
    "print(\"Starting Dual Pipeline Training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = DualPipeline(config)\n",
    "\n",
    "# Run the pipeline\n",
    "print(\"\\nTraining models with both WOE and RAW features...\")\n",
    "print(\"This may take 3-5 minutes depending on data size and HPO settings...\")\n",
    "print(\"\\nProgress:\")\n",
    "\n",
    "try:\n",
    "    # Run pipeline\n",
    "    pipeline.run(df)\n",
    "    \n",
    "    print(\"\\n‚úÖ Pipeline completed successfully!\")\n",
    "    \n",
    "    # Get summary\n",
    "    summary = pipeline.get_summary()\n",
    "    \n",
    "    print(\"\\nPipeline Summary:\")\n",
    "    print(f\"  - Best pipeline type: {summary['best_pipeline']}\")\n",
    "    print(f\"  - WOE features selected: {summary['n_features_woe']}\")\n",
    "    print(f\"  - RAW features selected: {summary['n_features_raw']}\")\n",
    "    print(f\"  - Total models trained: {len(pipeline.models_summary_)}\")\n",
    "    print(f\"  - Best model: {pipeline.best_model_name_}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during pipeline execution: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Results\n",
    "\n",
    "View and analyze the model performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View model results\n",
    "if hasattr(pipeline, 'models_summary_'):\n",
    "    print(\"Model Performance Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    summary_df = pipeline.models_summary_\n",
    "    \n",
    "    # Check which columns are available\n",
    "    available_cols = summary_df.columns.tolist()\n",
    "    \n",
    "    # Determine the feature count column name\n",
    "    feature_col = None\n",
    "    for col in ['n_features', 'n_vars', 'num_features', 'n_selected_features']:\n",
    "        if col in available_cols:\n",
    "            feature_col = col\n",
    "            break\n",
    "    \n",
    "    # Show top models\n",
    "    print(\"\\nTop 5 Models by OOT Gini:\")\n",
    "    \n",
    "    # Select columns that exist\n",
    "    display_cols = ['model_name', 'Gini_Train', 'Gini_OOT']\n",
    "    if 'Gini_Test' in available_cols:\n",
    "        display_cols.insert(2, 'Gini_Test')\n",
    "    if feature_col:\n",
    "        display_cols.append(feature_col)\n",
    "    \n",
    "    # Filter to existing columns only\n",
    "    display_cols = [col for col in display_cols if col in available_cols]\n",
    "    \n",
    "    top_models = summary_df.nlargest(5, 'Gini_OOT')[display_cols]\n",
    "    display(top_models)\n",
    "    \n",
    "    # Best model details\n",
    "    print(f\"\\nBest Model: {pipeline.best_model_name_}\")\n",
    "    best_row = summary_df[summary_df['model_name'] == pipeline.best_model_name_].iloc[0]\n",
    "    \n",
    "    print(f\"  Train Gini: {best_row['Gini_Train']:.4f}\")\n",
    "    if 'Gini_Test' in available_cols:\n",
    "        print(f\"  Test Gini: {best_row['Gini_Test']:.4f}\")\n",
    "    print(f\"  OOT Gini: {best_row['Gini_OOT']:.4f}\")\n",
    "    print(f\"  Train-OOT Gap: {abs(best_row['Gini_Train'] - best_row['Gini_OOT']):.4f}\")\n",
    "    \n",
    "    # Show feature count if available\n",
    "    if feature_col:\n",
    "        print(f\"  Features used: {int(best_row[feature_col])}\")\n",
    "else:\n",
    "    print(\"No model results available. Please run the pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applying Model to New Data\n",
    "\n",
    "Apply the trained model to score new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create completely new dataset for scoring\n",
    "print(\"Creating new scoring dataset...\")\n",
    "score_df = create_credit_data(n_samples=3000)\n",
    "print(f\"New data shape: {score_df.shape}\")\n",
    "print(f\"New data target rate: {score_df['target'].mean():.2%}\")\n",
    "\n",
    "# Apply the trained model\n",
    "print(\"\\nApplying trained model to new data...\")\n",
    "if hasattr(pipeline, 'predict_proba'):\n",
    "    score_probs = pipeline.predict_proba(score_df)\n",
    "    print(f\"Predictions generated: {len(score_probs)} samples\")\n",
    "    \n",
    "    # Evaluate performance\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc_score = roc_auc_score(score_df['target'], score_probs)\n",
    "    gini_score = 2 * auc_score - 1\n",
    "    \n",
    "    print(f\"\\nModel performance on new data:\")\n",
    "    print(f\"  AUC: {auc_score:.4f}\")\n",
    "    print(f\"  Gini: {gini_score:.4f}\")\n",
    "else:\n",
    "    print(\"Error: predict_proba method not available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Credit Scoring\n",
    "\n",
    "Convert probabilities to credit scores (300-850 range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_to_score(probs, base_score=600, pdo=20):\n",
    "    \"\"\"\n",
    "    Convert probability to credit score\n",
    "    Score = Base_Score - PDO * log(odds)\n",
    "    \"\"\"\n",
    "    # Clip probabilities to avoid inf values\n",
    "    probs_safe = np.clip(probs, 0.001, 0.999)\n",
    "    \n",
    "    # Calculate odds\n",
    "    odds = probs_safe / (1 - probs_safe)\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = base_score - pdo * np.log(odds)\n",
    "    \n",
    "    # Round to nearest integer\n",
    "    scores = np.round(scores).astype(int)\n",
    "    \n",
    "    # Ensure scores are in valid range\n",
    "    scores = np.clip(scores, 300, 850)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Convert probabilities to scores\n",
    "scores = probability_to_score(score_probs)\n",
    "\n",
    "print(\"Credit Score Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Min score: {scores.min()}\")\n",
    "print(f\"  Max score: {scores.max()}\")\n",
    "print(f\"  Mean score: {scores.mean():.0f}\")\n",
    "print(f\"  Median score: {np.median(scores):.0f}\")\n",
    "print(f\"  Std dev: {scores.std():.0f}\")\n",
    "\n",
    "# Score distribution\n",
    "print(\"\\nScore Distribution:\")\n",
    "for low, high in [(300, 400), (400, 500), (500, 600), (600, 700), (700, 800), (800, 850)]:\n",
    "    count = ((scores >= low) & (scores < high)).sum()\n",
    "    pct = count / len(scores) * 100\n",
    "    print(f\"  {low:3d}-{high:3d}: {count:4d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "Complete workflow summary and next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WORKFLOW SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. DATA PREPARATION:\")\n",
    "print(f\"   - Dataset size: {len(df):,} samples\")\n",
    "print(f\"   - Target rate: {df['target'].mean():.2%}\")\n",
    "print(f\"   - Features: {len(df.columns) - 3} (numeric + categorical)\")\n",
    "\n",
    "print(\"\\n2. MODEL TRAINING:\")\n",
    "if hasattr(pipeline, 'best_model_name_'):\n",
    "    best_row = pipeline.models_summary_[pipeline.models_summary_['model_name'] == pipeline.best_model_name_].iloc[0]\n",
    "    print(f\"   - Best model: {pipeline.best_model_name_}\")\n",
    "    print(f\"   - Train Gini: {best_row['Gini_Train']:.2%}\")\n",
    "    print(f\"   - OOT Gini: {best_row['Gini_OOT']:.2%}\")\n",
    "\n",
    "print(\"\\n3. SCORING:\")\n",
    "print(f\"   - Scored samples: {len(scores):,}\")\n",
    "print(f\"   - Score range: {scores.min()}-{scores.max()}\")\n",
    "print(f\"   - Mean score: {scores.mean():.0f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WORKFLOW COMPLETED SUCCESSFULLY!\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Deploy model to production\")\n",
    "print(\"  2. Set up monitoring for PSI and model drift\")\n",
    "print(\"  3. Implement A/B testing for model comparison\")\n",
    "print(\"  4. Schedule periodic model retraining\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}