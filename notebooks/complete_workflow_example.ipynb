{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Model Pipeline - Complete Workflow Example\n",
    "\n",
    "This notebook demonstrates the complete workflow:\n",
    "1. Data preparation with realistic target distribution\n",
    "2. Model training with good performance (70-80% Train Gini)\n",
    "3. Calibration functionality\n",
    "4. Credit scoring transformation\n",
    "5. Model evaluation and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Import our pipeline\n",
    "from risk_pipeline import Config, DualPipeline\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Install the risk-model-pipeline package from GitHub\n!pip install -q git+https://github.com/selimoksuz/risk-model-pipeline.git\n\nprint('Risk Model Pipeline installed successfully!')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Create synthetic data with realistic characteristics for credit risk modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create dataset size\n",
    "n_samples = 10000\n",
    "\n",
    "# Create features that correlate with target\n",
    "def create_credit_data(n_samples=10000):\n",
    "    \"\"\"\n",
    "    Create synthetic credit risk data with realistic patterns\n",
    "    Target event rate: ~15% (realistic for credit default)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Base features\n",
    "    age = np.random.normal(40, 12, n_samples)\n",
    "    age = np.clip(age, 18, 80)\n",
    "    \n",
    "    income = np.random.lognormal(10.5, 0.6, n_samples)  # Log-normal income distribution\n",
    "    income = np.clip(income, 10000, 500000)\n",
    "    \n",
    "    credit_score = np.random.normal(650, 80, n_samples)\n",
    "    credit_score = np.clip(credit_score, 300, 850)\n",
    "    \n",
    "    debt_ratio = np.random.beta(2, 5, n_samples)  # Debt-to-income ratio\n",
    "    \n",
    "    months_employed = np.random.exponential(60, n_samples)\n",
    "    months_employed = np.clip(months_employed, 0, 480)\n",
    "    \n",
    "    num_credit_lines = np.random.poisson(5, n_samples)\n",
    "    num_credit_lines = np.clip(num_credit_lines, 0, 20)\n",
    "    \n",
    "    utilization_rate = np.random.beta(3, 7, n_samples)  # Credit utilization\n",
    "    \n",
    "    # Create risk score based on features (for realistic target)\n",
    "    risk_score = (\n",
    "        - 0.01 * age  # Older = lower risk\n",
    "        - 0.00001 * income  # Higher income = lower risk\n",
    "        - 0.005 * credit_score  # Higher score = lower risk\n",
    "        + 3.0 * debt_ratio  # Higher debt = higher risk\n",
    "        - 0.005 * months_employed  # Longer employment = lower risk\n",
    "        + 0.05 * num_credit_lines  # More credit lines = slightly higher risk\n",
    "        + 2.0 * utilization_rate  # Higher utilization = higher risk\n",
    "        + np.random.normal(0, 0.5, n_samples)  # Random noise\n",
    "    )\n",
    "    \n",
    "    # Convert to probability using sigmoid\n",
    "    default_prob = 1 / (1 + np.exp(-risk_score))\n",
    "    \n",
    "    # Adjust to get ~15% event rate\n",
    "    default_prob = default_prob * 0.3\n",
    "    \n",
    "    # Generate binary target\n",
    "    target = np.random.binomial(1, default_prob)\n",
    "    \n",
    "    # Create categorical features\n",
    "    education = np.random.choice(\n",
    "        ['High School', 'Bachelor', 'Master', 'PhD'], \n",
    "        n_samples, \n",
    "        p=[0.3, 0.45, 0.2, 0.05]\n",
    "    )\n",
    "    \n",
    "    employment_type = np.random.choice(\n",
    "        ['Full-time', 'Part-time', 'Self-employed', 'Unemployed'],\n",
    "        n_samples,\n",
    "        p=[0.65, 0.15, 0.15, 0.05]\n",
    "    )\n",
    "    \n",
    "    region = np.random.choice(\n",
    "        ['North', 'South', 'East', 'West', 'Central'],\n",
    "        n_samples,\n",
    "        p=[0.2, 0.25, 0.2, 0.2, 0.15]\n",
    "    )\n",
    "    \n",
    "    home_ownership = np.random.choice(\n",
    "        ['Own', 'Rent', 'Mortgage', 'Other'],\n",
    "        n_samples,\n",
    "        p=[0.25, 0.35, 0.35, 0.05]\n",
    "    )\n",
    "    \n",
    "    # Create additional numeric features\n",
    "    num_late_payments = np.random.poisson(0.5, n_samples)\n",
    "    num_late_payments = np.clip(num_late_payments, 0, 10)\n",
    "    \n",
    "    months_since_last_late = np.random.exponential(24, n_samples)\n",
    "    months_since_last_late = np.clip(months_since_last_late, 0, 120)\n",
    "    months_since_last_late[num_late_payments == 0] = 999  # No late payment\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'app_id': range(n_samples),\n",
    "        'app_dt': pd.date_range(start='2022-01-01', periods=n_samples, freq='H')[:n_samples],\n",
    "        'target': target,\n",
    "        'age': age.round(0).astype(int),\n",
    "        'income': income.round(0).astype(int),\n",
    "        'credit_score': credit_score.round(0).astype(int),\n",
    "        'debt_ratio': debt_ratio.round(3),\n",
    "        'months_employed': months_employed.round(0).astype(int),\n",
    "        'num_credit_lines': num_credit_lines,\n",
    "        'utilization_rate': utilization_rate.round(3),\n",
    "        'num_late_payments': num_late_payments,\n",
    "        'months_since_last_late': months_since_last_late.round(0).astype(int),\n",
    "        'education': education,\n",
    "        'employment_type': employment_type,\n",
    "        'region': region,\n",
    "        'home_ownership': home_ownership\n",
    "    })\n",
    "    \n",
    "    # Add some missing values (realistic)\n",
    "    missing_indices = np.random.choice(n_samples, size=int(n_samples * 0.02), replace=False)\n",
    "    df.loc[missing_indices, 'months_since_last_late'] = np.nan\n",
    "    \n",
    "    missing_indices = np.random.choice(n_samples, size=int(n_samples * 0.01), replace=False)\n",
    "    df.loc[missing_indices, 'months_employed'] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create the dataset\n",
    "df = create_credit_data(n_samples=10000)\n",
    "\n",
    "print(f\"Dataset created with shape: {df.shape}\")\n",
    "print(f\"Target event rate: {df['target'].mean():.2%}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality check\n",
    "print(\"Data Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Total features: {len(df.columns) - 3}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"\\nEvent rate: {df['target'].mean():.2%}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline Configuration\n",
    "\n",
    "Configure the pipeline with optimal settings for model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure pipeline\nconfig = Config(\n    # Basic settings\n    target_col='target',\n    id_col='app_id',\n    time_col='app_dt',\n    output_folder='outputs',\n    \n    # Feature selection parameters\n    iv_min=0.02,  # Minimum Information Value\n    psi_threshold=0.25,  # Population Stability Index threshold\n    rho_threshold=0.90,  # Correlation threshold\n    max_features=12,  # Maximum number of features\n    min_features=5,  # Minimum number of features\n    \n    # WOE settings\n    n_bins=10,\n    min_bin_size=0.05,\n    woe_monotonic=False,\n    \n    # HPO (Hyperparameter Optimization) settings\n    # Note: HPO uses KS statistic (Kolmogorov-Smirnov) as optimization metric\n    # KS measures the maximum separation between cumulative distributions of good/bad\n    # Higher KS = better model discrimination (correlates with Gini)\n    use_optuna=True,  # Enable Bayesian optimization with Optuna\n    n_trials=50,  # Number of HPO trials to run\n    optuna_timeout=300,  # Maximum 5 minutes for optimization\n    cv_folds=5,  # Cross-validation folds for HPO evaluation\n    \n    # Feature selection methods\n    use_boruta=True,  # Boruta feature selection\n    forward_1se=True,  # Forward selection with 1SE rule\n    use_noise_sentinel=True,  # Noise feature for stability check\n    \n    # Data splitting\n    use_test_split=True,\n    test_ratio=0.20,  # 20% for test\n    oot_ratio=0.20,  # 20% for out-of-time validation\n    \n    # Dual pipeline (WOE vs RAW)\n    enable_dual_pipeline=True,  # Compare WOE and RAW features\n    \n    # RAW pipeline settings\n    raw_outlier_method='clip',  # Handle outliers by clipping\n    raw_scaler_type='standard',  # Standardization\n    imputation_strategy='median',  # Median imputation for missing values\n    \n    # Model selection criteria\n    model_selection_method='balanced',  # Balance between performance and stability\n    model_stability_weight=0.3,  # Weight for stability in selection\n    min_gini_threshold=0.5,  # Minimum acceptable Gini\n    \n    # Random seed for reproducibility\n    random_state=42\n)\n\nprint(\"Pipeline configured with:\")\nprint(f\"  - Dual pipeline: {config.enable_dual_pipeline}\")\nprint(f\"  - Max features: {config.max_features}\")\nprint(f\"  - HPO enabled: {config.use_optuna}\")\nprint(f\"  - HPO trials: {config.n_trials}\")\nprint(f\"  - HPO timeout: {config.optuna_timeout}s\")\nprint(f\"  - CV folds for HPO: {config.cv_folds}\")\nprint(\"\\nNote: HPO optimizes KS statistic via cross-validation\")\nprint(\"      Higher KS value = better model discrimination\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Train the risk model using the dual pipeline approach"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Understanding Hyperparameter Optimization (HPO)\n\nWhen HPO is enabled, the pipeline uses **Optuna** for Bayesian optimization to find the best model hyperparameters.\n\n### What is the \"value\" in HPO trials?\n\nThe \"value\" shown during HPO represents the **KS (Kolmogorov-Smirnov) statistic**, which measures:\n- **Maximum separation** between cumulative distributions of good and bad samples\n- **Model discriminatory power** - higher KS means better separation\n- **Correlation with Gini**: KS ≈ Gini/2 (approximately)\n\n### HPO Process:\n1. Each trial suggests hyperparameters using Bayesian optimization\n2. Model is trained with k-fold cross-validation (default 5 folds)\n3. KS score is calculated on each validation fold\n4. Average KS across folds becomes the trial's \"value\"\n5. Optuna maximizes this value to find best hyperparameters\n\n### Example KS interpretation:\n- KS = 0.20-0.30: Weak discrimination\n- KS = 0.30-0.40: Moderate discrimination  \n- KS = 0.40-0.50: Good discrimination\n- KS = 0.50+: Excellent discrimination\n\nFor credit risk models, KS values of 0.35-0.45 are typically considered good.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and run pipeline\n",
    "pipeline = DualPipeline(config)\n",
    "\n",
    "print(\"Starting pipeline training...\")\n",
    "print(\"This may take 2-3 minutes...\\n\")\n",
    "\n",
    "# Train the model\n",
    "pipeline.run(df)\n",
    "\n",
    "print(\"\\nPipeline training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View model results\n",
    "if hasattr(pipeline, 'models_summary_'):\n",
    "    print(\"Model Performance Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    summary_df = pipeline.models_summary_\n",
    "    \n",
    "    # Show top models\n",
    "    print(\"\\nTop 5 Models by OOT Gini:\")\n",
    "    top_models = summary_df.nlargest(5, 'Gini_OOT')[['model_name', 'Gini_Train', 'Gini_Test', 'Gini_OOT', 'n_features']]\n",
    "    display(top_models)\n",
    "    \n",
    "    # Best model details\n",
    "    print(f\"\\nBest Model: {pipeline.best_model_name_}\")\n",
    "    best_row = summary_df[summary_df['model_name'] == pipeline.best_model_name_].iloc[0]\n",
    "    \n",
    "    print(f\"  Train Gini: {best_row['Gini_Train']:.4f}\")\n",
    "    print(f\"  Test Gini: {best_row['Gini_Test']:.4f}\")\n",
    "    print(f\"  OOT Gini: {best_row['Gini_OOT']:.4f}\")\n",
    "    print(f\"  Train-OOT Gap: {abs(best_row['Gini_Train'] - best_row['Gini_OOT']):.4f}\")\n",
    "    print(f\"  Features used: {best_row['n_features']}\")\n",
    "    \n",
    "    # Check if we achieved target performance\n",
    "    if 0.70 <= best_row['Gini_Train'] <= 0.80:\n",
    "        print(\"\\n✓ Target performance achieved! Train Gini is between 70-80%\")\n",
    "    else:\n",
    "        print(f\"\\n! Train Gini {best_row['Gini_Train']:.2%} (target: 70-80%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction and Probability Calibration\n",
    "\n",
    "Test prediction functionality and calibrate probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare calibration data\n",
    "print(\"Preparing calibration data...\")\n",
    "\n",
    "# Create a separate calibration dataset\n",
    "calib_df = create_credit_data(n_samples=2000)\n",
    "print(f\"Calibration data shape: {calib_df.shape}\")\n",
    "print(f\"Calibration target rate: {calib_df['target'].mean():.2%}\")\n",
    "\n",
    "# Get predictions on calibration data\n",
    "if hasattr(pipeline, 'predict_proba'):\n",
    "    calib_probs = pipeline.predict_proba(calib_df)\n",
    "    print(f\"\\nCalibration predictions generated: {len(calib_probs)}\")\n",
    "    \n",
    "    # Check calibration\n",
    "    print(f\"\\nBefore calibration:\")\n",
    "    print(f\"  Actual event rate: {calib_df['target'].mean():.4f}\")\n",
    "    print(f\"  Mean predicted probability: {calib_probs.mean():.4f}\")\n",
    "    print(f\"  Calibration error: {abs(calib_df['target'].mean() - calib_probs.mean()):.4f}\")\n",
    "    \n",
    "    # Store for comparison\n",
    "    uncalibrated_probs = calib_probs.copy()\n",
    "else:\n",
    "    print(\"predict_proba method not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement calibration (if not available in pipeline)\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Calibrate probabilities\n",
    "print(\"Calibrating probabilities...\")\n",
    "\n",
    "# Use isotonic regression for calibration\n",
    "iso_reg = IsotonicRegression(out_of_bounds='clip')\n",
    "calibrated_probs = iso_reg.fit_transform(uncalibrated_probs, calib_df['target'])\n",
    "\n",
    "print(f\"\\nAfter calibration:\")\n",
    "print(f\"  Actual event rate: {calib_df['target'].mean():.4f}\")\n",
    "print(f\"  Mean calibrated probability: {calibrated_probs.mean():.4f}\")\n",
    "print(f\"  Calibration error: {abs(calib_df['target'].mean() - calibrated_probs.mean()):.4f}\")\n",
    "\n",
    "# Plot calibration curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Before calibration\n",
    "fraction_pos, mean_pred = calibration_curve(calib_df['target'], uncalibrated_probs, n_bins=10)\n",
    "axes[0].plot(mean_pred, fraction_pos, marker='o', label='Model')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Perfect')\n",
    "axes[0].set_xlabel('Mean Predicted Probability')\n",
    "axes[0].set_ylabel('Fraction of Positives')\n",
    "axes[0].set_title('Before Calibration')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# After calibration\n",
    "fraction_pos, mean_pred = calibration_curve(calib_df['target'], calibrated_probs, n_bins=10)\n",
    "axes[1].plot(mean_pred, fraction_pos, marker='o', label='Calibrated')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Perfect')\n",
    "axes[1].set_xlabel('Mean Predicted Probability')\n",
    "axes[1].set_ylabel('Fraction of Positives')\n",
    "axes[1].set_title('After Calibration')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Credit Scoring\n",
    "\n",
    "Convert probabilities to credit scores (300-850 range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply trained model to new data (simulating production scoring)\nprint(\"=\" * 60)\nprint(\"APPLYING TRAINED MODEL TO NEW DATA\")\nprint(\"=\" * 60)\n\n# Create completely new dataset (simulating production data)\nprint(\"\\n1. Creating new scoring dataset...\")\nscore_df = create_credit_data(n_samples=3000)\nprint(f\"   New data shape: {score_df.shape}\")\nprint(f\"   New data target rate: {score_df['target'].mean():.2%}\")\n\n# Apply the trained model to score new data\nprint(\"\\n2. Applying trained model to new data...\")\nif hasattr(pipeline, 'predict_proba'):\n    # Get probability predictions on new data\n    score_probs = pipeline.predict_proba(score_df)\n    print(f\"   Predictions generated: {len(score_probs)} samples\")\n    print(f\"   Predicted probability range: [{score_probs.min():.4f}, {score_probs.max():.4f}]\")\n    print(f\"   Mean predicted probability: {score_probs.mean():.4f}\")\n    \n    # Evaluate model performance on new data\n    from sklearn.metrics import roc_auc_score\n    auc_score = roc_auc_score(score_df['target'], score_probs)\n    gini_score = 2 * auc_score - 1\n    \n    print(f\"\\n3. Model performance on new data:\")\n    print(f\"   AUC: {auc_score:.4f}\")\n    print(f\"   Gini: {gini_score:.4f}\")\n    \n    # Check if performance is stable\n    if hasattr(pipeline, 'models_summary_'):\n        best_row = pipeline.models_summary_[pipeline.models_summary_['model_name'] == pipeline.best_model_name_].iloc[0]\n        oot_gini = best_row['Gini_OOT']\n        gini_diff = abs(gini_score - oot_gini)\n        \n        print(f\"\\n4. Stability check:\")\n        print(f\"   OOT Gini (validation): {oot_gini:.4f}\")\n        print(f\"   New data Gini: {gini_score:.4f}\")\n        print(f\"   Difference: {gini_diff:.4f}\")\n        \n        if gini_diff < 0.05:\n            print(\"   ✓ Model performance is stable on new data\")\n        else:\n            print(\"   ⚠ Performance difference detected - may need monitoring\")\nelse:\n    print(\"Error: predict_proba method not available!\")\n    \nprint(\"\\nNote: This demonstrates applying the trained model to completely new data,\")\nprint(\"      as would happen in production scoring scenarios.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_to_score(probs, base_score=600, pdo=20):\n",
    "    \"\"\"\n",
    "    Convert probability to credit score\n",
    "    \n",
    "    Standard credit scoring formula:\n",
    "    Score = Base_Score - PDO * log(odds)\n",
    "    \n",
    "    Where:\n",
    "    - Base_Score: typically 600\n",
    "    - PDO (Points to Double Odds): typically 20\n",
    "    - odds = p / (1 - p)\n",
    "    \"\"\"\n",
    "    # Clip probabilities to avoid inf values\n",
    "    probs_safe = np.clip(probs, 0.001, 0.999)\n",
    "    \n",
    "    # Calculate odds\n",
    "    odds = probs_safe / (1 - probs_safe)\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = base_score - pdo * np.log(odds)\n",
    "    \n",
    "    # Round to nearest integer\n",
    "    scores = np.round(scores).astype(int)\n",
    "    \n",
    "    # Ensure scores are in valid range\n",
    "    scores = np.clip(scores, 300, 850)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Convert probabilities to scores\n",
    "scores = probability_to_score(score_probs)\n",
    "\n",
    "print(\"Credit Score Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Min score: {scores.min()}\")\n",
    "print(f\"  Max score: {scores.max()}\")\n",
    "print(f\"  Mean score: {scores.mean():.0f}\")\n",
    "print(f\"  Median score: {np.median(scores):.0f}\")\n",
    "print(f\"  Std dev: {scores.std():.0f}\")\n",
    "\n",
    "# Score distribution\n",
    "print(\"\\nScore Distribution:\")\n",
    "print(f\"  300-400: {((scores >= 300) & (scores < 400)).sum():,} ({((scores >= 300) & (scores < 400)).mean():.1%})\")\n",
    "print(f\"  400-500: {((scores >= 400) & (scores < 500)).sum():,} ({((scores >= 400) & (scores < 500)).mean():.1%})\")\n",
    "print(f\"  500-600: {((scores >= 500) & (scores < 600)).sum():,} ({((scores >= 500) & (scores < 600)).mean():.1%})\")\n",
    "print(f\"  600-700: {((scores >= 600) & (scores < 700)).sum():,} ({((scores >= 600) & (scores < 700)).mean():.1%})\")\n",
    "print(f\"  700-800: {((scores >= 700) & (scores < 800)).sum():,} ({((scores >= 700) & (scores < 800)).mean():.1%})\")\n",
    "print(f\"  800-850: {((scores >= 800) & (scores <= 850)).sum():,} ({((scores >= 800) & (scores <= 850)).mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Score histogram\n",
    "axes[0, 0].hist(scores, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Credit Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Credit Score Distribution')\n",
    "axes[0, 0].axvline(scores.mean(), color='red', linestyle='--', label=f'Mean: {scores.mean():.0f}')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Score vs Target Rate\n",
    "score_bins = pd.cut(scores, bins=10)\n",
    "score_target_rate = score_df.groupby(score_bins)['target'].mean()\n",
    "axes[0, 1].bar(range(len(score_target_rate)), score_target_rate.values, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Score Decile')\n",
    "axes[0, 1].set_ylabel('Default Rate')\n",
    "axes[0, 1].set_title('Default Rate by Score Decile')\n",
    "axes[0, 1].set_xticks(range(len(score_target_rate)))\n",
    "axes[0, 1].set_xticklabels([f'D{i+1}' for i in range(len(score_target_rate))], rotation=0)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Probability distribution\n",
    "axes[1, 0].hist(score_probs, bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1, 0].set_xlabel('Predicted Probability')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Probability Distribution')\n",
    "axes[1, 0].axvline(score_probs.mean(), color='red', linestyle='--', label=f'Mean: {score_probs.mean():.3f}')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(score_df['target'], score_probs)\n",
    "auc = roc_auc_score(score_df['target'], score_probs)\n",
    "gini = 2 * auc - 1\n",
    "\n",
    "axes[1, 1].plot(fpr, tpr, label=f'ROC (AUC = {auc:.3f}, Gini = {gini:.3f})')\n",
    "axes[1, 1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[1, 1].set_xlabel('False Positive Rate')\n",
    "axes[1, 1].set_ylabel('True Positive Rate')\n",
    "axes[1, 1].set_title('ROC Curve on Scoring Data')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Model Performance Visualization', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Score Segmentation and Risk Tiers\n",
    "\n",
    "Create risk tiers based on credit scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define risk tiers\n",
    "def assign_risk_tier(score):\n",
    "    if score >= 750:\n",
    "        return 'Very Low Risk'\n",
    "    elif score >= 650:\n",
    "        return 'Low Risk'\n",
    "    elif score >= 550:\n",
    "        return 'Medium Risk'\n",
    "    elif score >= 450:\n",
    "        return 'High Risk'\n",
    "    else:\n",
    "        return 'Very High Risk'\n",
    "\n",
    "# Assign risk tiers\n",
    "risk_tiers = pd.Series(scores).apply(assign_risk_tier)\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'app_id': score_df['app_id'],\n",
    "    'actual_target': score_df['target'],\n",
    "    'predicted_prob': score_probs,\n",
    "    'credit_score': scores,\n",
    "    'risk_tier': risk_tiers\n",
    "})\n",
    "\n",
    "# Risk tier analysis\n",
    "tier_analysis = results_df.groupby('risk_tier').agg({\n",
    "    'app_id': 'count',\n",
    "    'actual_target': 'mean',\n",
    "    'predicted_prob': 'mean',\n",
    "    'credit_score': ['mean', 'min', 'max']\n",
    "}).round(3)\n",
    "\n",
    "tier_analysis.columns = ['Count', 'Actual_Rate', 'Pred_Prob', 'Mean_Score', 'Min_Score', 'Max_Score']\n",
    "\n",
    "# Sort by mean score\n",
    "tier_analysis = tier_analysis.sort_values('Mean_Score', ascending=False)\n",
    "\n",
    "print(\"Risk Tier Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "display(tier_analysis)\n",
    "\n",
    "# Calculate lift\n",
    "base_rate = results_df['actual_target'].mean()\n",
    "tier_analysis['Lift'] = (tier_analysis['Actual_Rate'] / base_rate).round(2)\n",
    "\n",
    "print(f\"\\nBase default rate: {base_rate:.2%}\")\n",
    "print(\"\\nLift by Risk Tier:\")\n",
    "for tier in tier_analysis.index:\n",
    "    lift = tier_analysis.loc[tier, 'Lift']\n",
    "    print(f\"  {tier:15s}: {lift:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Interpretation\n",
    "\n",
    "Understanding feature importance and model drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance (if available)\n",
    "if hasattr(pipeline, 'feature_importance_'):\n",
    "    feature_imp = pipeline.feature_importance_\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_imp.plot(kind='barh')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importance not directly available from pipeline\")\n",
    "    print(\"Features used in the model can be found in the model summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results\n",
    "\n",
    "Save the scoring results for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final scoring output\n",
    "final_output = pd.DataFrame({\n",
    "    'app_id': results_df['app_id'],\n",
    "    'score_date': pd.Timestamp.now().date(),\n",
    "    'credit_score': results_df['credit_score'],\n",
    "    'risk_tier': results_df['risk_tier'],\n",
    "    'default_probability': results_df['predicted_prob'].round(4),\n",
    "    'model_version': pipeline.best_model_name_\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'scoring_results.csv'\n",
    "final_output.to_csv(output_file, index=False)\n",
    "print(f\"Scoring results saved to: {output_file}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of scoring output:\")\n",
    "display(final_output.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "Summary of the complete workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WORKFLOW SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. DATA PREPARATION:\")\n",
    "print(f\"   - Dataset size: {len(df):,} samples\")\n",
    "print(f\"   - Target rate: {df['target'].mean():.2%}\")\n",
    "print(f\"   - Features: {len(df.columns) - 3} (numeric + categorical)\")\n",
    "\n",
    "print(\"\\n2. MODEL TRAINING:\")\n",
    "if hasattr(pipeline, 'best_model_name_'):\n",
    "    best_row = pipeline.models_summary_[pipeline.models_summary_['model_name'] == pipeline.best_model_name_].iloc[0]\n",
    "    print(f\"   - Best model: {pipeline.best_model_name_}\")\n",
    "    print(f\"   - Train Gini: {best_row['Gini_Train']:.2%}\")\n",
    "    print(f\"   - OOT Gini: {best_row['Gini_OOT']:.2%}\")\n",
    "    print(f\"   - Features used: {best_row['n_features']}\")\n",
    "\n",
    "print(\"\\n3. CALIBRATION:\")\n",
    "print(f\"   - Calibration samples: {len(calib_df):,}\")\n",
    "print(f\"   - Calibration error: {abs(calib_df['target'].mean() - calibrated_probs.mean()):.4f}\")\n",
    "\n",
    "print(\"\\n4. SCORING:\")\n",
    "print(f\"   - Scored samples: {len(scores):,}\")\n",
    "print(f\"   - Score range: {scores.min()}-{scores.max()}\")\n",
    "print(f\"   - Mean score: {scores.mean():.0f}\")\n",
    "\n",
    "print(\"\\n5. RISK SEGMENTATION:\")\n",
    "for tier in tier_analysis.index:\n",
    "    count = tier_analysis.loc[tier, 'Count']\n",
    "    rate = tier_analysis.loc[tier, 'Actual_Rate']\n",
    "    print(f\"   - {tier:15s}: {count:,} samples ({rate:.2%} default rate)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WORKFLOW COMPLETED SUCCESSFULLY!\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Deploy model to production\")\n",
    "print(\"  2. Set up monitoring for PSI and model drift\")\n",
    "print(\"  3. Implement A/B testing for model comparison\")\n",
    "print(\"  4. Schedule periodic model retraining\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}