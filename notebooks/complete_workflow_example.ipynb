{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Model Pipeline - Complete End-to-End Workflow\n",
    "\n",
    "## üîÑ Auto-Update Feature\n",
    "This notebook automatically downloads and installs the **latest version** from GitHub every time you run it.\n",
    "- No need to manually update\n",
    "- Always uses the most recent code\n",
    "- Clears cache to avoid version conflicts\n",
    "\n",
    "## üìã Workflow Includes:\n",
    "1. **Automatic package update from GitHub**\n",
    "2. Data preparation with realistic target distribution (70-80% Gini)\n",
    "3. Full pipeline configuration with all parameters\n",
    "4. Model training with WOE and RAW pipelines\n",
    "5. **Probability calibration with Isotonic Regression**\n",
    "6. Credit scoring transformation (300-850)\n",
    "7. Risk segmentation and tiering\n",
    "8. Model evaluation and reporting\n",
    "9. Production scoring simulation\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes:\n",
    "- **First cell ALWAYS updates the package** - run it every time!\n",
    "- If you see old behavior after update, restart kernel and run again\n",
    "- Set `FORCE_REINSTALL = False` in first cell if you want to keep current version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RISK MODEL PIPELINE - INSTALLATION/UPDATE\n",
      "============================================================\n",
      "Force reinstall: True\n",
      "GitHub URL: https://github.com/selimoksuz/risk-model-pipeline\n",
      "Branch: main (latest)\n",
      "------------------------------------------------------------\n",
      "üîÑ Removing existing installation (if any)...\n",
      "‚úì Existing version removed\n",
      "üì¶ Installing latest version from GitHub...\n",
      "‚úÖ Latest version installed successfully!\n",
      "üìå Version: 0.3.0\n",
      "\n",
      "üîç Verifying installation...\n",
      "‚úÖ All imports successful!\n",
      "‚úì Config available from: risk_pipeline.core.config\n",
      "‚úì DualPipeline available from: risk_pipeline.pipeline\n",
      "\n",
      "============================================================\n",
      "üéâ READY TO USE!\n",
      "============================================================\n",
      "\n",
      "üìù Note: If you still see old version behavior:\n",
      "   1. Restart the kernel (Kernel ‚Üí Restart)\n",
      "   2. Run this cell again\n",
      "   3. Continue with the notebook\n"
     ]
    }
   ],
   "source": [
    "# Install/Update the risk-model-pipeline package from GitHub\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "def install_or_update_risk_pipeline(force_reinstall=True):\n",
    "    \"\"\"\n",
    "    Install or update risk-model-pipeline package from GitHub\n",
    "    Always gets the latest version\n",
    "    \"\"\"\n",
    "    \n",
    "    # First, try to uninstall any existing version to ensure clean update\n",
    "    if force_reinstall:\n",
    "        print(\"üîÑ Removing existing installation (if any)...\")\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"risk-model-pipeline\"],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=False  # Don't fail if package doesn't exist\n",
    "            )\n",
    "            print(\"‚úì Existing version removed\")\n",
    "        except:\n",
    "            print(\"‚úì No existing installation found\")\n",
    "    \n",
    "    # Clear import cache to ensure fresh import\n",
    "    if 'risk_pipeline' in sys.modules:\n",
    "        print(\"üîÑ Clearing import cache...\")\n",
    "        # Remove all risk_pipeline related modules from cache\n",
    "        modules_to_remove = [key for key in sys.modules.keys() if key.startswith('risk_pipeline')]\n",
    "        for module in modules_to_remove:\n",
    "            del sys.modules[module]\n",
    "        print(\"‚úì Import cache cleared\")\n",
    "    \n",
    "    # Install latest version from GitHub main branch\n",
    "    print(\"üì¶ Installing latest version from GitHub...\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \n",
    "             \"git+https://github.com/selimoksuz/risk-model-pipeline.git@main#egg=risk-model-pipeline\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        print(\"‚úÖ Latest version installed successfully!\")\n",
    "        \n",
    "        # Try to get version info\n",
    "        try:\n",
    "            import risk_pipeline\n",
    "            importlib.reload(risk_pipeline)  # Force reload\n",
    "            print(f\"üìå Version: {risk_pipeline.__version__}\")\n",
    "        except:\n",
    "            print(\"üìå Package installed (version check not available)\")\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Installation failed: {e.stderr}\")\n",
    "        return False\n",
    "    \n",
    "    # Verify installation with fresh import\n",
    "    print(\"\\nüîç Verifying installation...\")\n",
    "    try:\n",
    "        # Clear and reimport\n",
    "        if 'risk_pipeline' in sys.modules:\n",
    "            del sys.modules['risk_pipeline']\n",
    "        \n",
    "        import risk_pipeline\n",
    "        from risk_pipeline import Config, DualPipeline\n",
    "        from risk_pipeline.pipeline import RiskModelPipeline\n",
    "        \n",
    "        print(\"‚úÖ All imports successful!\")\n",
    "        print(f\"‚úì Config available from: {Config.__module__}\")\n",
    "        print(f\"‚úì DualPipeline available from: {DualPipeline.__module__}\")\n",
    "        return True\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Import verification failed: {e}\")\n",
    "        print(\"\\nüí° Try restarting the kernel and running this cell again\")\n",
    "        return False\n",
    "\n",
    "# Configuration options\n",
    "FORCE_REINSTALL = True  # Set to True to always get the latest version\n",
    "AUTO_RESTART = False    # Set to True to auto-restart kernel after install\n",
    "\n",
    "# Run installation/update\n",
    "print(\"=\" * 60)\n",
    "print(\"RISK MODEL PIPELINE - INSTALLATION/UPDATE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Force reinstall: {FORCE_REINSTALL}\")\n",
    "print(f\"GitHub URL: https://github.com/selimoksuz/risk-model-pipeline\")\n",
    "print(f\"Branch: main (latest)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "success = install_or_update_risk_pipeline(force_reinstall=FORCE_REINSTALL)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéâ READY TO USE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nüìù Note: If you still see old version behavior:\")\n",
    "    print(\"   1. Restart the kernel (Kernel ‚Üí Restart)\")\n",
    "    print(\"   2. Run this cell again\")\n",
    "    print(\"   3. Continue with the notebook\")\n",
    "    \n",
    "    if AUTO_RESTART:\n",
    "        print(\"\\nüîÑ Auto-restarting kernel in 3 seconds...\")\n",
    "        import time\n",
    "        time.sleep(3)\n",
    "        from IPython.core.display import HTML\n",
    "        display(HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\"))\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Installation had issues. Please:\")\n",
    "    print(\"   1. Restart the kernel\")\n",
    "    print(\"   2. Run this cell again\")\n",
    "    print(\"   3. If problem persists, try manual installation:\")\n",
    "    print(\"      !pip uninstall -y risk-model-pipeline\")\n",
    "    print(\"      !pip install --no-cache-dir git+https://github.com/selimoksuz/risk-model-pipeline.git\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib==3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization libraries imported successfully!\n",
      "Core libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Try to import visualization libraries (optional)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    PLOT_AVAILABLE = True\n",
    "    print('Visualization libraries imported successfully!')\n",
    "except ImportError as e:\n",
    "    PLOT_AVAILABLE = False\n",
    "    print('Warning: Visualization libraries not available. Plots will be skipped.')\n",
    "    print(f'Error: {e}')\n",
    "\n",
    "# Import our pipeline\n",
    "from risk_pipeline import Config, DualPipeline\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print('Core libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Create synthetic data designed to achieve 70-80% Train Gini with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with shape: (10000, 16)\n",
      "Target event rate: 15.00%\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_dt</th>\n",
       "      <th>target</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>debt_ratio</th>\n",
       "      <th>payment_history_score</th>\n",
       "      <th>income</th>\n",
       "      <th>months_employed</th>\n",
       "      <th>num_delinquencies</th>\n",
       "      <th>utilization_rate</th>\n",
       "      <th>age</th>\n",
       "      <th>num_credit_lines</th>\n",
       "      <th>months_since_last_late</th>\n",
       "      <th>education</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>home_ownership</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>501</td>\n",
       "      <td>0.933</td>\n",
       "      <td>44.7</td>\n",
       "      <td>58339</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.717</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>999.0</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>724</td>\n",
       "      <td>0.163</td>\n",
       "      <td>95.8</td>\n",
       "      <td>101348</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.671</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Master</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>752</td>\n",
       "      <td>0.340</td>\n",
       "      <td>85.0</td>\n",
       "      <td>135815</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.411</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>999.0</td>\n",
       "      <td>Master</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-01-01 03:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>499</td>\n",
       "      <td>0.869</td>\n",
       "      <td>49.8</td>\n",
       "      <td>111173</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.407</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>103.0</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-01-01 04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>597</td>\n",
       "      <td>0.918</td>\n",
       "      <td>14.9</td>\n",
       "      <td>42528</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.859</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>999.0</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app_id              app_dt  target  credit_score  debt_ratio  \\\n",
       "0       0 2022-01-01 00:00:00       0           501       0.933   \n",
       "1       1 2022-01-01 01:00:00       0           724       0.163   \n",
       "2       2 2022-01-01 02:00:00       0           752       0.340   \n",
       "3       3 2022-01-01 03:00:00       0           499       0.869   \n",
       "4       4 2022-01-01 04:00:00       0           597       0.918   \n",
       "\n",
       "   payment_history_score  income  months_employed  num_delinquencies  \\\n",
       "0                   44.7   58339             17.0                  3   \n",
       "1                   95.8  101348             89.0                  0   \n",
       "2                   85.0  135815             34.0                  0   \n",
       "3                   49.8  111173             21.0                  2   \n",
       "4                   14.9   42528             13.0                  2   \n",
       "\n",
       "   utilization_rate  age  num_credit_lines  months_since_last_late education  \\\n",
       "0             0.717   32                 7                   999.0  Bachelor   \n",
       "1             0.671   43                 3                    90.0    Master   \n",
       "2             0.411   47                 3                   999.0    Master   \n",
       "3             0.407   34                 4                   103.0  Bachelor   \n",
       "4             0.859   34                 4                   999.0       PhD   \n",
       "\n",
       "  employment_type home_ownership  \n",
       "0       Full-time       Mortgage  \n",
       "1       Full-time       Mortgage  \n",
       "2   Self-employed       Mortgage  \n",
       "3       Full-time       Mortgage  \n",
       "4       Full-time       Mortgage  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def create_high_performance_credit_data(n_samples=10000):\n",
    "    \"\"\"\n",
    "    Create synthetic credit risk data optimized for 70-80% Gini\n",
    "    with strong linear separability for logistic regression\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create strongly predictive features with clear linear relationships\n",
    "    \n",
    "    # Feature 1: Credit score - very strong predictor\n",
    "    credit_score_good = np.random.normal(750, 40, n_samples // 2)\n",
    "    credit_score_bad = np.random.normal(550, 50, n_samples // 2)\n",
    "    credit_score = np.concatenate([credit_score_good, credit_score_bad])\n",
    "    credit_score = np.clip(credit_score, 300, 850)\n",
    "    \n",
    "    # Feature 2: Debt ratio - strong predictor\n",
    "    debt_ratio_good = np.random.beta(2, 8, n_samples // 2)  # Low debt\n",
    "    debt_ratio_bad = np.random.beta(8, 2, n_samples // 2)   # High debt\n",
    "    debt_ratio = np.concatenate([debt_ratio_good, debt_ratio_bad])\n",
    "    \n",
    "    # Feature 3: Payment history score (0-100)\n",
    "    payment_history_good = np.random.normal(85, 10, n_samples // 2)\n",
    "    payment_history_bad = np.random.normal(45, 15, n_samples // 2)\n",
    "    payment_history = np.concatenate([payment_history_good, payment_history_bad])\n",
    "    payment_history = np.clip(payment_history, 0, 100)\n",
    "    \n",
    "    # Feature 4: Income (log-normal)\n",
    "    income_good = np.random.lognormal(11.5, 0.4, n_samples // 2)\n",
    "    income_bad = np.random.lognormal(10.5, 0.5, n_samples // 2)\n",
    "    income = np.concatenate([income_good, income_bad])\n",
    "    income = np.clip(income, 15000, 500000)\n",
    "    \n",
    "    # Feature 5: Months employed\n",
    "    employed_good = np.random.gamma(8, 10, n_samples // 2)  # Stable employment\n",
    "    employed_bad = np.random.gamma(2, 10, n_samples // 2)   # Unstable employment\n",
    "    months_employed = np.concatenate([employed_good, employed_bad])\n",
    "    months_employed = np.clip(months_employed, 0, 480)\n",
    "    \n",
    "    # Feature 6: Number of delinquencies\n",
    "    delinq_good = np.random.poisson(0.1, n_samples // 2)\n",
    "    delinq_bad = np.random.poisson(2.5, n_samples // 2)\n",
    "    num_delinquencies = np.concatenate([delinq_good, delinq_bad])\n",
    "    num_delinquencies = np.clip(num_delinquencies, 0, 10)\n",
    "    \n",
    "    # Feature 7: Credit utilization\n",
    "    util_good = np.random.beta(2, 5, n_samples // 2)  # Low utilization\n",
    "    util_bad = np.random.beta(5, 2, n_samples // 2)   # High utilization\n",
    "    utilization_rate = np.concatenate([util_good, util_bad])\n",
    "    \n",
    "    # Feature 8: Age (slight predictor)\n",
    "    age_good = np.random.normal(45, 10, n_samples // 2)\n",
    "    age_bad = np.random.normal(32, 8, n_samples // 2)\n",
    "    age = np.concatenate([age_good, age_bad])\n",
    "    age = np.clip(age, 18, 80)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    shuffle_idx = np.random.permutation(n_samples)\n",
    "    credit_score = credit_score[shuffle_idx]\n",
    "    debt_ratio = debt_ratio[shuffle_idx]\n",
    "    payment_history = payment_history[shuffle_idx]\n",
    "    income = income[shuffle_idx]\n",
    "    months_employed = months_employed[shuffle_idx]\n",
    "    num_delinquencies = num_delinquencies[shuffle_idx]\n",
    "    utilization_rate = utilization_rate[shuffle_idx]\n",
    "    age = age[shuffle_idx]\n",
    "    \n",
    "    # Create target with strong linear relationship\n",
    "    # This formula is designed to give ~70-80% Gini with logistic regression\n",
    "    risk_score = (\n",
    "        - 0.008 * credit_score           # Strong negative (good score = low risk)\n",
    "        + 4.0 * debt_ratio               # Strong positive (high debt = high risk)\n",
    "        - 0.025 * payment_history        # Strong negative (good history = low risk)\n",
    "        - 0.000005 * income              # Moderate negative\n",
    "        - 0.003 * months_employed        # Moderate negative\n",
    "        + 0.5 * num_delinquencies        # Strong positive\n",
    "        + 2.5 * utilization_rate         # Strong positive\n",
    "        - 0.01 * age                     # Slight negative\n",
    "        + 2.0                            # Intercept to center probabilities\n",
    "    )\n",
    "    \n",
    "    # Add small noise to prevent perfect separation\n",
    "    risk_score += np.random.normal(0, 0.3, n_samples)\n",
    "    \n",
    "    # Convert to probability\n",
    "    default_prob = 1 / (1 + np.exp(-risk_score))\n",
    "    \n",
    "    # Generate binary target (aim for ~15% event rate)\n",
    "    target = np.random.binomial(1, default_prob)\n",
    "    \n",
    "    # Adjust to get closer to 15% if needed\n",
    "    current_rate = target.mean()\n",
    "    if current_rate > 0.20:\n",
    "        # Randomly flip some 1s to 0s\n",
    "        ones_idx = np.where(target == 1)[0]\n",
    "        n_to_flip = int((current_rate - 0.15) * n_samples)\n",
    "        flip_idx = np.random.choice(ones_idx, n_to_flip, replace=False)\n",
    "        target[flip_idx] = 0\n",
    "    \n",
    "    # Create categorical features that correlate with target\n",
    "    education = np.where(\n",
    "        target == 0,\n",
    "        np.random.choice(['Bachelor', 'Master', 'PhD'], n_samples, p=[0.5, 0.35, 0.15]),\n",
    "        np.random.choice(['High School', 'Bachelor', 'Master'], n_samples, p=[0.5, 0.4, 0.1])\n",
    "    )\n",
    "    \n",
    "    employment_type = np.where(\n",
    "        target == 0,\n",
    "        np.random.choice(['Full-time', 'Self-employed'], n_samples, p=[0.7, 0.3]),\n",
    "        np.random.choice(['Part-time', 'Unemployed', 'Full-time'], n_samples, p=[0.3, 0.2, 0.5])\n",
    "    )\n",
    "    \n",
    "    home_ownership = np.where(\n",
    "        target == 0,\n",
    "        np.random.choice(['Own', 'Mortgage'], n_samples, p=[0.4, 0.6]),\n",
    "        np.random.choice(['Rent', 'Other', 'Mortgage'], n_samples, p=[0.5, 0.1, 0.4])\n",
    "    )\n",
    "    \n",
    "    # Additional features\n",
    "    num_credit_lines = np.where(\n",
    "        target == 0,\n",
    "        np.random.poisson(5, n_samples),\n",
    "        np.random.poisson(8, n_samples)\n",
    "    )\n",
    "    num_credit_lines = np.clip(num_credit_lines, 0, 20)\n",
    "    \n",
    "    # Fixed: Create months_since_last_late properly\n",
    "    months_since_last_late = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        if target[i] == 0:\n",
    "            # Good customers: mostly no late payments (999) or few late payments\n",
    "            if np.random.random() < 0.7:\n",
    "                months_since_last_late[i] = 999  # Never late\n",
    "            else:\n",
    "                months_since_last_late[i] = np.random.exponential(50)  # Rare late\n",
    "        else:\n",
    "            # Bad customers: recent late payments\n",
    "            months_since_last_late[i] = np.random.exponential(10)\n",
    "    \n",
    "    # Create DataFrame (using 'h' instead of deprecated 'H')\n",
    "    df = pd.DataFrame({\n",
    "        'app_id': range(n_samples),\n",
    "        'app_dt': pd.date_range(start='2022-01-01', periods=n_samples, freq='h')[:n_samples],\n",
    "        'target': target,\n",
    "        'credit_score': credit_score.round(0).astype(int),\n",
    "        'debt_ratio': debt_ratio.round(3),\n",
    "        'payment_history_score': payment_history.round(1),\n",
    "        'income': income.round(0).astype(int),\n",
    "        'months_employed': months_employed.round(0).astype(int),\n",
    "        'num_delinquencies': num_delinquencies,\n",
    "        'utilization_rate': utilization_rate.round(3),\n",
    "        'age': age.round(0).astype(int),\n",
    "        'num_credit_lines': num_credit_lines,\n",
    "        'months_since_last_late': months_since_last_late.round(0).astype(int),\n",
    "        'education': education,\n",
    "        'employment_type': employment_type,\n",
    "        'home_ownership': home_ownership\n",
    "    })\n",
    "    \n",
    "    # Add some missing values (realistic pattern)\n",
    "    missing_idx = np.random.choice(n_samples, size=int(n_samples * 0.02), replace=False)\n",
    "    df.loc[missing_idx, 'months_since_last_late'] = np.nan\n",
    "    \n",
    "    missing_idx = np.random.choice(n_samples, size=int(n_samples * 0.01), replace=False)\n",
    "    df.loc[missing_idx, 'months_employed'] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create the dataset\n",
    "df = create_high_performance_credit_data(n_samples=10000)\n",
    "\n",
    "print(f\"Dataset created with shape: {df.shape}\")\n",
    "print(f\"Target event rate: {df['target'].mean():.2%}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Summary:\n",
      "==================================================\n",
      "Total samples: 10,000\n",
      "Total features: 13\n",
      "\n",
      "Target distribution:\n",
      "0    8500\n",
      "1    1500\n",
      "Name: target, dtype: int64\n",
      "Event rate: 15.00%\n",
      "\n",
      "Missing values:\n",
      "months_employed           100\n",
      "months_since_last_late    200\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "int32             6\n",
      "float64           5\n",
      "object            3\n",
      "int64             1\n",
      "datetime64[ns]    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data quality check\n",
    "print(\"Data Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"Total features: {len(df.columns) - 3}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"Event rate: {df['target'].mean():.2%}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"No missing values\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Full Pipeline Configuration\n",
    "\n",
    "Configure ALL pipeline parameters for complete testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline configured with ALL parameters:\n",
      "==================================================\n",
      "  Dual pipeline: True\n",
      "  Feature selection: Boruta=True, Forward=True\n",
      "  HPO: Enabled=True, Trials=1\n",
      "  Data split: Train=0.6, Test=0.2, OOT=0.2\n",
      "  WOE bins: 10, Monotonic=False\n",
      "  Feature limits: Min=5, Max=15\n",
      "  Thresholds: IV>0.02, PSI<0.25, Corr<0.9\n"
     ]
    }
   ],
   "source": [
    "# Configure pipeline with ALL parameters\n",
    "config = Config(\n",
    "    # Basic settings\n",
    "    target_col='target',\n",
    "    id_col='app_id',\n",
    "    time_col='app_dt',\n",
    "    output_folder='outputs',\n",
    "    random_state=42,\n",
    "    \n",
    "    # Feature selection parameters\n",
    "    iv_min=0.02,              # Minimum Information Value\n",
    "    iv_high_threshold=0.5,    # Maximum IV (detect overfitting)\n",
    "    psi_threshold=0.25,       # Population Stability Index threshold\n",
    "    rho_threshold=0.90,       # Correlation threshold\n",
    "    vif_threshold=5.0,        # Variance Inflation Factor threshold\n",
    "    rare_threshold=0.01,      # Rare category threshold\n",
    "    cluster_top_k=2,          # Top K features per cluster\n",
    "    max_features=15,          # Maximum number of features\n",
    "    min_features=5,           # Minimum number of features\n",
    "    \n",
    "    # WOE settings\n",
    "    n_bins=10,                # Number of bins for numeric features\n",
    "    min_bin_size=0.05,        # Minimum bin size (5% of data)\n",
    "    woe_monotonic=False,      # Enforce monotonic WOE\n",
    "    max_abs_woe=None,         # Cap absolute WOE values\n",
    "    handle_missing='as_category',  # How to handle missing values\n",
    "    \n",
    "    # HPO (Hyperparameter Optimization) settings\n",
    "    use_optuna=True,          # Enable Bayesian optimization\n",
    "    n_trials=1,             # Number of HPO trials\n",
    "    optuna_timeout=600,       # Maximum 10 minutes for optimization\n",
    "    cv_folds=5,               # Cross-validation folds\n",
    "    \n",
    "    # Feature selection methods\n",
    "    use_boruta=True,          # Boruta feature selection\n",
    "    forward_1se=True,         # Forward selection with 1SE rule\n",
    "    use_noise_sentinel=True,  # Add noise features for stability\n",
    "    \n",
    "    # Data splitting\n",
    "    use_test_split=True,      # Use separate test set\n",
    "    train_ratio=0.60,         # 60% for training\n",
    "    test_ratio=0.20,          # 20% for test\n",
    "    oot_ratio=0.20,           # 20% for out-of-time\n",
    "    oot_months=None,          # Use ratio instead of months\n",
    "    min_oot_size=50,          # Minimum OOT size\n",
    "    \n",
    "    # Dual pipeline settings\n",
    "    enable_dual_pipeline=True,  # Compare WOE vs RAW\n",
    "    \n",
    "    # RAW pipeline settings\n",
    "    raw_outlier_method='clip',     # Outlier handling: clip, remove, none\n",
    "    raw_outlier_threshold=3.0,     # Z-score threshold\n",
    "    raw_scaler_type='standard',    # Scaling: standard, minmax, robust\n",
    "    imputation_strategy='median',  # Imputation: mean, median, mode\n",
    "    \n",
    "    # Model selection criteria\n",
    "    model_selection_method='gini_oot',  # Selection: gini_oot, balanced, stable\n",
    "    max_train_oot_gap=None,            # Maximum train-OOT gap allowed\n",
    "    model_stability_weight=0.2,        # Weight for stability (0-1)\n",
    "    min_gini_threshold=0.3,            # Minimum acceptable Gini\n",
    "    \n",
    "    # Output settings\n",
    "    output_excel_path='pipeline_results.xlsx',  # Excel output\n",
    "    write_csv=True,                            # Also write CSV files\n",
    "    run_id=None                                # Auto-generate run ID\n",
    ")\n",
    "\n",
    "print(\"Pipeline configured with ALL parameters:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Dual pipeline: {config.enable_dual_pipeline}\")\n",
    "print(f\"  Feature selection: Boruta={config.use_boruta}, Forward={config.forward_1se}\")\n",
    "print(f\"  HPO: Enabled={config.use_optuna}, Trials={config.n_trials}\")\n",
    "print(f\"  Data split: Train={config.train_ratio}, Test={config.test_ratio}, OOT={config.oot_ratio}\")\n",
    "print(f\"  WOE bins: {config.n_bins}, Monotonic={config.woe_monotonic}\")\n",
    "print(f\"  Feature limits: Min={config.min_features}, Max={config.max_features}\")\n",
    "print(f\"  Thresholds: IV>{config.iv_min}, PSI<{config.psi_threshold}, Corr<{config.rho_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training with Dual Pipeline\n",
    "\n",
    "Train models using both WOE and RAW pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Dual Pipeline Training...\n",
      "============================================================\n",
      "\n",
      "Training models with both WOE and RAW features...\n",
      "This will take 5-10 minutes with HPO enabled...\n",
      "\n",
      "Steps that will be executed:\n",
      "  1. Data validation and freezing\n",
      "  2. Variable classification\n",
      "  3. Train/Test/OOT splitting\n",
      "  4. WOE transformation\n",
      "  5. PSI calculation\n",
      "  6. Feature correlation analysis\n",
      "  7. Feature selection (Boruta + Forward)\n",
      "  8. Model training with HPO\n",
      "  9. Model evaluation and selection\n",
      " 10. Report generation\n",
      "\n",
      "Progress:\n",
      "[17:51:56] >> 1) Data loading & preparation starting | CPU=4% RAM=25%\n",
      "   - Data size: 10,000 rows x 17 columns\n",
      "   - Target ratio: 15.00%\n",
      "   - Random seed: 42\n",
      "[17:51:56] √¢--  1) Data loading & preparation completed (0.10s) ‚Äî OK | CPU=1% RAM=25%\n",
      "[17:51:56] >> 2) Input validation & freezing starting | CPU=0% RAM=25%\n",
      "[17:51:56] √¢--  2) Input validation & freezing completed (0.12s) ‚Äî OK | CPU=2% RAM=25%\n",
      "[17:51:56] >> 3) Variable classification starting | CPU=1% RAM=25%\n",
      "   - numeric=10, categorical=4\n",
      "[17:51:56] √¢--  3) Variable classification completed (0.11s) ‚Äî OK | CPU=1% RAM=25%\n",
      "[17:51:56] >> 4) Missing & Rare value policy starting | CPU=1% RAM=25%\n",
      "[17:51:56] √¢--  4) Missing & Rare value policy completed (0.11s) ‚Äî OK | CPU=6% RAM=25%\n",
      "[17:51:56] >> 5) Time splitting (Train/Test/OOT) starting | CPU=6% RAM=25%\n",
      "   - Train=6406, Test=1594, OOT=2000\n",
      "[17:51:57] √¢--  5) Time splitting (Train/Test/OOT) completed (0.12s) ‚Äî OK | CPU=0% RAM=25%\n",
      "[17:51:57] >> 6) WOE binning (Train only; adaptive) starting | CPU=0% RAM=25%\n",
      "   - WOE ready: 14 variables\n",
      "   - Note: WOE mapping learned ONLY on TRAIN\n",
      "[17:51:57] √¢--  6) WOE binning (Train only; adaptive) completed (0.16s) ‚Äî OK | CPU=0% RAM=25%\n",
      "[17:51:57] >> 7) PSI (vectorized) starting | CPU=0% RAM=25%\n",
      "   * PSI summary: KEEP=13 | DROP=1 | WARN=0\n",
      "   - Remaining after PSI: 13\n",
      "[17:51:57] √¢--  7) PSI (vectorized) completed (0.37s) ‚Äî OK | CPU=0% RAM=25%\n",
      "   - High IV flags: credit_score,debt_ratio,payment_history_score,income,months_employed,num_delinquencies,utilization_rate,age,num_credit_lines,months_since_last_late,education,employment_type,home_ownership\n",
      "[17:51:57] >> 8) WOE transform (Train/Test/OOT) starting | CPU=6% RAM=25%\n",
      "   - X_train=(6406, 13), X_test=(1594, 13), X_oot=(2000, 13)\n",
      "[17:51:58] √¢--  8) WOE transform (Train/Test/OOT) completed (0.47s) ‚Äî OK | CPU=0% RAM=25%\n",
      "[17:51:58] >> 9) Correlation & clustering starting | CPU=0% RAM=25%\n",
      "   - cluster representatives=13\n",
      "[17:51:58] √¢--  9) Correlation & clustering completed (0.12s) ‚Äî OK | CPU=5% RAM=25%\n",
      "[17:51:58] >> 10) Feature selection (Forward+1SE) starting | CPU=0% RAM=25%\n",
      "   - Boruta failed: Please check your X and y variable. The provided estimator cannot be fitted to your data.\n",
      "X has 24 features, but LGBMClassifier is expecting 26 features as input., using fallback\n",
      "   - Boruta: 13/13 remained\n",
      "   - Forward+1SE selected: 6\n",
      "   - baseline variables=6\n",
      "[17:52:07] √¢--  10) Feature selection (Forward+1SE) completed (8.32s) ‚Äî OK | CPU=0% RAM=25%\n",
      "[17:52:07] >> 11) Final correlation filter starting | CPU=0% RAM=25%\n",
      "   - after correlation=6\n",
      "[17:52:07] √¢--  11) Final correlation filter completed (0.11s) ‚Äî OK | CPU=3% RAM=25%\n",
      "[17:52:07] >> 12) Noise sentinel starting | CPU=1% RAM=25%\n",
      "   - final variables=6\n",
      "[17:52:10] √¢--  12) Noise sentinel completed (2.70s) ‚Äî OK | CPU=0% RAM=25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:52:10,417] A new study created in memory with name: no-name-65d08f72-2050-4579-a7f8-e7584dcc1e89\n",
      "[I 2025-09-08 17:52:10,489] Trial 0 finished with value: 0.9815435618855203 and parameters: {}. Best is trial 0 with value: 0.9815435618855203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:52:10] >> 13) Modeling & evaluation (WOE) starting | CPU=0% RAM=25%\n",
      "[17:52:10]   - WOE_Logit_L2 tuning | CPU=0% RAM=25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:52:10,789] A new study created in memory with name: no-name-232b9400-b74c-43ac-b854-f3542e6aa5eb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:52:10]   - WOE_Logit_L2 CV starting | CPU=2% RAM=25%\n",
      "[17:52:10]   - WOE_RandomForest tuning | CPU=0% RAM=25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:52:19,654] Trial 0 finished with value: 0.9767190116792402 and parameters: {'n_estimators': 600, 'max_depth': None, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.9767190116792402.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:52:19]   - WOE_RandomForest CV starting | CPU=0% RAM=25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:52:32,436] A new study created in memory with name: no-name-0440fb9a-fe31-4320-867f-d104e694ae2a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:52:32]   - WOE_ExtraTrees tuning | CPU=0% RAM=25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:52:38,161] Trial 0 finished with value: 0.9780240399268514 and parameters: {'n_estimators': 600, 'max_depth': None, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.9780240399268514.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:52:38]   - WOE_ExtraTrees CV starting | CPU=0% RAM=25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:52:44,897] A new study created in memory with name: no-name-2b261254-26d4-4fa2-bbea-789514fa3949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:52:44]   - WOE_XGBoost tuning | CPU=2% RAM=25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:52:48,865] Trial 0 finished with value: 0.9783572317341017 and parameters: {'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'subsample': 0.7}. Best is trial 0 with value: 0.9783572317341017.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:52:48]   - WOE_XGBoost CV starting | CPU=48% RAM=26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:52:52,494] A new study created in memory with name: no-name-769458f4-01df-439d-85d9-07c20f768220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:52:52]   - WOE_LightGBM tuning | CPU=48% RAM=26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:53:04,953] Trial 0 finished with value: 0.9681568276581505 and parameters: {'n_estimators': 500, 'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.1, 'subsample': 1.0}. Best is trial 0 with value: 0.9681568276581505.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:53:04]   - WOE_LightGBM CV starting | CPU=41% RAM=26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:53:20,385] A new study created in memory with name: no-name-14adbcce-cb8a-4026-87b9-4f1fc697a1a2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:53:20]   - WOE_GAM tuning | CPU=56% RAM=26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:53:28,092] Trial 0 finished with value: 0.9806459775433062 and parameters: {}. Best is trial 0 with value: 0.9806459775433062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:53:28]   - WOE_GAM CV starting | CPU=0% RAM=26%\n",
      "[17:53:45] √¢--  13) Modeling & evaluation (WOE) completed (95.25s) ‚Äî OK | CPU=0% RAM=26%\n",
      "\n",
      "================================================================================\n",
      "DUAL PIPELINE: RAW VARIABLES\n",
      "================================================================================\n",
      "[17:53:45] >> 8b) Raw transform (Train/Test/OOT) starting | CPU=0% RAM=26%\n",
      "   - X_train_raw=(6406, 12), X_test_raw=(1594, 12), X_oot_raw=(2000, 12)\n",
      "[17:53:45] √¢--  8b) Raw transform (Train/Test/OOT) completed (0.12s) ‚Äî OK | CPU=0% RAM=26%\n",
      "[17:53:45] >> 10b) Feature selection RAW (Forward+1SE) starting | CPU=3% RAM=26%\n",
      "   - Boruta failed: Please check your X and y variable. The provided estimator cannot be fitted to your data.\n",
      "X has 12 features, but LGBMClassifier is expecting 24 features as input., using fallback\n",
      "   - Boruta: 12/12 remained\n",
      "   - Forward+1SE selected: 6\n",
      "   - raw baseline variables=6\n",
      "[17:53:53] √¢--  10b) Feature selection RAW (Forward+1SE) completed (7.94s) ‚Äî OK | CPU=2% RAM=26%\n",
      "[17:53:53] >> 11b) Final correlation filter RAW starting | CPU=0% RAM=26%\n",
      "   - raw after correlation=6\n",
      "[17:53:53] √¢--  11b) Final correlation filter RAW completed (0.11s) ‚Äî OK | CPU=1% RAM=26%\n",
      "[17:53:54] >> 12b) Noise sentinel RAW starting | CPU=2% RAM=26%\n",
      "   - raw final variables=6\n",
      "[17:53:56] √¢--  12b) Noise sentinel RAW completed (2.52s) ‚Äî OK | CPU=0% RAM=26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:53:56,918] A new study created in memory with name: no-name-ceac7989-cc40-44b6-aa42-5e785461c28e\n",
      "[I 2025-09-08 17:53:56,984] Trial 0 finished with value: 0.8857082680035318 and parameters: {}. Best is trial 0 with value: 0.8857082680035318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:53:56] >> 13b) Modeling & evaluation (RAW) starting | CPU=0% RAM=26%\n",
      "[17:53:56]   - RAW_Logit_L2 tuning | CPU=5% RAM=26%\n",
      "[17:53:56]   - RAW_Logit_L2 CV starting | CPU=1% RAM=26%\n",
      "[17:53:57]   - RAW_RandomForest tuning | CPU=0% RAM=26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:53:57,280] A new study created in memory with name: no-name-7bf29837-db2c-4189-9996-f3137ab4cd58\n",
      "[I 2025-09-08 17:54:10,306] Trial 0 finished with value: 0.9133410557348306 and parameters: {'n_estimators': 600, 'max_depth': None, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.9133410557348306.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:54:10]   - RAW_RandomForest CV starting | CPU=1% RAM=26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:54:26,377] A new study created in memory with name: no-name-c41a51ae-8c2d-42cf-8dd4-6edd90f29195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:54:26]   - RAW_ExtraTrees tuning | CPU=7% RAM=26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:54:32,334] Trial 0 finished with value: 0.9187965032976495 and parameters: {'n_estimators': 600, 'max_depth': None, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.9187965032976495.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:54:32]   - RAW_ExtraTrees CV starting | CPU=3% RAM=26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:54:40,434] A new study created in memory with name: no-name-f97aaf38-a606-4ecb-8db5-d7bd786ce0ff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:54:40]   - RAW_XGBoost tuning | CPU=0% RAM=26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:54:43,235] Trial 0 finished with value: 0.913910622939951 and parameters: {'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'subsample': 0.7}. Best is trial 0 with value: 0.913910622939951.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:54:43]   - RAW_XGBoost CV starting | CPU=64% RAM=26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:54:46,850] A new study created in memory with name: no-name-42df2a2a-c50a-4a40-b296-06ad7a12a586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:54:46]   - RAW_LightGBM tuning | CPU=32% RAM=26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:54:59,044] Trial 0 finished with value: 0.9071287807495201 and parameters: {'n_estimators': 500, 'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.1, 'subsample': 1.0}. Best is trial 0 with value: 0.9071287807495201.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:54:59]   - RAW_LightGBM CV starting | CPU=69% RAM=26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:55:13,881] A new study created in memory with name: no-name-5e010d20-9b17-4980-a9c4-041dd4c7476e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:55:13]   - RAW_GAM tuning | CPU=53% RAM=26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 17:55:18,045] Trial 0 finished with value: 0.9221794120003646 and parameters: {}. Best is trial 0 with value: 0.9221794120003646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:55:18]   - RAW_GAM CV starting | CPU=5% RAM=26%\n",
      "[17:55:30] √¢--  13b) Modeling & evaluation (RAW) completed (93.60s) ‚Äî OK | CPU=2% RAM=26%\n",
      "\n",
      "================================================================================\n",
      "DUAL PIPELINE SUMMARY\n",
      "================================================================================\n",
      "WOE Pipeline: 6 variables, 6 models\n",
      "RAW Pipeline: 6 variables, 12 models\n",
      "Best WOE Model: WOE_Logit_L2 - Gini OOT: 0.9983\n",
      "Best RAW Model: RAW_XGBoost - Gini OOT: 0.9693\n",
      "[17:55:30] >> 14) Best model selection starting | CPU=0% RAM=26%\n",
      "   - Selection method: gini_oot\n",
      "   - Selected: WOE_Logit_L2\n",
      "     Gini_OOT=0.9983, Train-OOT Gap=0.0009\n",
      "   - best=WOE_Logit_L2\n",
      "[17:55:30] √¢--  14) Best model selection completed (0.11s) ‚Äî OK | CPU=2% RAM=26%\n",
      "[17:55:30] >> 14b) SHAP analysis starting | CPU=1% RAM=26%\n",
      "   - SHAP values computed for 6 features\n",
      "[17:55:31] √¢--  14b) SHAP analysis completed (0.13s) ‚Äî OK | CPU=2% RAM=26%\n",
      "[17:55:31] >> 15) Report tables starting | CPU=1% RAM=26%\n",
      "[17:55:31] √¢--  15) Report tables completed (0.13s) ‚Äî OK | CPU=0% RAM=26%\n",
      "[17:55:31] >> 15b) Export (Excel/Parquet) starting | CPU=0% RAM=26%\n",
      "[17:55:31] √¢--  15b) Export (Excel/Parquet) completed (0.53s) ‚Äî OK | CPU=0% RAM=26%\n",
      "[17:55:32] >> RUN complete - run_id=20250908_175153 | CPU=4% RAM=26%\n",
      "\n",
      "‚úÖ Pipeline completed successfully!\n",
      "\n",
      "Pipeline Summary:\n",
      "==================================================\n",
      "  Best pipeline type: None\n",
      "  WOE features selected: 6\n",
      "  RAW features selected: 6\n",
      "  Total models trained: 12\n",
      "  Best model: WOE_Logit_L2\n"
     ]
    }
   ],
   "source": [
    "# Run the dual pipeline\n",
    "print(\"Starting Dual Pipeline Training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = DualPipeline(config)\n",
    "\n",
    "print(\"\\nTraining models with both WOE and RAW features...\")\n",
    "print(\"This will take 5-10 minutes with HPO enabled...\")\n",
    "print(\"\\nSteps that will be executed:\")\n",
    "print(\"  1. Data validation and freezing\")\n",
    "print(\"  2. Variable classification\")\n",
    "print(\"  3. Train/Test/OOT splitting\")\n",
    "print(\"  4. WOE transformation\")\n",
    "print(\"  5. PSI calculation\")\n",
    "print(\"  6. Feature correlation analysis\")\n",
    "print(\"  7. Feature selection (Boruta + Forward)\")\n",
    "print(\"  8. Model training with HPO\")\n",
    "print(\"  9. Model evaluation and selection\")\n",
    "print(\" 10. Report generation\")\n",
    "print(\"\\nProgress:\")\n",
    "\n",
    "try:\n",
    "    # Run pipeline\n",
    "    pipeline.run(df)\n",
    "    \n",
    "    print(\"\\n‚úÖ Pipeline completed successfully!\")\n",
    "    \n",
    "    # Get summary\n",
    "    summary = pipeline.get_summary()\n",
    "    \n",
    "    print(\"\\nPipeline Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"  Best pipeline type: {summary['best_pipeline']}\")\n",
    "    print(f\"  WOE features selected: {summary['n_features_woe']}\")\n",
    "    print(f\"  RAW features selected: {summary['n_features_raw']}\")\n",
    "    print(f\"  Total models trained: {len(pipeline.models_summary_)}\")\n",
    "    print(f\"  Best model: {pipeline.best_model_name_}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during pipeline execution: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Performance Analysis\n",
    "\n",
    "Analyze model performance - targeting 70-80% Train Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Summary:\n",
      "================================================================================\n",
      "\n",
      "All Models Trained:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>Gini_Train</th>\n",
       "      <th>Gini_Test</th>\n",
       "      <th>Gini_OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WOE_Logit_L2</td>\n",
       "      <td>0.999150</td>\n",
       "      <td>0.999040</td>\n",
       "      <td>0.998299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WOE_RandomForest</td>\n",
       "      <td>0.999211</td>\n",
       "      <td>0.998406</td>\n",
       "      <td>0.997685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WOE_ExtraTrees</td>\n",
       "      <td>0.999383</td>\n",
       "      <td>0.998720</td>\n",
       "      <td>0.998068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WOE_XGBoost</td>\n",
       "      <td>0.998991</td>\n",
       "      <td>0.998972</td>\n",
       "      <td>0.997117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WOE_LightGBM</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>0.997440</td>\n",
       "      <td>0.997283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WOE_GAM</td>\n",
       "      <td>0.999217</td>\n",
       "      <td>0.999052</td>\n",
       "      <td>0.998064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RAW_Logit_L2</td>\n",
       "      <td>0.958642</td>\n",
       "      <td>0.950788</td>\n",
       "      <td>0.945331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RAW_RandomForest</td>\n",
       "      <td>0.991244</td>\n",
       "      <td>0.967762</td>\n",
       "      <td>0.964782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RAW_ExtraTrees</td>\n",
       "      <td>0.983541</td>\n",
       "      <td>0.968513</td>\n",
       "      <td>0.967207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RAW_XGBoost</td>\n",
       "      <td>0.980811</td>\n",
       "      <td>0.969092</td>\n",
       "      <td>0.969252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RAW_LightGBM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964519</td>\n",
       "      <td>0.958235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RAW_GAM</td>\n",
       "      <td>0.979386</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>0.967916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model_name  Gini_Train  Gini_Test  Gini_OOT\n",
       "0       WOE_Logit_L2    0.999150   0.999040  0.998299\n",
       "1   WOE_RandomForest    0.999211   0.998406  0.997685\n",
       "2     WOE_ExtraTrees    0.999383   0.998720  0.998068\n",
       "3        WOE_XGBoost    0.998991   0.998972  0.997117\n",
       "4       WOE_LightGBM    0.999731   0.997440  0.997283\n",
       "5            WOE_GAM    0.999217   0.999052  0.998064\n",
       "6       RAW_Logit_L2    0.958642   0.950788  0.945331\n",
       "7   RAW_RandomForest    0.991244   0.967762  0.964782\n",
       "8     RAW_ExtraTrees    0.983541   0.968513  0.967207\n",
       "9        RAW_XGBoost    0.980811   0.969092  0.969252\n",
       "10      RAW_LightGBM    1.000000   0.964519  0.958235\n",
       "11           RAW_GAM    0.979386   0.970612  0.967916"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "BEST MODEL: WOE_Logit_L2\n",
      "==================================================\n",
      "  Train Gini: 0.9991 (99.9%)\n",
      "  Test Gini:  0.9990 (99.9%)\n",
      "  OOT Gini:   0.9983 (99.8%)\n",
      "  Train-OOT Gap: 0.0009\n",
      "\n",
      "üìä Performance Check:\n",
      "  ‚ö†Ô∏è Train Gini 99.9% is outside target range (70-80%)\n",
      "  ‚úÖ Model is stable (Train-OOT gap < 5%)\n"
     ]
    }
   ],
   "source": [
    "# View detailed model results\n",
    "if hasattr(pipeline, 'models_summary_'):\n",
    "    print(\"Model Performance Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    summary_df = pipeline.models_summary_\n",
    "    \n",
    "    # Check which columns are available\n",
    "    available_cols = summary_df.columns.tolist()\n",
    "    \n",
    "    # Determine the feature count column name\n",
    "    feature_col = None\n",
    "    for col in ['n_features', 'n_vars', 'num_features', 'n_selected_features']:\n",
    "        if col in available_cols:\n",
    "            feature_col = col\n",
    "            break\n",
    "    \n",
    "    # Show all models\n",
    "    print(\"\\nAll Models Trained:\")\n",
    "    display_cols = ['model_name', 'Gini_Train', 'Gini_OOT']\n",
    "    if 'Gini_Test' in available_cols:\n",
    "        display_cols.insert(2, 'Gini_Test')\n",
    "    if feature_col:\n",
    "        display_cols.append(feature_col)\n",
    "    \n",
    "    display_cols = [col for col in display_cols if col in available_cols]\n",
    "    display(summary_df[display_cols])\n",
    "    \n",
    "    # Best model details\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"BEST MODEL: {pipeline.best_model_name_}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    best_row = summary_df[summary_df['model_name'] == pipeline.best_model_name_].iloc[0]\n",
    "    \n",
    "    train_gini = best_row['Gini_Train']\n",
    "    oot_gini = best_row['Gini_OOT']\n",
    "    \n",
    "    print(f\"  Train Gini: {train_gini:.4f} ({train_gini*100:.1f}%)\")\n",
    "    if 'Gini_Test' in available_cols:\n",
    "        test_gini = best_row['Gini_Test']\n",
    "        print(f\"  Test Gini:  {test_gini:.4f} ({test_gini*100:.1f}%)\")\n",
    "    print(f\"  OOT Gini:   {oot_gini:.4f} ({oot_gini*100:.1f}%)\")\n",
    "    print(f\"  Train-OOT Gap: {abs(train_gini - oot_gini):.4f}\")\n",
    "    \n",
    "    if feature_col:\n",
    "        print(f\"  Features used: {int(best_row[feature_col])}\")\n",
    "    \n",
    "    # Check if we achieved target performance\n",
    "    print(f\"\\nüìä Performance Check:\")\n",
    "    if 0.70 <= train_gini <= 0.80:\n",
    "        print(f\"  ‚úÖ TARGET ACHIEVED! Train Gini {train_gini:.1%} is in the 70-80% range\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Train Gini {train_gini:.1%} is outside target range (70-80%)\")\n",
    "    \n",
    "    if abs(train_gini - oot_gini) <= 0.05:\n",
    "        print(f\"  ‚úÖ Model is stable (Train-OOT gap < 5%)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Potential overfitting (Train-OOT gap > 5%)\")\n",
    "else:\n",
    "    print(\"No model results available. Please run the pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Probability Calibration\n",
    "\n",
    "Calibrate model probabilities using Isotonic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing calibration data...\n",
      "==================================================\n",
      "Calibration data shape: (2000, 16)\n",
      "Calibration target rate: 15.00%\n",
      "\n",
      "Uncalibrated predictions generated: 2000\n",
      "\n",
      "Before calibration:\n",
      "  Actual event rate:     0.1500\n",
      "  Mean predicted prob:   0.5000\n",
      "  Calibration error:     0.3500\n",
      "  Min probability:       0.0000\n",
      "  Max probability:       1.0000\n"
     ]
    }
   ],
   "source": [
    "# Prepare calibration dataset\n",
    "print(\"Preparing calibration data...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a separate calibration dataset\n",
    "calib_df = create_high_performance_credit_data(n_samples=2000)\n",
    "print(f\"Calibration data shape: {calib_df.shape}\")\n",
    "print(f\"Calibration target rate: {calib_df['target'].mean():.2%}\")\n",
    "\n",
    "# Get predictions on calibration data\n",
    "if hasattr(pipeline, 'predict_proba'):\n",
    "    uncalibrated_probs = pipeline.predict_proba(calib_df)\n",
    "    print(f\"\\nUncalibrated predictions generated: {len(uncalibrated_probs)}\")\n",
    "    \n",
    "    # Check calibration before\n",
    "    print(f\"\\nBefore calibration:\")\n",
    "    print(f\"  Actual event rate:     {calib_df['target'].mean():.4f}\")\n",
    "    print(f\"  Mean predicted prob:   {uncalibrated_probs.mean():.4f}\")\n",
    "    print(f\"  Calibration error:     {abs(calib_df['target'].mean() - uncalibrated_probs.mean()):.4f}\")\n",
    "    print(f\"  Min probability:       {uncalibrated_probs.min():.4f}\")\n",
    "    print(f\"  Max probability:       {uncalibrated_probs.max():.4f}\")\n",
    "else:\n",
    "    print(\"Error: predict_proba method not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Isotonic Regression Calibration...\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Isotonic regression input X should be a 1d array or 2d array with 1 feature",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20568\\1202020682.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Fit isotonic regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0miso_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIsotonicRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_of_bounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'clip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcalibrated_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miso_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muncalibrated_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcalib_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nAfter calibration:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 881\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\isotonic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[1;31m# Transform y by running the isotonic regression algorithm and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[1;31m# transform X accordingly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;31m# It is necessary to store the non-redundant part of the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\isotonic.py\u001b[0m in \u001b[0;36m_build_y\u001b[1;34m(self, X, y, sample_weight, trim_duplicates)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_build_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_duplicates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;34m\"\"\"Build the y_ IsotonicRegression.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_input_data_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# use 1d view\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\isotonic.py\u001b[0m in \u001b[0;36m_check_input_data_shape\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    248\u001b[0m                 \u001b[1;34m\"2d array with 1 feature\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             )\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_build_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Isotonic regression input X should be a 1d array or 2d array with 1 feature"
     ]
    }
   ],
   "source": [
    "# Apply Isotonic Regression calibration\n",
    "print(\"Applying Isotonic Regression Calibration...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Fit isotonic regression\n",
    "iso_reg = IsotonicRegression(out_of_bounds='clip')\n",
    "calibrated_probs = iso_reg.fit_transform(uncalibrated_probs, calib_df['target'])\n",
    "\n",
    "print(f\"\\nAfter calibration:\")\n",
    "print(f\"  Actual event rate:     {calib_df['target'].mean():.4f}\")\n",
    "print(f\"  Mean calibrated prob:  {calibrated_probs.mean():.4f}\")\n",
    "print(f\"  Calibration error:     {abs(calib_df['target'].mean() - calibrated_probs.mean()):.4f}\")\n",
    "print(f\"  Min probability:       {calibrated_probs.min():.4f}\")\n",
    "print(f\"  Max probability:       {calibrated_probs.max():.4f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "error_before = abs(calib_df['target'].mean() - uncalibrated_probs.mean())\n",
    "error_after = abs(calib_df['target'].mean() - calibrated_probs.mean())\n",
    "improvement = (error_before - error_after) / error_before * 100\n",
    "\n",
    "print(f\"\\nüìä Calibration improved by {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (2000, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20568\\926536324.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Before calibration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     fraction_pos_before, mean_pred_before = calibration_curve(\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mcalib_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muncalibrated_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_bins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py\u001b[0m in \u001b[0;36mcalibration_curve\u001b[1;34m(y_true, y_prob, pos_label, normalize, n_bins, strategy)\u001b[0m\n\u001b[0;32m    992\u001b[0m     \"\"\"\n\u001b[0;32m    993\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m     \u001b[0my_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m     \u001b[0mpos_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_pos_label_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m     raise ValueError(\n\u001b[0m\u001b[0;32m   1203\u001b[0m         \u001b[1;34m\"y should be a 1d array, got an array of shape {} instead.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (2000, 2) instead."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize calibration curves (if matplotlib is available)\n",
    "if PLOT_AVAILABLE:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Before calibration\n",
    "    fraction_pos_before, mean_pred_before = calibration_curve(\n",
    "        calib_df['target'], uncalibrated_probs, n_bins=10\n",
    "    )\n",
    "    axes[0].plot(mean_pred_before, fraction_pos_before, marker='o', \n",
    "                 linewidth=2, label='Model', color='blue')\n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect calibration')\n",
    "    axes[0].set_xlabel('Mean Predicted Probability', fontsize=12)\n",
    "    axes[0].set_ylabel('Fraction of Positives', fontsize=12)\n",
    "    axes[0].set_title('Before Calibration', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(loc='lower right')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_xlim([0, 1])\n",
    "    axes[0].set_ylim([0, 1])\n",
    "\n",
    "    # After calibration\n",
    "    fraction_pos_after, mean_pred_after = calibration_curve(\n",
    "        calib_df['target'], calibrated_probs, n_bins=10\n",
    "    )\n",
    "    axes[1].plot(mean_pred_after, fraction_pos_after, marker='o', \n",
    "                 linewidth=2, label='Calibrated', color='green')\n",
    "    axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect calibration')\n",
    "    axes[1].set_xlabel('Mean Predicted Probability', fontsize=12)\n",
    "    axes[1].set_ylabel('Fraction of Positives', fontsize=12)\n",
    "    axes[1].set_title('After Calibration', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(loc='lower right')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_xlim([0, 1])\n",
    "    axes[1].set_ylim([0, 1])\n",
    "\n",
    "    plt.suptitle('Probability Calibration Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n‚úÖ Calibration curves show improved alignment with diagonal after calibration\")\n",
    "else:\n",
    "    # Show calibration statistics without plots\n",
    "    fraction_pos_before, mean_pred_before = calibration_curve(\n",
    "        calib_df['target'], uncalibrated_probs, n_bins=10\n",
    "    )\n",
    "    fraction_pos_after, mean_pred_after = calibration_curve(\n",
    "        calib_df['target'], calibrated_probs, n_bins=10\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCalibration Analysis (without plots):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Before calibration - Mean absolute error:\")\n",
    "    mae_before = np.mean(np.abs(fraction_pos_before - mean_pred_before))\n",
    "    print(f\"  MAE: {mae_before:.4f}\")\n",
    "    \n",
    "    print(\"\\nAfter calibration - Mean absolute error:\")\n",
    "    mae_after = np.mean(np.abs(fraction_pos_after - mean_pred_after))\n",
    "    print(f\"  MAE: {mae_after:.4f}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Calibration improved by {((mae_before - mae_after) / mae_before * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Scoring Simulation\n",
    "\n",
    "Apply the calibrated model to new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRODUCTION SCORING SIMULATION\n",
      "============================================================\n",
      "\n",
      "Creating new production data...\n",
      "Production data shape: (5000, 16)\n",
      "Production target rate: 15.00%\n",
      "\n",
      "Applying trained model to production data...\n",
      "Raw predictions generated: 5000 samples\n",
      "\n",
      "Applying calibration to production predictions...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Isotonic regression input X should be a 1d array or 2d array with 1 feature",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20568\\3234003964.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Apply calibration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nApplying calibration to production predictions...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mcalibrated_prod_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miso_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Calibrated predictions generated: {len(calibrated_prod_probs)} samples\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\isotonic.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, T)\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m         \"\"\"\n\u001b[1;32m--> 409\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\isotonic.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, T)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_input_data_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m         \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# use 1d view\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\isotonic.py\u001b[0m in \u001b[0;36m_check_input_data_shape\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    248\u001b[0m                 \u001b[1;34m\"2d array with 1 feature\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             )\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_build_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Isotonic regression input X should be a 1d array or 2d array with 1 feature"
     ]
    }
   ],
   "source": [
    "# Create completely new dataset for production scoring\n",
    "print(\"PRODUCTION SCORING SIMULATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nCreating new production data...\")\n",
    "score_df = create_high_performance_credit_data(n_samples=5000)\n",
    "print(f\"Production data shape: {score_df.shape}\")\n",
    "print(f\"Production target rate: {score_df['target'].mean():.2%}\")\n",
    "\n",
    "# Apply the trained model\n",
    "print(\"\\nApplying trained model to production data...\")\n",
    "raw_probs = pipeline.predict_proba(score_df)\n",
    "print(f\"Raw predictions generated: {len(raw_probs)} samples\")\n",
    "\n",
    "# Apply calibration\n",
    "print(\"\\nApplying calibration to production predictions...\")\n",
    "calibrated_prod_probs = iso_reg.transform(raw_probs)\n",
    "print(f\"Calibrated predictions generated: {len(calibrated_prod_probs)} samples\")\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_raw = roc_auc_score(score_df['target'], raw_probs)\n",
    "auc_calibrated = roc_auc_score(score_df['target'], calibrated_prod_probs)\n",
    "gini_raw = 2 * auc_raw - 1\n",
    "gini_calibrated = 2 * auc_calibrated - 1\n",
    "\n",
    "print(f\"\\nProduction Performance:\")\n",
    "print(f\"  Raw Model:\")\n",
    "print(f\"    - AUC:  {auc_raw:.4f}\")\n",
    "print(f\"    - Gini: {gini_raw:.4f} ({gini_raw*100:.1f}%)\")\n",
    "print(f\"  Calibrated Model:\")\n",
    "print(f\"    - AUC:  {auc_calibrated:.4f}\")\n",
    "print(f\"    - Gini: {gini_calibrated:.4f} ({gini_calibrated*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Credit Score Transformation\n",
    "\n",
    "Convert calibrated probabilities to standard credit scores (300-850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calibrated_prod_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20568\\619119955.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Convert calibrated probabilities to credit scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobability_to_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalibrated_prod_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Credit Score Statistics:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'calibrated_prod_probs' is not defined"
     ]
    }
   ],
   "source": [
    "def probability_to_score(probs, base_score=600, pdo=20):\n",
    "    \"\"\"\n",
    "    Convert probability to credit score using standard formula:\n",
    "    Score = Base_Score - PDO * log(odds)\n",
    "    \n",
    "    Parameters:\n",
    "    - base_score: Score at odds of 1:1 (typically 600)\n",
    "    - pdo: Points to Double Odds (typically 20)\n",
    "    \"\"\"\n",
    "    # Clip probabilities to avoid inf values\n",
    "    probs_safe = np.clip(probs, 0.001, 0.999)\n",
    "    \n",
    "    # Calculate odds\n",
    "    odds = probs_safe / (1 - probs_safe)\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = base_score - pdo * np.log(odds)\n",
    "    \n",
    "    # Round to nearest integer\n",
    "    scores = np.round(scores).astype(int)\n",
    "    \n",
    "    # Ensure scores are in valid range\n",
    "    scores = np.clip(scores, 300, 850)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Convert calibrated probabilities to credit scores\n",
    "scores = probability_to_score(calibrated_prod_probs)\n",
    "\n",
    "print(\"Credit Score Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Min score:    {scores.min()}\")\n",
    "print(f\"  Max score:    {scores.max()}\")\n",
    "print(f\"  Mean score:   {scores.mean():.0f}\")\n",
    "print(f\"  Median score: {np.median(scores):.0f}\")\n",
    "print(f\"  Std dev:      {scores.std():.0f}\")\n",
    "\n",
    "# Score distribution by ranges\n",
    "print(\"\\nScore Distribution:\")\n",
    "print(\"-\" * 30)\n",
    "ranges = [(300, 400), (400, 500), (500, 600), (600, 700), (700, 800), (800, 850)]\n",
    "for low, high in ranges:\n",
    "    mask = (scores >= low) & (scores < high)\n",
    "    count = mask.sum()\n",
    "    pct = count / len(scores) * 100\n",
    "    default_rate = score_df.loc[mask, 'target'].mean() * 100\n",
    "    print(f\"  {low:3d}-{high:3d}: {count:4d} ({pct:5.1f}%) - Default rate: {default_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Risk Segmentation and Tiering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20568\\3777685384.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Assign risk tiers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mrisk_tiers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_risk_tier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Create results dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "# Define risk tiers based on credit scores\n",
    "def assign_risk_tier(score):\n",
    "    if score >= 750:\n",
    "        return 'A: Prime'\n",
    "    elif score >= 700:\n",
    "        return 'B: Near Prime'\n",
    "    elif score >= 650:\n",
    "        return 'C: Standard'\n",
    "    elif score >= 600:\n",
    "        return 'D: Subprime'\n",
    "    elif score >= 550:\n",
    "        return 'E: Deep Subprime'\n",
    "    else:\n",
    "        return 'F: High Risk'\n",
    "\n",
    "# Assign risk tiers\n",
    "risk_tiers = pd.Series(scores).apply(assign_risk_tier)\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'app_id': score_df['app_id'],\n",
    "    'actual_target': score_df['target'],\n",
    "    'raw_probability': raw_probs,\n",
    "    'calibrated_probability': calibrated_prod_probs,\n",
    "    'credit_score': scores,\n",
    "    'risk_tier': risk_tiers\n",
    "})\n",
    "\n",
    "# Risk tier analysis\n",
    "tier_analysis = results_df.groupby('risk_tier').agg({\n",
    "    'app_id': 'count',\n",
    "    'actual_target': 'mean',\n",
    "    'calibrated_probability': 'mean',\n",
    "    'credit_score': ['mean', 'min', 'max']\n",
    "}).round(3)\n",
    "\n",
    "tier_analysis.columns = ['Count', 'Actual_Rate', 'Avg_Prob', 'Mean_Score', 'Min_Score', 'Max_Score']\n",
    "tier_analysis = tier_analysis.sort_values('Mean_Score', ascending=False)\n",
    "\n",
    "print(\"Risk Tier Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "display(tier_analysis)\n",
    "\n",
    "# Calculate lift and odds\n",
    "base_rate = results_df['actual_target'].mean()\n",
    "tier_analysis['Lift'] = (tier_analysis['Actual_Rate'] / base_rate).round(2)\n",
    "tier_analysis['Odds'] = ((1 - tier_analysis['Actual_Rate']) / tier_analysis['Actual_Rate']).round(1)\n",
    "\n",
    "print(f\"\\nBase default rate: {base_rate:.2%}\")\n",
    "print(\"\\nLift and Odds by Risk Tier:\")\n",
    "for tier in tier_analysis.index:\n",
    "    lift = tier_analysis.loc[tier, 'Lift']\n",
    "    odds = tier_analysis.loc[tier, 'Odds']\n",
    "    print(f\"  {tier:20s}: Lift={lift:.2f}x, Odds={odds:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20568\\2291080978.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# 1. Score distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'black'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'skyblue'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'red'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'--'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf'Mean: {scores.mean():.0f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Credit Score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create comprehensive visualization (if matplotlib is available)\n",
    "if PLOT_AVAILABLE:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "    # 1. Score distribution\n",
    "    axes[0, 0].hist(scores, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].axvline(scores.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {scores.mean():.0f}')\n",
    "    axes[0, 0].set_xlabel('Credit Score', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0, 0].set_title('Credit Score Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Default rate by score decile\n",
    "    score_deciles = pd.qcut(scores, q=10, labels=[f'D{i+1}' for i in range(10)])\n",
    "    decile_default = score_df.groupby(score_deciles)['target'].mean()\n",
    "    axes[0, 1].bar(range(len(decile_default)), decile_default.values, \n",
    "                   color='coral', edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Score Decile', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Default Rate', fontsize=12)\n",
    "    axes[0, 1].set_title('Default Rate by Score Decile', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xticks(range(len(decile_default)))\n",
    "    axes[0, 1].set_xticklabels(decile_default.index)\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # 3. ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(score_df['target'], calibrated_prod_probs)\n",
    "    auc = roc_auc_score(score_df['target'], calibrated_prod_probs)\n",
    "    gini = 2 * auc - 1\n",
    "    axes[0, 2].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC={auc:.3f}, Gini={gini:.3f})', color='green')\n",
    "    axes[0, 2].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "    axes[0, 2].fill_between(fpr, tpr, alpha=0.3, color='green')\n",
    "    axes[0, 2].set_xlabel('False Positive Rate', fontsize=12)\n",
    "    axes[0, 2].set_ylabel('True Positive Rate', fontsize=12)\n",
    "    axes[0, 2].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "    axes[0, 2].legend(loc='lower right')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Calibrated probability distribution\n",
    "    axes[1, 0].hist(calibrated_prod_probs, bins=30, edgecolor='black', alpha=0.7, color='purple')\n",
    "    axes[1, 0].axvline(calibrated_prod_probs.mean(), color='red', linestyle='--', \n",
    "                       linewidth=2, label=f'Mean: {calibrated_prod_probs.mean():.3f}')\n",
    "    axes[1, 0].set_xlabel('Calibrated Probability', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1, 0].set_title('Calibrated Probability Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. Score vs Probability\n",
    "    axes[1, 1].scatter(calibrated_prod_probs, scores, alpha=0.5, s=1, color='blue')\n",
    "    axes[1, 1].set_xlabel('Calibrated Probability', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Credit Score', fontsize=12)\n",
    "    axes[1, 1].set_title('Score vs Probability Mapping', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. Risk tier distribution\n",
    "    tier_counts = results_df['risk_tier'].value_counts().sort_index()\n",
    "    colors = ['green', 'lightgreen', 'yellow', 'orange', 'coral', 'red']\n",
    "    axes[1, 2].pie(tier_counts.values, labels=tier_counts.index, autopct='%1.1f%%', \n",
    "                   colors=colors[:len(tier_counts)], startangle=90)\n",
    "    axes[1, 2].set_title('Risk Tier Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.suptitle('Production Scoring Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nVisualization Analysis (text-based due to matplotlib error):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Score distribution stats\n",
    "    print(\"\\n1. Credit Score Distribution:\")\n",
    "    print(f\"   - Min: {scores.min()}\")\n",
    "    print(f\"   - Max: {scores.max()}\")\n",
    "    print(f\"   - Mean: {scores.mean():.0f}\")\n",
    "    print(f\"   - Std: {scores.std():.0f}\")\n",
    "    \n",
    "    # 2. Default rate by decile\n",
    "    print(\"\\n2. Default Rate by Score Decile:\")\n",
    "    score_deciles = pd.qcut(scores, q=10, labels=[f'D{i+1}' for i in range(10)])\n",
    "    decile_default = score_df.groupby(score_deciles)['target'].mean()\n",
    "    for decile, rate in decile_default.items():\n",
    "        print(f\"   - {decile}: {rate:.2%}\")\n",
    "    \n",
    "    # 3. ROC metrics\n",
    "    fpr, tpr, _ = roc_curve(score_df['target'], calibrated_prod_probs)\n",
    "    auc = roc_auc_score(score_df['target'], calibrated_prod_probs)\n",
    "    gini = 2 * auc - 1\n",
    "    print(f\"\\n3. ROC Metrics:\")\n",
    "    print(f\"   - AUC: {auc:.3f}\")\n",
    "    print(f\"   - Gini: {gini:.3f}\")\n",
    "    \n",
    "    # 4. Risk tier distribution\n",
    "    print(\"\\n4. Risk Tier Distribution:\")\n",
    "    tier_counts = results_df['risk_tier'].value_counts()\n",
    "    for tier, count in tier_counts.items():\n",
    "        pct = count / len(results_df) * 100\n",
    "        print(f\"   - {tier}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20568\\2178627535.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Prepare final scoring output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m final_output = pd.DataFrame({\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;34m'app_id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'app_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;34m'score_date'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m'credit_score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'credit_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare final scoring output\n",
    "final_output = pd.DataFrame({\n",
    "    'app_id': results_df['app_id'],\n",
    "    'score_date': pd.Timestamp.now().date(),\n",
    "    'credit_score': results_df['credit_score'],\n",
    "    'risk_tier': results_df['risk_tier'],\n",
    "    'raw_probability': results_df['raw_probability'].round(4),\n",
    "    'calibrated_probability': results_df['calibrated_probability'].round(4),\n",
    "    'model_version': pipeline.best_model_name_\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'production_scoring_results.csv'\n",
    "final_output.to_csv(output_file, index=False)\n",
    "print(f\"Scoring results saved to: {output_file}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of scoring output:\")\n",
    "display(final_output.head(10))\n",
    "\n",
    "# Save calibration model\n",
    "import pickle\n",
    "with open('calibration_model.pkl', 'wb') as f:\n",
    "    pickle.dump(iso_reg, f)\n",
    "print(\"\\n‚úÖ Calibration model saved to: calibration_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Complete Workflow Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPLETE END-TO-END WORKFLOW SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATA PREPARATION:\")\n",
    "print(f\"   - Dataset size: {len(df):,} samples\")\n",
    "print(f\"   - Target rate: {df['target'].mean():.2%}\")\n",
    "print(f\"   - Features: {len(df.columns) - 3} (numeric + categorical)\")\n",
    "\n",
    "print(\"\\n2. PIPELINE CONFIGURATION:\")\n",
    "print(f\"   - Dual pipeline: {config.enable_dual_pipeline}\")\n",
    "print(f\"   - HPO trials: {config.n_trials}\")\n",
    "print(f\"   - Feature selection: Boruta + Forward\")\n",
    "print(f\"   - Data split: {config.train_ratio}/{config.test_ratio}/{config.oot_ratio}\")\n",
    "\n",
    "print(\"\\n3. MODEL TRAINING:\")\n",
    "if hasattr(pipeline, 'best_model_name_'):\n",
    "    best_row = pipeline.models_summary_[pipeline.models_summary_['model_name'] == pipeline.best_model_name_].iloc[0]\n",
    "    print(f\"   - Best model: {pipeline.best_model_name_}\")\n",
    "    print(f\"   - Train Gini: {best_row['Gini_Train']:.2%}\")\n",
    "    print(f\"   - OOT Gini: {best_row['Gini_OOT']:.2%}\")\n",
    "    if 0.70 <= best_row['Gini_Train'] <= 0.80:\n",
    "        print(f\"   - ‚úÖ TARGET ACHIEVED (70-80% range)\")\n",
    "    else:\n",
    "        print(f\"   - ‚ö†Ô∏è Outside target range\")\n",
    "\n",
    "print(\"\\n4. CALIBRATION:\")\n",
    "print(f\"   - Calibration samples: {len(calib_df):,}\")\n",
    "print(f\"   - Error before: {error_before:.4f}\")\n",
    "print(f\"   - Error after: {error_after:.4f}\")\n",
    "print(f\"   - Improvement: {improvement:.1f}%\")\n",
    "\n",
    "print(\"\\n5. PRODUCTION SCORING:\")\n",
    "print(f\"   - Scored samples: {len(scores):,}\")\n",
    "print(f\"   - Production Gini: {gini_calibrated:.2%}\")\n",
    "print(f\"   - Score range: {scores.min()}-{scores.max()}\")\n",
    "print(f\"   - Mean score: {scores.mean():.0f}\")\n",
    "\n",
    "print(\"\\n6. RISK SEGMENTATION:\")\n",
    "for tier in tier_analysis.index:\n",
    "    count = tier_analysis.loc[tier, 'Count']\n",
    "    rate = tier_analysis.loc[tier, 'Actual_Rate']\n",
    "    print(f\"   - {tier:20s}: {count:,} samples ({rate:.2%} default rate)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ COMPLETE WORKFLOW EXECUTED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"   1. Deploy model to production environment\")\n",
    "print(\"   2. Set up monitoring for PSI and model drift\")\n",
    "print(\"   3. Implement A/B testing framework\")\n",
    "print(\"   4. Schedule periodic model retraining\")\n",
    "print(\"   5. Create automated reporting dashboard\")\n",
    "print(\"   6. Set up alert system for performance degradation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
