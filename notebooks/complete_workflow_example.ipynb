{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Model Pipeline - Complete End-to-End Workflow\n",
    "\n",
    "## 🔄 Auto-Update Feature\n",
    "This notebook automatically downloads and installs the **latest version** from GitHub every time you run it.\n",
    "- No need to manually update\n",
    "- Always uses the most recent code\n",
    "- Clears cache to avoid version conflicts\n",
    "\n",
    "## 📋 Workflow Includes:\n",
    "1. **Automatic package update from GitHub**\n",
    "2. Data preparation with realistic target distribution (70-80% Gini)\n",
    "3. Full pipeline configuration with all parameters\n",
    "4. Model training with WOE and RAW pipelines\n",
    "5. **Probability calibration with Isotonic Regression**\n",
    "6. Credit scoring transformation (300-850)\n",
    "7. Risk segmentation and tiering\n",
    "8. Model evaluation and reporting\n",
    "9. Production scoring simulation\n",
    "\n",
    "## ⚠️ Important Notes:\n",
    "- **First cell ALWAYS updates the package** - run it every time!\n",
    "- If you see old behavior after update, restart kernel and run again\n",
    "- Set `FORCE_REINSTALL = False` in first cell if you want to keep current version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RISK MODEL PIPELINE - INSTALLATION/UPDATE\n",
      "============================================================\n",
      "Force reinstall: True\n",
      "GitHub URL: https://github.com/selimoksuz/risk-model-pipeline\n",
      "Branch: main (latest)\n",
      "------------------------------------------------------------\n",
      "🔄 Removing existing installation (if any)...\n",
      "✓ Existing version removed\n",
      "📦 Installing latest version from GitHub...\n",
      "❌ Installation failed: WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/selimoksuz/risk-model-pipeline.git 'C:\\Users\\Acer\\AppData\\Local\\Temp\\pip-install-emln4svd\\risk-model-pipeline_4e1289b16b2340b0b1d6d420d0d4ad78'\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [122 lines of output]\n",
      "  C:\\Users\\Acer\\AppData\\Local\\Temp\\pip-build-env-7o156vvg\\overlay\\Lib\\site-packages\\setuptools\\config\\_apply_pyprojecttoml.py:75: _MissingDynamic: `keywords` defined outside of `pyproject.toml` is ignored.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          The following seems to be defined outside of `pyproject.toml`:\n",
      "  \n",
      "          `keywords = ['risk-modeling', 'credit-scoring', 'machine-learning', 'woe-transformation', 'financial-modeling', 'scikit-learn', 'data-science', 'banking', 'credit-risk']`\n",
      "  \n",
      "          According to the spec (see the link below), however, setuptools CANNOT\n",
      "          consider this value unless `keywords` is listed as `dynamic`.\n",
      "  \n",
      "          https://packaging.python.org/en/latest/specifications/pyproject-toml/#declaring-project-metadata-the-project-table\n",
      "  \n",
      "          To prevent this problem, you can list `keywords` under `dynamic` or alternatively\n",
      "          remove the `[project]` table from your file and rely entirely on other means of\n",
      "          configuration.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    _handle_missing_dynamic(dist, project_table)\n",
      "  C:\\Users\\Acer\\AppData\\Local\\Temp\\pip-build-env-7o156vvg\\overlay\\Lib\\site-packages\\setuptools\\config\\_apply_pyprojecttoml.py:75: _MissingDynamic: `optional-dependencies` defined outside of `pyproject.toml` is ignored.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          The following seems to be defined outside of `pyproject.toml`:\n",
      "  \n",
      "          `optional-dependencies = {'dev': ['pytest>=7.0.0', 'pytest-cov>=3.0.0', 'black>=22.0.0', 'isort>=5.0.0', 'flake8>=4.0.0', 'mypy>=0.950', 'pre-commit>=2.0.0', 'twine>=4.0.0', 'wheel>=0.37.0', 'build>=0.7.0'], 'viz': ['matplotlib<3.7.0,>=3.5.0', 'seaborn>=0.12.0', 'plotly>=5.0.0'], 'ml': ['optuna>=3.0.0', 'shap>=0.41.0', 'imbalanced-learn>=0.9.0', 'scikit-learn-extra>=0.2.0'], 'notebook': ['jupyter>=1.0.0', 'notebook>=6.0.0', 'ipywidgets>=7.0.0'], 'all': []}`\n",
      "  \n",
      "          According to the spec (see the link below), however, setuptools CANNOT\n",
      "          consider this value unless `optional-dependencies` is listed as `dynamic`.\n",
      "  \n",
      "          https://packaging.python.org/en/latest/specifications/pyproject-toml/#declaring-project-metadata-the-project-table\n",
      "  \n",
      "          To prevent this problem, you can list `optional-dependencies` under `dynamic` or alternatively\n",
      "          remove the `[project]` table from your file and rely entirely on other means of\n",
      "          configuration.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    _handle_missing_dynamic(dist, project_table)\n",
      "  C:\\Users\\Acer\\AppData\\Local\\Temp\\pip-build-env-7o156vvg\\overlay\\Lib\\site-packages\\setuptools\\config\\_apply_pyprojecttoml.py:82: SetuptoolsDeprecationWarning: `project.license` as a TOML table is deprecated\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Please use a simple string containing a SPDX expression for `project.license`. You can also use `project.license-files`. (Both options available on setuptools>=77.0.0).\n",
      "  \n",
      "          By 2026-Feb-18, you need to update your project and remove deprecated calls\n",
      "          or your builds will no longer be supported.\n",
      "  \n",
      "          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    corresp(dist, value, root_dir)\n",
      "  C:\\Users\\Acer\\AppData\\Local\\Temp\\pip-build-env-7o156vvg\\overlay\\Lib\\site-packages\\setuptools\\config\\_apply_pyprojecttoml.py:82: SetuptoolsWarning: `extras_require` overwritten in `pyproject.toml` (optional-dependencies)\n",
      "    corresp(dist, value, root_dir)\n",
      "  C:\\Users\\Acer\\AppData\\Local\\Temp\\pip-build-env-7o156vvg\\overlay\\Lib\\site-packages\\setuptools\\config\\_apply_pyprojecttoml.py:61: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \n",
      "          License :: OSI Approved :: MIT License\n",
      "  \n",
      "          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    dist._finalize_license_expression()\n",
      "  C:\\Users\\Acer\\AppData\\Local\\Temp\\pip-build-env-7o156vvg\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \n",
      "          License :: OSI Approved :: MIT License\n",
      "  \n",
      "          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    self._finalize_license_expression()\n",
      "  running egg_info\n",
      "  creating src\\risk_model_pipeline.egg-info\n",
      "  writing src\\risk_model_pipeline.egg-info\\PKG-INFO\n",
      "  writing dependency_links to src\\risk_model_pipeline.egg-info\\dependency_links.txt\n",
      "  writing entry points to src\\risk_model_pipeline.egg-info\\entry_points.txt\n",
      "  writing requirements to src\\risk_model_pipeline.egg-info\\requires.txt\n",
      "  writing top-level names to src\\risk_model_pipeline.egg-info\\top_level.txt\n",
      "  writing manifest file 'src\\risk_model_pipeline.egg-info\\SOURCES.txt'\n",
      "  reading manifest file 'src\\risk_model_pipeline.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no files found matching 'CHANGELOG.md'\n",
      "  warning: no files found matching '*.json' under directory 'src\\risk_pipeline'\n",
      "  warning: no files found matching '*.yaml' under directory 'src\\risk_pipeline'\n",
      "  warning: no files found matching '*.yml' under directory 'src\\risk_pipeline'\n",
      "  warning: no files found matching '*.json' under directory 'tests'\n",
      "  warning: no files found matching '*.csv' under directory 'tests'\n",
      "  warning: no previously-included files matching '*.py[cod]' found anywhere in distribution\n",
      "  warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*.so' found anywhere in distribution\n",
      "  warning: no previously-included files matching '.git*' found anywhere in distribution\n",
      "  warning: no previously-included files matching '.DS_Store' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*.egg-info' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*' found under directory 'docs'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'src\\risk_model_pipeline.egg-info\\SOURCES.txt'\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py\", line 363, in <module>\n",
      "      main()\n",
      "    File \"C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py\", line 345, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "    File \"C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py\", line 130, in get_requires_for_build_wheel\n",
      "      return hook(config_settings)\n",
      "    File \"C:\\Users\\Acer\\AppData\\Local\\Temp\\pip-build-env-7o156vvg\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 331, in get_requires_for_build_wheel\n",
      "      return self._get_build_requires(config_settings, requirements=[])\n",
      "    File \"C:\\Users\\Acer\\AppData\\Local\\Temp\\pip-build-env-7o156vvg\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 301, in _get_build_requires\n",
      "      self.run_setup()\n",
      "    File \"C:\\Users\\Acer\\AppData\\Local\\Temp\\pip-build-env-7o156vvg\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 317, in run_setup\n",
      "      exec(code, locals())\n",
      "    File \"<string>\", line 126, in <module>\n",
      "  AttributeError: 'function' object has no attribute 'extras_require'\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "\n",
      "⚠️ Installation had issues. Please:\n",
      "   1. Restart the kernel\n",
      "   2. Run this cell again\n",
      "   3. If problem persists, try manual installation:\n",
      "      !pip uninstall -y risk-model-pipeline\n",
      "      !pip install --no-cache-dir git+https://github.com/selimoksuz/risk-model-pipeline.git\n"
     ]
    }
   ],
   "source": [
    "# Install/Update the risk-model-pipeline package from GitHub\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "def install_or_update_risk_pipeline(force_reinstall=True):\n",
    "    \"\"\"\n",
    "    Install or update risk-model-pipeline package from GitHub\n",
    "    Always gets the latest version\n",
    "    \"\"\"\n",
    "    \n",
    "    # First, try to uninstall any existing version to ensure clean update\n",
    "    if force_reinstall:\n",
    "        print(\"🔄 Removing existing installation (if any)...\")\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"risk-model-pipeline\"],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=False  # Don't fail if package doesn't exist\n",
    "            )\n",
    "            print(\"✓ Existing version removed\")\n",
    "        except:\n",
    "            print(\"✓ No existing installation found\")\n",
    "    \n",
    "    # Clear import cache to ensure fresh import\n",
    "    if 'risk_pipeline' in sys.modules:\n",
    "        print(\"🔄 Clearing import cache...\")\n",
    "        # Remove all risk_pipeline related modules from cache\n",
    "        modules_to_remove = [key for key in sys.modules.keys() if key.startswith('risk_pipeline')]\n",
    "        for module in modules_to_remove:\n",
    "            del sys.modules[module]\n",
    "        print(\"✓ Import cache cleared\")\n",
    "    \n",
    "    # Install latest version from GitHub main branch\n",
    "    print(\"📦 Installing latest version from GitHub...\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \n",
    "             \"git+https://github.com/selimoksuz/risk-model-pipeline.git@main#egg=risk-model-pipeline\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        print(\"✅ Latest version installed successfully!\")\n",
    "        \n",
    "        # Try to get version info\n",
    "        try:\n",
    "            import risk_pipeline\n",
    "            importlib.reload(risk_pipeline)  # Force reload\n",
    "            print(f\"📌 Version: {risk_pipeline.__version__}\")\n",
    "        except:\n",
    "            print(\"📌 Package installed (version check not available)\")\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Installation failed: {e.stderr}\")\n",
    "        return False\n",
    "    \n",
    "    # Verify installation with fresh import\n",
    "    print(\"\\n🔍 Verifying installation...\")\n",
    "    try:\n",
    "        # Clear and reimport\n",
    "        if 'risk_pipeline' in sys.modules:\n",
    "            del sys.modules['risk_pipeline']\n",
    "        \n",
    "        import risk_pipeline\n",
    "        from risk_pipeline import Config, DualPipeline\n",
    "        from risk_pipeline.pipeline import RiskModelPipeline\n",
    "        \n",
    "        print(\"✅ All imports successful!\")\n",
    "        print(f\"✓ Config available from: {Config.__module__}\")\n",
    "        print(f\"✓ DualPipeline available from: {DualPipeline.__module__}\")\n",
    "        return True\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"❌ Import verification failed: {e}\")\n",
    "        print(\"\\n💡 Try restarting the kernel and running this cell again\")\n",
    "        return False\n",
    "\n",
    "# Configuration options\n",
    "FORCE_REINSTALL = True  # Set to True to always get the latest version\n",
    "AUTO_RESTART = False    # Set to True to auto-restart kernel after install\n",
    "\n",
    "# Run installation/update\n",
    "print(\"=\" * 60)\n",
    "print(\"RISK MODEL PIPELINE - INSTALLATION/UPDATE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Force reinstall: {FORCE_REINSTALL}\")\n",
    "print(f\"GitHub URL: https://github.com/selimoksuz/risk-model-pipeline\")\n",
    "print(f\"Branch: main (latest)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "success = install_or_update_risk_pipeline(force_reinstall=FORCE_REINSTALL)\n",
    "\n",
    "if success:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🎉 READY TO USE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n📝 Note: If you still see old version behavior:\")\n",
    "    print(\"   1. Restart the kernel (Kernel → Restart)\")\n",
    "    print(\"   2. Run this cell again\")\n",
    "    print(\"   3. Continue with the notebook\")\n",
    "    \n",
    "    if AUTO_RESTART:\n",
    "        print(\"\\n🔄 Auto-restarting kernel in 3 seconds...\")\n",
    "        import time\n",
    "        time.sleep(3)\n",
    "        from IPython.core.display import HTML\n",
    "        display(HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\"))\n",
    "else:\n",
    "    print(\"\\n⚠️ Installation had issues. Please:\")\n",
    "    print(\"   1. Restart the kernel\")\n",
    "    print(\"   2. Run this cell again\")\n",
    "    print(\"   3. If problem persists, try manual installation:\")\n",
    "    print(\"      !pip uninstall -y risk-model-pipeline\")\n",
    "    print(\"      !pip install --no-cache-dir git+https://github.com/selimoksuz/risk-model-pipeline.git\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib==3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization libraries imported successfully!\n",
      "Core libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Try to import visualization libraries (optional)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    PLOT_AVAILABLE = True\n",
    "    print('Visualization libraries imported successfully!')\n",
    "except ImportError as e:\n",
    "    PLOT_AVAILABLE = False\n",
    "    print('Warning: Visualization libraries not available. Plots will be skipped.')\n",
    "    print(f'Error: {e}')\n",
    "\n",
    "# Import our pipeline\n",
    "from risk_pipeline import Config, DualPipeline\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print('Core libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Create synthetic data designed to achieve 70-80% Train Gini with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def create_high_performance_credit_data(n_samples=10000):\n",
    "    \"\"\"\n",
    "    Create synthetic credit risk data optimized for 70-80% Gini\n",
    "    with strong linear separability for logistic regression\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create strongly predictive features with clear linear relationships\n",
    "    \n",
    "    # Feature 1: Credit score - very strong predictor\n",
    "    credit_score_good = np.random.normal(750, 40, n_samples // 2)\n",
    "    credit_score_bad = np.random.normal(550, 50, n_samples // 2)\n",
    "    credit_score = np.concatenate([credit_score_good, credit_score_bad])\n",
    "    credit_score = np.clip(credit_score, 300, 850)\n",
    "    \n",
    "    # Feature 2: Debt ratio - strong predictor\n",
    "    debt_ratio_good = np.random.beta(2, 8, n_samples // 2)  # Low debt\n",
    "    debt_ratio_bad = np.random.beta(8, 2, n_samples // 2)   # High debt\n",
    "    debt_ratio = np.concatenate([debt_ratio_good, debt_ratio_bad])\n",
    "    \n",
    "    # Feature 3: Payment history score (0-100)\n",
    "    payment_history_good = np.random.normal(85, 10, n_samples // 2)\n",
    "    payment_history_bad = np.random.normal(45, 15, n_samples // 2)\n",
    "    payment_history = np.concatenate([payment_history_good, payment_history_bad])\n",
    "    payment_history = np.clip(payment_history, 0, 100)\n",
    "    \n",
    "    # Feature 4: Income (log-normal)\n",
    "    income_good = np.random.lognormal(11.5, 0.4, n_samples // 2)\n",
    "    income_bad = np.random.lognormal(10.5, 0.5, n_samples // 2)\n",
    "    income = np.concatenate([income_good, income_bad])\n",
    "    income = np.clip(income, 15000, 500000)\n",
    "    \n",
    "    # Feature 5: Months employed\n",
    "    employed_good = np.random.gamma(8, 10, n_samples // 2)  # Stable employment\n",
    "    employed_bad = np.random.gamma(2, 10, n_samples // 2)   # Unstable employment\n",
    "    months_employed = np.concatenate([employed_good, employed_bad])\n",
    "    months_employed = np.clip(months_employed, 0, 480)\n",
    "    \n",
    "    # Feature 6: Number of delinquencies\n",
    "    delinq_good = np.random.poisson(0.1, n_samples // 2)\n",
    "    delinq_bad = np.random.poisson(2.5, n_samples // 2)\n",
    "    num_delinquencies = np.concatenate([delinq_good, delinq_bad])\n",
    "    num_delinquencies = np.clip(num_delinquencies, 0, 10)\n",
    "    \n",
    "    # Feature 7: Credit utilization\n",
    "    util_good = np.random.beta(2, 5, n_samples // 2)  # Low utilization\n",
    "    util_bad = np.random.beta(5, 2, n_samples // 2)   # High utilization\n",
    "    utilization_rate = np.concatenate([util_good, util_bad])\n",
    "    \n",
    "    # Feature 8: Age (slight predictor)\n",
    "    age_good = np.random.normal(45, 10, n_samples // 2)\n",
    "    age_bad = np.random.normal(32, 8, n_samples // 2)\n",
    "    age = np.concatenate([age_good, age_bad])\n",
    "    age = np.clip(age, 18, 80)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    shuffle_idx = np.random.permutation(n_samples)\n",
    "    credit_score = credit_score[shuffle_idx]\n",
    "    debt_ratio = debt_ratio[shuffle_idx]\n",
    "    payment_history = payment_history[shuffle_idx]\n",
    "    income = income[shuffle_idx]\n",
    "    months_employed = months_employed[shuffle_idx]\n",
    "    num_delinquencies = num_delinquencies[shuffle_idx]\n",
    "    utilization_rate = utilization_rate[shuffle_idx]\n",
    "    age = age[shuffle_idx]\n",
    "    \n",
    "    # Create target with strong linear relationship\n",
    "    # This formula is designed to give ~70-80% Gini with logistic regression\n",
    "    risk_score = (\n",
    "        - 0.008 * credit_score           # Strong negative (good score = low risk)\n",
    "        + 4.0 * debt_ratio               # Strong positive (high debt = high risk)\n",
    "        - 0.025 * payment_history        # Strong negative (good history = low risk)\n",
    "        - 0.000005 * income              # Moderate negative\n",
    "        - 0.003 * months_employed        # Moderate negative\n",
    "        + 0.5 * num_delinquencies        # Strong positive\n",
    "        + 2.5 * utilization_rate         # Strong positive\n",
    "        - 0.01 * age                     # Slight negative\n",
    "        + 2.0                            # Intercept to center probabilities\n",
    "    )\n",
    "    \n",
    "    # Add small noise to prevent perfect separation\n",
    "    risk_score += np.random.normal(0, 0.3, n_samples)\n",
    "    \n",
    "    # Convert to probability\n",
    "    default_prob = 1 / (1 + np.exp(-risk_score))\n",
    "    \n",
    "    # Generate binary target (aim for ~15% event rate)\n",
    "    target = np.random.binomial(1, default_prob)\n",
    "    \n",
    "    # Adjust to get closer to 15% if needed\n",
    "    current_rate = target.mean()\n",
    "    if current_rate > 0.20:\n",
    "        # Randomly flip some 1s to 0s\n",
    "        ones_idx = np.where(target == 1)[0]\n",
    "        n_to_flip = int((current_rate - 0.15) * n_samples)\n",
    "        flip_idx = np.random.choice(ones_idx, n_to_flip, replace=False)\n",
    "        target[flip_idx] = 0\n",
    "    \n",
    "    # Create categorical features that correlate with target\n",
    "    education = np.where(\n",
    "        target == 0,\n",
    "        np.random.choice(['Bachelor', 'Master', 'PhD'], n_samples, p=[0.5, 0.35, 0.15]),\n",
    "        np.random.choice(['High School', 'Bachelor', 'Master'], n_samples, p=[0.5, 0.4, 0.1])\n",
    "    )\n",
    "    \n",
    "    employment_type = np.where(\n",
    "        target == 0,\n",
    "        np.random.choice(['Full-time', 'Self-employed'], n_samples, p=[0.7, 0.3]),\n",
    "        np.random.choice(['Part-time', 'Unemployed', 'Full-time'], n_samples, p=[0.3, 0.2, 0.5])\n",
    "    )\n",
    "    \n",
    "    home_ownership = np.where(\n",
    "        target == 0,\n",
    "        np.random.choice(['Own', 'Mortgage'], n_samples, p=[0.4, 0.6]),\n",
    "        np.random.choice(['Rent', 'Other', 'Mortgage'], n_samples, p=[0.5, 0.1, 0.4])\n",
    "    )\n",
    "    \n",
    "    # Additional features\n",
    "    num_credit_lines = np.where(\n",
    "        target == 0,\n",
    "        np.random.poisson(5, n_samples),\n",
    "        np.random.poisson(8, n_samples)\n",
    "    )\n",
    "    num_credit_lines = np.clip(num_credit_lines, 0, 20)\n",
    "    \n",
    "    # Fixed: Create months_since_last_late properly\n",
    "    months_since_last_late = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        if target[i] == 0:\n",
    "            # Good customers: mostly no late payments (999) or few late payments\n",
    "            if np.random.random() < 0.7:\n",
    "                months_since_last_late[i] = 999  # Never late\n",
    "            else:\n",
    "                months_since_last_late[i] = np.random.exponential(50)  # Rare late\n",
    "        else:\n",
    "            # Bad customers: recent late payments\n",
    "            months_since_last_late[i] = np.random.exponential(10)\n",
    "    \n",
    "    # Create DataFrame (using 'h' instead of deprecated 'H')\n",
    "    df = pd.DataFrame({\n",
    "        'app_id': range(n_samples),\n",
    "        'app_dt': pd.date_range(start='2022-01-01', periods=n_samples, freq='h')[:n_samples],\n",
    "        'target': target,\n",
    "        'credit_score': credit_score.round(0).astype(int),\n",
    "        'debt_ratio': debt_ratio.round(3),\n",
    "        'payment_history_score': payment_history.round(1),\n",
    "        'income': income.round(0).astype(int),\n",
    "        'months_employed': months_employed.round(0).astype(int),\n",
    "        'num_delinquencies': num_delinquencies,\n",
    "        'utilization_rate': utilization_rate.round(3),\n",
    "        'age': age.round(0).astype(int),\n",
    "        'num_credit_lines': num_credit_lines,\n",
    "        'months_since_last_late': months_since_last_late.round(0).astype(int),\n",
    "        'education': education,\n",
    "        'employment_type': employment_type,\n",
    "        'home_ownership': home_ownership\n",
    "    })\n",
    "    \n",
    "    # Add some missing values (realistic pattern)\n",
    "    missing_idx = np.random.choice(n_samples, size=int(n_samples * 0.02), replace=False)\n",
    "    df.loc[missing_idx, 'months_since_last_late'] = np.nan\n",
    "    \n",
    "    missing_idx = np.random.choice(n_samples, size=int(n_samples * 0.01), replace=False)\n",
    "    df.loc[missing_idx, 'months_employed'] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create the dataset\n",
    "df = create_high_performance_credit_data(n_samples=10000)\n",
    "\n",
    "print(f\"Dataset created with shape: {df.shape}\")\n",
    "print(f\"Target event rate: {df['target'].mean():.2%}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality check\n",
    "print(\"Data Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"Total features: {len(df.columns) - 3}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"Event rate: {df['target'].mean():.2%}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"No missing values\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Full Pipeline Configuration\n",
    "\n",
    "Configure ALL pipeline parameters for complete testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline with ALL parameters\n",
    "config = Config(\n",
    "    # Basic settings\n",
    "    target_col='target',\n",
    "    id_col='app_id',\n",
    "    time_col='app_dt',\n",
    "    output_folder='outputs',\n",
    "    random_state=42,\n",
    "    \n",
    "    # Feature selection parameters\n",
    "    iv_min=0.02,              # Minimum Information Value\n",
    "    iv_high_threshold=0.5,    # Maximum IV (detect overfitting)\n",
    "    psi_threshold=0.25,       # Population Stability Index threshold\n",
    "    rho_threshold=0.90,       # Correlation threshold\n",
    "    vif_threshold=5.0,        # Variance Inflation Factor threshold\n",
    "    rare_threshold=0.01,      # Rare category threshold\n",
    "    cluster_top_k=2,          # Top K features per cluster\n",
    "    max_features=15,          # Maximum number of features\n",
    "    min_features=5,           # Minimum number of features\n",
    "    \n",
    "    # WOE settings\n",
    "    n_bins=10,                # Number of bins for numeric features\n",
    "    min_bin_size=0.05,        # Minimum bin size (5% of data)\n",
    "    woe_monotonic=False,      # Enforce monotonic WOE\n",
    "    max_abs_woe=None,         # Cap absolute WOE values\n",
    "    handle_missing='as_category',  # How to handle missing values\n",
    "    \n",
    "    # HPO (Hyperparameter Optimization) settings\n",
    "    use_optuna=True,          # Enable Bayesian optimization\n",
    "    n_trials=100,             # Number of HPO trials\n",
    "    optuna_timeout=600,       # Maximum 10 minutes for optimization\n",
    "    cv_folds=5,               # Cross-validation folds\n",
    "    \n",
    "    # Feature selection methods\n",
    "    use_boruta=True,          # Boruta feature selection\n",
    "    forward_1se=True,         # Forward selection with 1SE rule\n",
    "    use_noise_sentinel=True,  # Add noise features for stability\n",
    "    \n",
    "    # Data splitting\n",
    "    use_test_split=True,      # Use separate test set\n",
    "    train_ratio=0.60,         # 60% for training\n",
    "    test_ratio=0.20,          # 20% for test\n",
    "    oot_ratio=0.20,           # 20% for out-of-time\n",
    "    oot_months=None,          # Use ratio instead of months\n",
    "    min_oot_size=50,          # Minimum OOT size\n",
    "    \n",
    "    # Dual pipeline settings\n",
    "    enable_dual_pipeline=True,  # Compare WOE vs RAW\n",
    "    \n",
    "    # RAW pipeline settings\n",
    "    raw_outlier_method='clip',     # Outlier handling: clip, remove, none\n",
    "    raw_outlier_threshold=3.0,     # Z-score threshold\n",
    "    raw_scaler_type='standard',    # Scaling: standard, minmax, robust\n",
    "    imputation_strategy='median',  # Imputation: mean, median, mode\n",
    "    \n",
    "    # Model selection criteria\n",
    "    model_selection_method='gini_oot',  # Selection: gini_oot, balanced, stable\n",
    "    max_train_oot_gap=None,            # Maximum train-OOT gap allowed\n",
    "    model_stability_weight=0.2,        # Weight for stability (0-1)\n",
    "    min_gini_threshold=0.3,            # Minimum acceptable Gini\n",
    "    \n",
    "    # Output settings\n",
    "    output_excel_path='pipeline_results.xlsx',  # Excel output\n",
    "    write_csv=True,                            # Also write CSV files\n",
    "    run_id=None                                # Auto-generate run ID\n",
    ")\n",
    "\n",
    "print(\"Pipeline configured with ALL parameters:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Dual pipeline: {config.enable_dual_pipeline}\")\n",
    "print(f\"  Feature selection: Boruta={config.use_boruta}, Forward={config.forward_1se}\")\n",
    "print(f\"  HPO: Enabled={config.use_optuna}, Trials={config.n_trials}\")\n",
    "print(f\"  Data split: Train={config.train_ratio}, Test={config.test_ratio}, OOT={config.oot_ratio}\")\n",
    "print(f\"  WOE bins: {config.n_bins}, Monotonic={config.woe_monotonic}\")\n",
    "print(f\"  Feature limits: Min={config.min_features}, Max={config.max_features}\")\n",
    "print(f\"  Thresholds: IV>{config.iv_min}, PSI<{config.psi_threshold}, Corr<{config.rho_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training with Dual Pipeline\n",
    "\n",
    "Train models using both WOE and RAW pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the dual pipeline\n",
    "print(\"Starting Dual Pipeline Training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = DualPipeline(config)\n",
    "\n",
    "print(\"\\nTraining models with both WOE and RAW features...\")\n",
    "print(\"This will take 5-10 minutes with HPO enabled...\")\n",
    "print(\"\\nSteps that will be executed:\")\n",
    "print(\"  1. Data validation and freezing\")\n",
    "print(\"  2. Variable classification\")\n",
    "print(\"  3. Train/Test/OOT splitting\")\n",
    "print(\"  4. WOE transformation\")\n",
    "print(\"  5. PSI calculation\")\n",
    "print(\"  6. Feature correlation analysis\")\n",
    "print(\"  7. Feature selection (Boruta + Forward)\")\n",
    "print(\"  8. Model training with HPO\")\n",
    "print(\"  9. Model evaluation and selection\")\n",
    "print(\" 10. Report generation\")\n",
    "print(\"\\nProgress:\")\n",
    "\n",
    "try:\n",
    "    # Run pipeline\n",
    "    pipeline.run(df)\n",
    "    \n",
    "    print(\"\\n✅ Pipeline completed successfully!\")\n",
    "    \n",
    "    # Get summary\n",
    "    summary = pipeline.get_summary()\n",
    "    \n",
    "    print(\"\\nPipeline Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"  Best pipeline type: {summary['best_pipeline']}\")\n",
    "    print(f\"  WOE features selected: {summary['n_features_woe']}\")\n",
    "    print(f\"  RAW features selected: {summary['n_features_raw']}\")\n",
    "    print(f\"  Total models trained: {len(pipeline.models_summary_)}\")\n",
    "    print(f\"  Best model: {pipeline.best_model_name_}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error during pipeline execution: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Performance Analysis\n",
    "\n",
    "Analyze model performance - targeting 70-80% Train Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View detailed model results\n",
    "if hasattr(pipeline, 'models_summary_'):\n",
    "    print(\"Model Performance Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    summary_df = pipeline.models_summary_\n",
    "    \n",
    "    # Check which columns are available\n",
    "    available_cols = summary_df.columns.tolist()\n",
    "    \n",
    "    # Determine the feature count column name\n",
    "    feature_col = None\n",
    "    for col in ['n_features', 'n_vars', 'num_features', 'n_selected_features']:\n",
    "        if col in available_cols:\n",
    "            feature_col = col\n",
    "            break\n",
    "    \n",
    "    # Show all models\n",
    "    print(\"\\nAll Models Trained:\")\n",
    "    display_cols = ['model_name', 'Gini_Train', 'Gini_OOT']\n",
    "    if 'Gini_Test' in available_cols:\n",
    "        display_cols.insert(2, 'Gini_Test')\n",
    "    if feature_col:\n",
    "        display_cols.append(feature_col)\n",
    "    \n",
    "    display_cols = [col for col in display_cols if col in available_cols]\n",
    "    display(summary_df[display_cols])\n",
    "    \n",
    "    # Best model details\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"BEST MODEL: {pipeline.best_model_name_}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    best_row = summary_df[summary_df['model_name'] == pipeline.best_model_name_].iloc[0]\n",
    "    \n",
    "    train_gini = best_row['Gini_Train']\n",
    "    oot_gini = best_row['Gini_OOT']\n",
    "    \n",
    "    print(f\"  Train Gini: {train_gini:.4f} ({train_gini*100:.1f}%)\")\n",
    "    if 'Gini_Test' in available_cols:\n",
    "        test_gini = best_row['Gini_Test']\n",
    "        print(f\"  Test Gini:  {test_gini:.4f} ({test_gini*100:.1f}%)\")\n",
    "    print(f\"  OOT Gini:   {oot_gini:.4f} ({oot_gini*100:.1f}%)\")\n",
    "    print(f\"  Train-OOT Gap: {abs(train_gini - oot_gini):.4f}\")\n",
    "    \n",
    "    if feature_col:\n",
    "        print(f\"  Features used: {int(best_row[feature_col])}\")\n",
    "    \n",
    "    # Check if we achieved target performance\n",
    "    print(f\"\\n📊 Performance Check:\")\n",
    "    if 0.70 <= train_gini <= 0.80:\n",
    "        print(f\"  ✅ TARGET ACHIEVED! Train Gini {train_gini:.1%} is in the 70-80% range\")\n",
    "    else:\n",
    "        print(f\"  ⚠️ Train Gini {train_gini:.1%} is outside target range (70-80%)\")\n",
    "    \n",
    "    if abs(train_gini - oot_gini) <= 0.05:\n",
    "        print(f\"  ✅ Model is stable (Train-OOT gap < 5%)\")\n",
    "    else:\n",
    "        print(f\"  ⚠️ Potential overfitting (Train-OOT gap > 5%)\")\n",
    "else:\n",
    "    print(\"No model results available. Please run the pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Probability Calibration\n",
    "\n",
    "Calibrate model probabilities using Isotonic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare calibration dataset\n",
    "print(\"Preparing calibration data...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a separate calibration dataset\n",
    "calib_df = create_high_performance_credit_data(n_samples=2000)\n",
    "print(f\"Calibration data shape: {calib_df.shape}\")\n",
    "print(f\"Calibration target rate: {calib_df['target'].mean():.2%}\")\n",
    "\n",
    "# Get predictions on calibration data\n",
    "if hasattr(pipeline, 'predict_proba'):\n",
    "    uncalibrated_probs = pipeline.predict_proba(calib_df)\n",
    "    print(f\"\\nUncalibrated predictions generated: {len(uncalibrated_probs)}\")\n",
    "    \n",
    "    # Check calibration before\n",
    "    print(f\"\\nBefore calibration:\")\n",
    "    print(f\"  Actual event rate:     {calib_df['target'].mean():.4f}\")\n",
    "    print(f\"  Mean predicted prob:   {uncalibrated_probs.mean():.4f}\")\n",
    "    print(f\"  Calibration error:     {abs(calib_df['target'].mean() - uncalibrated_probs.mean()):.4f}\")\n",
    "    print(f\"  Min probability:       {uncalibrated_probs.min():.4f}\")\n",
    "    print(f\"  Max probability:       {uncalibrated_probs.max():.4f}\")\n",
    "else:\n",
    "    print(\"Error: predict_proba method not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Isotonic Regression calibration\n",
    "print(\"Applying Isotonic Regression Calibration...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Fit isotonic regression\n",
    "iso_reg = IsotonicRegression(out_of_bounds='clip')\n",
    "calibrated_probs = iso_reg.fit_transform(uncalibrated_probs, calib_df['target'])\n",
    "\n",
    "print(f\"\\nAfter calibration:\")\n",
    "print(f\"  Actual event rate:     {calib_df['target'].mean():.4f}\")\n",
    "print(f\"  Mean calibrated prob:  {calibrated_probs.mean():.4f}\")\n",
    "print(f\"  Calibration error:     {abs(calib_df['target'].mean() - calibrated_probs.mean()):.4f}\")\n",
    "print(f\"  Min probability:       {calibrated_probs.min():.4f}\")\n",
    "print(f\"  Max probability:       {calibrated_probs.max():.4f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "error_before = abs(calib_df['target'].mean() - uncalibrated_probs.mean())\n",
    "error_after = abs(calib_df['target'].mean() - calibrated_probs.mean())\n",
    "improvement = (error_before - error_after) / error_before * 100\n",
    "\n",
    "print(f\"\\n📊 Calibration improved by {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize calibration curves (if matplotlib is available)\n",
    "if PLOT_AVAILABLE:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Before calibration\n",
    "    fraction_pos_before, mean_pred_before = calibration_curve(\n",
    "        calib_df['target'], uncalibrated_probs, n_bins=10\n",
    "    )\n",
    "    axes[0].plot(mean_pred_before, fraction_pos_before, marker='o', \n",
    "                 linewidth=2, label='Model', color='blue')\n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect calibration')\n",
    "    axes[0].set_xlabel('Mean Predicted Probability', fontsize=12)\n",
    "    axes[0].set_ylabel('Fraction of Positives', fontsize=12)\n",
    "    axes[0].set_title('Before Calibration', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(loc='lower right')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_xlim([0, 1])\n",
    "    axes[0].set_ylim([0, 1])\n",
    "\n",
    "    # After calibration\n",
    "    fraction_pos_after, mean_pred_after = calibration_curve(\n",
    "        calib_df['target'], calibrated_probs, n_bins=10\n",
    "    )\n",
    "    axes[1].plot(mean_pred_after, fraction_pos_after, marker='o', \n",
    "                 linewidth=2, label='Calibrated', color='green')\n",
    "    axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect calibration')\n",
    "    axes[1].set_xlabel('Mean Predicted Probability', fontsize=12)\n",
    "    axes[1].set_ylabel('Fraction of Positives', fontsize=12)\n",
    "    axes[1].set_title('After Calibration', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(loc='lower right')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_xlim([0, 1])\n",
    "    axes[1].set_ylim([0, 1])\n",
    "\n",
    "    plt.suptitle('Probability Calibration Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n✅ Calibration curves show improved alignment with diagonal after calibration\")\n",
    "else:\n",
    "    # Show calibration statistics without plots\n",
    "    fraction_pos_before, mean_pred_before = calibration_curve(\n",
    "        calib_df['target'], uncalibrated_probs, n_bins=10\n",
    "    )\n",
    "    fraction_pos_after, mean_pred_after = calibration_curve(\n",
    "        calib_df['target'], calibrated_probs, n_bins=10\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCalibration Analysis (without plots):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Before calibration - Mean absolute error:\")\n",
    "    mae_before = np.mean(np.abs(fraction_pos_before - mean_pred_before))\n",
    "    print(f\"  MAE: {mae_before:.4f}\")\n",
    "    \n",
    "    print(\"\\nAfter calibration - Mean absolute error:\")\n",
    "    mae_after = np.mean(np.abs(fraction_pos_after - mean_pred_after))\n",
    "    print(f\"  MAE: {mae_after:.4f}\")\n",
    "    \n",
    "    print(f\"\\n✅ Calibration improved by {((mae_before - mae_after) / mae_before * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Scoring Simulation\n",
    "\n",
    "Apply the calibrated model to new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create completely new dataset for production scoring\n",
    "print(\"PRODUCTION SCORING SIMULATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nCreating new production data...\")\n",
    "score_df = create_high_performance_credit_data(n_samples=5000)\n",
    "print(f\"Production data shape: {score_df.shape}\")\n",
    "print(f\"Production target rate: {score_df['target'].mean():.2%}\")\n",
    "\n",
    "# Apply the trained model\n",
    "print(\"\\nApplying trained model to production data...\")\n",
    "raw_probs = pipeline.predict_proba(score_df)\n",
    "print(f\"Raw predictions generated: {len(raw_probs)} samples\")\n",
    "\n",
    "# Apply calibration\n",
    "print(\"\\nApplying calibration to production predictions...\")\n",
    "calibrated_prod_probs = iso_reg.transform(raw_probs)\n",
    "print(f\"Calibrated predictions generated: {len(calibrated_prod_probs)} samples\")\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_raw = roc_auc_score(score_df['target'], raw_probs)\n",
    "auc_calibrated = roc_auc_score(score_df['target'], calibrated_prod_probs)\n",
    "gini_raw = 2 * auc_raw - 1\n",
    "gini_calibrated = 2 * auc_calibrated - 1\n",
    "\n",
    "print(f\"\\nProduction Performance:\")\n",
    "print(f\"  Raw Model:\")\n",
    "print(f\"    - AUC:  {auc_raw:.4f}\")\n",
    "print(f\"    - Gini: {gini_raw:.4f} ({gini_raw*100:.1f}%)\")\n",
    "print(f\"  Calibrated Model:\")\n",
    "print(f\"    - AUC:  {auc_calibrated:.4f}\")\n",
    "print(f\"    - Gini: {gini_calibrated:.4f} ({gini_calibrated*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Credit Score Transformation\n",
    "\n",
    "Convert calibrated probabilities to standard credit scores (300-850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_to_score(probs, base_score=600, pdo=20):\n",
    "    \"\"\"\n",
    "    Convert probability to credit score using standard formula:\n",
    "    Score = Base_Score - PDO * log(odds)\n",
    "    \n",
    "    Parameters:\n",
    "    - base_score: Score at odds of 1:1 (typically 600)\n",
    "    - pdo: Points to Double Odds (typically 20)\n",
    "    \"\"\"\n",
    "    # Clip probabilities to avoid inf values\n",
    "    probs_safe = np.clip(probs, 0.001, 0.999)\n",
    "    \n",
    "    # Calculate odds\n",
    "    odds = probs_safe / (1 - probs_safe)\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = base_score - pdo * np.log(odds)\n",
    "    \n",
    "    # Round to nearest integer\n",
    "    scores = np.round(scores).astype(int)\n",
    "    \n",
    "    # Ensure scores are in valid range\n",
    "    scores = np.clip(scores, 300, 850)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Convert calibrated probabilities to credit scores\n",
    "scores = probability_to_score(calibrated_prod_probs)\n",
    "\n",
    "print(\"Credit Score Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Min score:    {scores.min()}\")\n",
    "print(f\"  Max score:    {scores.max()}\")\n",
    "print(f\"  Mean score:   {scores.mean():.0f}\")\n",
    "print(f\"  Median score: {np.median(scores):.0f}\")\n",
    "print(f\"  Std dev:      {scores.std():.0f}\")\n",
    "\n",
    "# Score distribution by ranges\n",
    "print(\"\\nScore Distribution:\")\n",
    "print(\"-\" * 30)\n",
    "ranges = [(300, 400), (400, 500), (500, 600), (600, 700), (700, 800), (800, 850)]\n",
    "for low, high in ranges:\n",
    "    mask = (scores >= low) & (scores < high)\n",
    "    count = mask.sum()\n",
    "    pct = count / len(scores) * 100\n",
    "    default_rate = score_df.loc[mask, 'target'].mean() * 100\n",
    "    print(f\"  {low:3d}-{high:3d}: {count:4d} ({pct:5.1f}%) - Default rate: {default_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Risk Segmentation and Tiering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define risk tiers based on credit scores\n",
    "def assign_risk_tier(score):\n",
    "    if score >= 750:\n",
    "        return 'A: Prime'\n",
    "    elif score >= 700:\n",
    "        return 'B: Near Prime'\n",
    "    elif score >= 650:\n",
    "        return 'C: Standard'\n",
    "    elif score >= 600:\n",
    "        return 'D: Subprime'\n",
    "    elif score >= 550:\n",
    "        return 'E: Deep Subprime'\n",
    "    else:\n",
    "        return 'F: High Risk'\n",
    "\n",
    "# Assign risk tiers\n",
    "risk_tiers = pd.Series(scores).apply(assign_risk_tier)\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'app_id': score_df['app_id'],\n",
    "    'actual_target': score_df['target'],\n",
    "    'raw_probability': raw_probs,\n",
    "    'calibrated_probability': calibrated_prod_probs,\n",
    "    'credit_score': scores,\n",
    "    'risk_tier': risk_tiers\n",
    "})\n",
    "\n",
    "# Risk tier analysis\n",
    "tier_analysis = results_df.groupby('risk_tier').agg({\n",
    "    'app_id': 'count',\n",
    "    'actual_target': 'mean',\n",
    "    'calibrated_probability': 'mean',\n",
    "    'credit_score': ['mean', 'min', 'max']\n",
    "}).round(3)\n",
    "\n",
    "tier_analysis.columns = ['Count', 'Actual_Rate', 'Avg_Prob', 'Mean_Score', 'Min_Score', 'Max_Score']\n",
    "tier_analysis = tier_analysis.sort_values('Mean_Score', ascending=False)\n",
    "\n",
    "print(\"Risk Tier Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "display(tier_analysis)\n",
    "\n",
    "# Calculate lift and odds\n",
    "base_rate = results_df['actual_target'].mean()\n",
    "tier_analysis['Lift'] = (tier_analysis['Actual_Rate'] / base_rate).round(2)\n",
    "tier_analysis['Odds'] = ((1 - tier_analysis['Actual_Rate']) / tier_analysis['Actual_Rate']).round(1)\n",
    "\n",
    "print(f\"\\nBase default rate: {base_rate:.2%}\")\n",
    "print(\"\\nLift and Odds by Risk Tier:\")\n",
    "for tier in tier_analysis.index:\n",
    "    lift = tier_analysis.loc[tier, 'Lift']\n",
    "    odds = tier_analysis.loc[tier, 'Odds']\n",
    "    print(f\"  {tier:20s}: Lift={lift:.2f}x, Odds={odds:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization (if matplotlib is available)\n",
    "if PLOT_AVAILABLE:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "    # 1. Score distribution\n",
    "    axes[0, 0].hist(scores, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].axvline(scores.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {scores.mean():.0f}')\n",
    "    axes[0, 0].set_xlabel('Credit Score', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0, 0].set_title('Credit Score Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Default rate by score decile\n",
    "    score_deciles = pd.qcut(scores, q=10, labels=[f'D{i+1}' for i in range(10)])\n",
    "    decile_default = score_df.groupby(score_deciles)['target'].mean()\n",
    "    axes[0, 1].bar(range(len(decile_default)), decile_default.values, \n",
    "                   color='coral', edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Score Decile', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Default Rate', fontsize=12)\n",
    "    axes[0, 1].set_title('Default Rate by Score Decile', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xticks(range(len(decile_default)))\n",
    "    axes[0, 1].set_xticklabels(decile_default.index)\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # 3. ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(score_df['target'], calibrated_prod_probs)\n",
    "    auc = roc_auc_score(score_df['target'], calibrated_prod_probs)\n",
    "    gini = 2 * auc - 1\n",
    "    axes[0, 2].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC={auc:.3f}, Gini={gini:.3f})', color='green')\n",
    "    axes[0, 2].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "    axes[0, 2].fill_between(fpr, tpr, alpha=0.3, color='green')\n",
    "    axes[0, 2].set_xlabel('False Positive Rate', fontsize=12)\n",
    "    axes[0, 2].set_ylabel('True Positive Rate', fontsize=12)\n",
    "    axes[0, 2].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "    axes[0, 2].legend(loc='lower right')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Calibrated probability distribution\n",
    "    axes[1, 0].hist(calibrated_prod_probs, bins=30, edgecolor='black', alpha=0.7, color='purple')\n",
    "    axes[1, 0].axvline(calibrated_prod_probs.mean(), color='red', linestyle='--', \n",
    "                       linewidth=2, label=f'Mean: {calibrated_prod_probs.mean():.3f}')\n",
    "    axes[1, 0].set_xlabel('Calibrated Probability', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1, 0].set_title('Calibrated Probability Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. Score vs Probability\n",
    "    axes[1, 1].scatter(calibrated_prod_probs, scores, alpha=0.5, s=1, color='blue')\n",
    "    axes[1, 1].set_xlabel('Calibrated Probability', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Credit Score', fontsize=12)\n",
    "    axes[1, 1].set_title('Score vs Probability Mapping', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. Risk tier distribution\n",
    "    tier_counts = results_df['risk_tier'].value_counts().sort_index()\n",
    "    colors = ['green', 'lightgreen', 'yellow', 'orange', 'coral', 'red']\n",
    "    axes[1, 2].pie(tier_counts.values, labels=tier_counts.index, autopct='%1.1f%%', \n",
    "                   colors=colors[:len(tier_counts)], startangle=90)\n",
    "    axes[1, 2].set_title('Risk Tier Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.suptitle('Production Scoring Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nVisualization Analysis (text-based due to matplotlib error):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Score distribution stats\n",
    "    print(\"\\n1. Credit Score Distribution:\")\n",
    "    print(f\"   - Min: {scores.min()}\")\n",
    "    print(f\"   - Max: {scores.max()}\")\n",
    "    print(f\"   - Mean: {scores.mean():.0f}\")\n",
    "    print(f\"   - Std: {scores.std():.0f}\")\n",
    "    \n",
    "    # 2. Default rate by decile\n",
    "    print(\"\\n2. Default Rate by Score Decile:\")\n",
    "    score_deciles = pd.qcut(scores, q=10, labels=[f'D{i+1}' for i in range(10)])\n",
    "    decile_default = score_df.groupby(score_deciles)['target'].mean()\n",
    "    for decile, rate in decile_default.items():\n",
    "        print(f\"   - {decile}: {rate:.2%}\")\n",
    "    \n",
    "    # 3. ROC metrics\n",
    "    fpr, tpr, _ = roc_curve(score_df['target'], calibrated_prod_probs)\n",
    "    auc = roc_auc_score(score_df['target'], calibrated_prod_probs)\n",
    "    gini = 2 * auc - 1\n",
    "    print(f\"\\n3. ROC Metrics:\")\n",
    "    print(f\"   - AUC: {auc:.3f}\")\n",
    "    print(f\"   - Gini: {gini:.3f}\")\n",
    "    \n",
    "    # 4. Risk tier distribution\n",
    "    print(\"\\n4. Risk Tier Distribution:\")\n",
    "    tier_counts = results_df['risk_tier'].value_counts()\n",
    "    for tier, count in tier_counts.items():\n",
    "        pct = count / len(results_df) * 100\n",
    "        print(f\"   - {tier}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final scoring output\n",
    "final_output = pd.DataFrame({\n",
    "    'app_id': results_df['app_id'],\n",
    "    'score_date': pd.Timestamp.now().date(),\n",
    "    'credit_score': results_df['credit_score'],\n",
    "    'risk_tier': results_df['risk_tier'],\n",
    "    'raw_probability': results_df['raw_probability'].round(4),\n",
    "    'calibrated_probability': results_df['calibrated_probability'].round(4),\n",
    "    'model_version': pipeline.best_model_name_\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'production_scoring_results.csv'\n",
    "final_output.to_csv(output_file, index=False)\n",
    "print(f\"Scoring results saved to: {output_file}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of scoring output:\")\n",
    "display(final_output.head(10))\n",
    "\n",
    "# Save calibration model\n",
    "import pickle\n",
    "with open('calibration_model.pkl', 'wb') as f:\n",
    "    pickle.dump(iso_reg, f)\n",
    "print(\"\\n✅ Calibration model saved to: calibration_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Complete Workflow Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPLETE END-TO-END WORKFLOW SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATA PREPARATION:\")\n",
    "print(f\"   - Dataset size: {len(df):,} samples\")\n",
    "print(f\"   - Target rate: {df['target'].mean():.2%}\")\n",
    "print(f\"   - Features: {len(df.columns) - 3} (numeric + categorical)\")\n",
    "\n",
    "print(\"\\n2. PIPELINE CONFIGURATION:\")\n",
    "print(f\"   - Dual pipeline: {config.enable_dual_pipeline}\")\n",
    "print(f\"   - HPO trials: {config.n_trials}\")\n",
    "print(f\"   - Feature selection: Boruta + Forward\")\n",
    "print(f\"   - Data split: {config.train_ratio}/{config.test_ratio}/{config.oot_ratio}\")\n",
    "\n",
    "print(\"\\n3. MODEL TRAINING:\")\n",
    "if hasattr(pipeline, 'best_model_name_'):\n",
    "    best_row = pipeline.models_summary_[pipeline.models_summary_['model_name'] == pipeline.best_model_name_].iloc[0]\n",
    "    print(f\"   - Best model: {pipeline.best_model_name_}\")\n",
    "    print(f\"   - Train Gini: {best_row['Gini_Train']:.2%}\")\n",
    "    print(f\"   - OOT Gini: {best_row['Gini_OOT']:.2%}\")\n",
    "    if 0.70 <= best_row['Gini_Train'] <= 0.80:\n",
    "        print(f\"   - ✅ TARGET ACHIEVED (70-80% range)\")\n",
    "    else:\n",
    "        print(f\"   - ⚠️ Outside target range\")\n",
    "\n",
    "print(\"\\n4. CALIBRATION:\")\n",
    "print(f\"   - Calibration samples: {len(calib_df):,}\")\n",
    "print(f\"   - Error before: {error_before:.4f}\")\n",
    "print(f\"   - Error after: {error_after:.4f}\")\n",
    "print(f\"   - Improvement: {improvement:.1f}%\")\n",
    "\n",
    "print(\"\\n5. PRODUCTION SCORING:\")\n",
    "print(f\"   - Scored samples: {len(scores):,}\")\n",
    "print(f\"   - Production Gini: {gini_calibrated:.2%}\")\n",
    "print(f\"   - Score range: {scores.min()}-{scores.max()}\")\n",
    "print(f\"   - Mean score: {scores.mean():.0f}\")\n",
    "\n",
    "print(\"\\n6. RISK SEGMENTATION:\")\n",
    "for tier in tier_analysis.index:\n",
    "    count = tier_analysis.loc[tier, 'Count']\n",
    "    rate = tier_analysis.loc[tier, 'Actual_Rate']\n",
    "    print(f\"   - {tier:20s}: {count:,} samples ({rate:.2%} default rate)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ COMPLETE WORKFLOW EXECUTED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📋 Next Steps:\")\n",
    "print(\"   1. Deploy model to production environment\")\n",
    "print(\"   2. Set up monitoring for PSI and model drift\")\n",
    "print(\"   3. Implement A/B testing framework\")\n",
    "print(\"   4. Schedule periodic model retraining\")\n",
    "print(\"   5. Create automated reporting dashboard\")\n",
    "print(\"   6. Set up alert system for performance degradation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
