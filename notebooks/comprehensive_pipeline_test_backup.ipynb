{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Risk Model Pipeline Test\\n",
    "\\n",
    "This notebook demonstrates all features of the unified risk model pipeline:\\n",
    "- Single pipeline with config control\\n",
    "- Data dictionary support\\n",
    "- Stage 1 and Stage 2 calibration\\n",
    "- Complete selection methods (forward/backward/stepwise)\\n",
    "- Optimized binning for IV/Gini\\n",
    "- Support for GAM, CatBoost, ExtraTrees\\n",
    "- Risk band optimization with Herfindahl Index\\n",
    "- Comprehensive reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\\n",
    "import sys\\n",
    "import os\\n",
    "import warnings\\n",
    "import numpy as np\\n",
    "import pandas as pd\\n",
    "from datetime import datetime, timedelta\\n",
    "import matplotlib.pyplot as plt\\n",
    "import seaborn as sns\\n",
    "\\n",
    "warnings.filterwarnings('ignore')\\n",
    "\\n",
    "# Add parent directory to path\\n",
    "sys.path.insert(0, os.path.abspath('../'))\\n",
    "\\n",
    "# Import pipeline components\\n",
    "from src.risk_pipeline.core.config import Config\\n",
    "from src.risk_pipeline.pipeline_v2 import UnifiedRiskPipeline\\n",
    "\\n",
    "print(\\\"All libraries imported successfully!\\\")\\n",
    "print(f\\\"Working directory: {os.getcwd()}\\\")\\n",
    "print(f\\\"Python version: {sys.version}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data with Realistic Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\\n",
    "np.random.seed(42)\\n",
    "\\n",
    "# Generate synthetic credit risk data\\n",
    "n_samples = 10000\\n",
    "\\n",
    "# Create base features with different patterns\\n",
    "data = pd.DataFrame({\\n",
    "    'customer_id': range(n_samples),\\n",
    "    'application_date': pd.date_range(\\n",
    "        start='2020-01-01', \\n",
    "        periods=n_samples, \\n",
    "        freq='H'\\n",
    "    ),\\n",
    "    \\n",
    "    # Numeric features with different distributions\\n",
    "    'age': np.random.normal(40, 12, n_samples).clip(18, 80).astype(int),\\n",
    "    'income': np.random.lognormal(10.5, 0.6, n_samples),\\n",
    "    'loan_amount': np.random.lognormal(9.5, 0.8, n_samples),\\n",
    "    'employment_years': np.random.exponential(5, n_samples).clip(0, 40),\\n",
    "    'credit_score': np.random.normal(650, 100, n_samples).clip(300, 850),\\n",
    "    'debt_to_income': np.random.beta(2, 5, n_samples) * 100,\\n",
    "    'num_credit_lines': np.random.poisson(3, n_samples),\\n",
    "    'months_since_last_delinquent': np.random.exponential(24, n_samples),\\n",
    "    \\n",
    "    # Categorical features\\n",
    "    'home_ownership': np.random.choice(['RENT', 'OWN', 'MORTGAGE'], n_samples, p=[0.35, 0.25, 0.40]),\\n",
    "    'loan_purpose': np.random.choice(['debt_consolidation', 'credit_card', 'home_improvement', 'other'], \\n",
    "                                    n_samples, p=[0.4, 0.2, 0.2, 0.2]),\\n",
    "    'employment_type': np.random.choice(['Full-time', 'Part-time', 'Self-employed', 'Retired'], \\n",
    "                                       n_samples, p=[0.6, 0.15, 0.20, 0.05]),\\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], \\n",
    "                                 n_samples, p=[0.3, 0.4, 0.25, 0.05]),\\n",
    "    'marital_status': np.random.choice(['Single', 'Married', 'Divorced'], \\n",
    "                                      n_samples, p=[0.3, 0.5, 0.2])\\n",
    "})\\n",
    "\\n",
    "# Add some missing values\\n",
    "missing_cols = ['months_since_last_delinquent', 'employment_years']\\n",
    "for col in missing_cols:\\n",
    "    missing_idx = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)\\n",
    "    data.loc[missing_idx, col] = np.nan\\n",
    "\\n",
    "# Create target variable with realistic default patterns\\n",
    "# Higher default probability for certain conditions\\n",
    "default_prob = 0.05  # Base default rate\\n",
    "\\n",
    "# Calculate risk score\\n",
    "risk_score = (\\n",
    "    (data['credit_score'] < 600).astype(float) * 0.3 +\\n",
    "    (data['debt_to_income'] > 40).astype(float) * 0.25 +\\n",
    "    (data['loan_amount'] / data['income'] > 0.5).astype(float) * 0.2 +\\n",
    "    (data['employment_years'] < 2).astype(float) * 0.15 +\\n",
    "    (data['home_ownership'] == 'RENT').astype(float) * 0.1\\n",
    ")\\n",
    "\\n",
    "# Add some noise\\n",
    "risk_score += np.random.normal(0, 0.05, n_samples)\\n",
    "\\n",
    "# Convert to probability and generate defaults\\n",
    "default_prob_adjusted = default_prob * (1 + risk_score * 3)\\n",
    "default_prob_adjusted = np.clip(default_prob_adjusted, 0, 0.5)\\n",
    "data['default'] = (np.random.random(n_samples) < default_prob_adjusted).astype(int)\\n",
    "\\n",
    "print(f\\\"Dataset created with {n_samples} samples\\\")\\n",
    "print(f\\\"Default rate: {data['default'].mean():.2%}\\\")\\n",
    "print(f\\\"Shape: {data.shape}\\\")\\n",
    "print(f\\\"\\\\nFeatures: {list(data.columns)}\\\")\\n",
    "print(f\\\"\\\\nFirst 5 rows:\\\")\\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data dictionary\\n",
    "data_dictionary = pd.DataFrame([\\n",
    "    {'variable': 'age', 'description': 'Customer age in years', 'type': 'numeric', 'category': 'demographic'},\\n",
    "    {'variable': 'income', 'description': 'Annual income in USD', 'type': 'numeric', 'category': 'financial'},\\n",
    "    {'variable': 'loan_amount', 'description': 'Requested loan amount', 'type': 'numeric', 'category': 'loan'},\\n",
    "    {'variable': 'employment_years', 'description': 'Years at current employer', 'type': 'numeric', 'category': 'employment'},\\n",
    "    {'variable': 'credit_score', 'description': 'Credit bureau score', 'type': 'numeric', 'category': 'credit'},\\n",
    "    {'variable': 'debt_to_income', 'description': 'Debt to income ratio (%)', 'type': 'numeric', 'category': 'financial'},\\n",
    "    {'variable': 'num_credit_lines', 'description': 'Number of open credit lines', 'type': 'numeric', 'category': 'credit'},\\n",
    "    {'variable': 'months_since_last_delinquent', 'description': 'Months since last delinquency', 'type': 'numeric', 'category': 'credit'},\\n",
    "    {'variable': 'home_ownership', 'description': 'Home ownership status', 'type': 'categorical', 'category': 'demographic'},\\n",
    "    {'variable': 'loan_purpose', 'description': 'Purpose of the loan', 'type': 'categorical', 'category': 'loan'},\\n",
    "    {'variable': 'employment_type', 'description': 'Type of employment', 'type': 'categorical', 'category': 'employment'},\\n",
    "    {'variable': 'education', 'description': 'Education level', 'type': 'categorical', 'category': 'demographic'},\\n",
    "    {'variable': 'marital_status', 'description': 'Marital status', 'type': 'categorical', 'category': 'demographic'}\\n",
    "])\\n",
    "\\n",
    "print(\\\"Data dictionary created:\\\")\\n",
    "data_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Calibration Data for Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create recent data for Stage 2 calibration (last 3 months)\\n",
    "recent_cutoff = data['application_date'].max() - pd.DateOffset(months=3)\\n",
    "stage2_data = data[data['application_date'] >= recent_cutoff].copy()\\n",
    "\\n",
    "# Simulate a slight shift in recent default rates\\n",
    "stage2_data['default'] = stage2_data['default'].apply(\\n",
    "    lambda x: 1 if np.random.random() < 0.15 else x  # Increase recent defaults slightly\\n",
    ")\\n",
    "\\n",
    "print(f\\\"Stage 2 calibration data:\\\")\\n",
    "print(f\\\"  Samples: {len(stage2_data)}\\\")\\n",
    "print(f\\\"  Date range: {stage2_data['application_date'].min()} to {stage2_data['application_date'].max()}\\\")\\n",
    "print(f\\\"  Default rate: {stage2_data['default'].mean():.2%}\\\")\\n",
    "\\n",
    "# Create long-run calibration data\\n",
    "calibration_data = data.copy()\\n",
    "print(f\\\"\\\\nStage 1 calibration data (long-run):\\\")\\n",
    "print(f\\\"  Samples: {len(calibration_data)}\\\")\\n",
    "print(f\\\"  Default rate: {calibration_data['default'].mean():.2%}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Pipeline with All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive configuration\\n",
    "config = Config(\\n",
    "    # Basic settings\\n",
    "    target_col='default',\\n",
    "    id_col='customer_id',\\n",
    "    time_col='application_date',\\n",
    "    random_state=42,\\n",
    "    \\n",
    "    # Pipeline mode\\n",
    "    enable_scoring=False,  # Scoring disabled by default\\n",
    "    enable_woe=True,\\n",
    "    enable_noise_sentinel=False,  # Disabled due to known issues\\n",
    "    \\n",
    "    # Split configuration\\n",
    "    test_ratio=0.2,\\n",
    "    oot_months=6,\\n",
    "    equal_default_splits=True,  # Equal default rates across splits\\n",
    "    \\n",
    "    # Selection configuration\\n",
    "    selection_order=['psi', 'vif', 'correlation', 'iv', 'boruta', 'stepwise'],\\n",
    "    selection_method='stepwise',  # Use stepwise selection\\n",
    "    max_features=15,\\n",
    "    \\n",
    "    # Binning configuration\\n",
    "    binning_method='optimized',  # IV/Gini optimized binning\\n",
    "    min_bin_size=0.05,\\n",
    "    max_bins=10,\\n",
    "    monotonic_woe=True,\\n",
    "    \\n",
    "    # Threshold configuration\\n",
    "    psi_threshold=0.25,\\n",
    "    vif_threshold=10,\\n",
    "    correlation_threshold=0.9,\\n",
    "    iv_threshold=0.02,\\n",
    "    \\n",
    "    # Model configuration\\n",
    "    model_type='all',  # Train all available models\\n",
    "    use_optuna=False,  # Disable for speed\\n",
    "    \\n",
    "    # Calibration configuration\\n",
    "    enable_calibration=True,\\n",
    "    calibration_method='isotonic',\\n",
    "    stage2_method='lower_mean',\\n",
    "    \\n",
    "    # Risk bands configuration\\n",
    "    n_risk_bands=10,\\n",
    "    band_method='quantile',\\n",
    "    \\n",
    "    # Output configuration\\n",
    "    output_dir='../outputs',\\n",
    "    save_plots=True,\\n",
    "    save_model=True\\n",
    ")\\n",
    "\\n",
    "print(\\\"Pipeline configuration created\\\")\\n",
    "print(f\\\"\\\\nKey settings:\\\")\\n",
    "print(f\\\"  Selection order: {config.selection_order}\\\")\\n",
    "print(f\\\"  Selection method: {config.selection_method}\\\")\\n",
    "print(f\\\"  Binning method: {config.binning_method}\\\")\\n",
    "print(f\\\"  Equal default splits: {config.equal_default_splits}\\\")\\n",
    "print(f\\\"  Calibration enabled: {config.enable_calibration}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize and Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize unified pipeline\\n",
    "pipeline = UnifiedRiskPipeline(config=config)\\n",
    "\\n",
    "print(\\\"Pipeline initialized successfully!\\\")\\n",
    "print(f\\\"\\\\nComponents:\\\")\\n",
    "print(f\\\"  Data Processor: {type(pipeline.processor).__name__}\\\")\\n",
    "print(f\\\"  Feature Selector: {type(pipeline.selector).__name__}\\\")\\n",
    "print(f\\\"  Model Builder: {type(pipeline.model_builder).__name__}\\\")\\n",
    "print(f\\\"  Risk Band Optimizer: {type(pipeline.risk_band_optimizer).__name__}\\\")\\n",
    "print(f\\\"  Calibration Engine: {type(pipeline.calibration_engine).__name__}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete pipeline\\n",
    "print(\\\"Running unified pipeline...\\\")\\n",
    "print(\\\"=\\\" * 80)\\n",
    "\\n",
    "results = pipeline.fit(\\n",
    "    df=data,\\n",
    "    data_dictionary=data_dictionary,\\n",
    "    calibration_data=calibration_data,\\n",
    "    stage2_data=stage2_data\\n",
    ")\\n",
    "\\n",
    "print(\\\"\\\\n\\\" + \\\"=\\\" * 80)\\n",
    "print(\\\"Pipeline completed successfully!\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model results\\n",
    "print(\\\"Model Performance Summary\\\")\\n",
    "print(\\\"=\\\" * 60)\\n",
    "\\n",
    "if 'model_scores' in results:\\n",
    "    scores_df = pd.DataFrame(results['model_scores']).T\\n",
    "    print(scores_df.round(4))\\n",
    "    \\n",
    "    # Best model\\n",
    "    print(f\\\"\\\\nBest Model: {results.get('best_model_name', 'N/A')}\\\")\\n",
    "    print(f\\\"Best AUC: {results.get('best_auc', 0):.4f}\\\")\\n",
    "\\n",
    "# Display selected features\\n",
    "if 'selected_features' in results:\\n",
    "    print(f\\\"\\\\nSelected Features ({len(results['selected_features'])}):\\\")\\n",
    "    for i, feat in enumerate(results['selected_features'][:10], 1):\\n",
    "        print(f\\\"  {i}. {feat}\\\")\\n",
    "    if len(results['selected_features']) > 10:\\n",
    "        print(f\\\"  ... and {len(results['selected_features'])-10} more\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display risk bands analysis\\n",
    "print(\\\"Risk Bands Analysis\\\")\\n",
    "print(\\\"=\\\" * 60)\\n",
    "\\n",
    "if 'risk_bands' in results:\\n",
    "    risk_bands = results['risk_bands']\\n",
    "    print(risk_bands[['band', 'count', 'bad_rate', 'avg_score', 'ks', 'psi']].round(4))\\n",
    "    \\n",
    "    # Display Herfindahl Index and other metrics\\n",
    "    if 'risk_band_metrics' in results:\\n",
    "        metrics = results['risk_band_metrics']\\n",
    "        print(f\\\"\\\\nRisk Band Metrics:\\\")\\n",
    "        print(f\\\"  Herfindahl Index: {metrics.get('herfindahl_index', 0):.4f}\\\")\\n",
    "        print(f\\\"  Entropy: {metrics.get('entropy', 0):.4f}\\\")\\n",
    "        print(f\\\"  Gini Coefficient: {metrics.get('gini_coefficient', 0):.4f}\\\")\\n",
    "        print(f\\\"  Hosmer-Lemeshow p-value: {metrics.get('hosmer_lemeshow_p', 0):.4f}\\\")\\n",
    "        \\n",
    "        # Binomial test results\\n",
    "        if 'binomial_tests' in metrics:\\n",
    "            print(f\\\"\\\\nBinomial Test Results:\\\")\\n",
    "            for band, p_value in metrics['binomial_tests'].items():\\n",
    "                status = 'PASS' if p_value > 0.05 else 'FAIL'\\n",
    "                print(f\\\"    Band {band}: p={p_value:.4f} [{status}]\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display calibration results\\n",
    "print(\\\"Calibration Analysis\\\")\\n",
    "print(\\\"=\\\" * 60)\\n",
    "\\n",
    "if 'calibration_metrics' in results:\\n",
    "    cal_metrics = results['calibration_metrics']\\n",
    "    \\n",
    "    print(\\\"Stage 1 Calibration (Long-run):\\\")\\n",
    "    if 'stage1' in cal_metrics:\\n",
    "        for key, value in cal_metrics['stage1'].items():\\n",
    "            print(f\\\"  {key}: {value:.4f}\\\" if isinstance(value, (int, float)) else f\\\"  {key}: {value}\\\")\\n",
    "    \\n",
    "    print(\\\"\\\\nStage 2 Calibration (Recent period):\\\")\\n",
    "    if 'stage2' in cal_metrics:\\n",
    "        for key, value in cal_metrics['stage2'].items():\\n",
    "            print(f\\\"  {key}: {value:.4f}\\\" if isinstance(value, (int, float)) else f\\\"  {key}: {value}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PSI analysis\\n",
    "print(\\\"Population Stability Index (PSI) Analysis\\\")\\n",
    "print(\\\"=\\\" * 60)\\n",
    "\\n",
    "if 'psi_results' in results:\\n",
    "    psi_df = pd.DataFrame(results['psi_results'])\\n",
    "    if not psi_df.empty:\\n",
    "        # Feature PSI\\n",
    "        if 'feature_psi' in psi_df.columns:\\n",
    "            print(\\\"Feature PSI:\\\")\\n",
    "            print(psi_df[['feature', 'feature_psi']].sort_values('feature_psi', ascending=False).head(10))\\n",
    "        \\n",
    "        # Score PSI\\n",
    "        if 'score_psi' in results:\\n",
    "            print(f\\\"\\\\nScore PSI: {results['score_psi']:.4f}\\\")\\n",
    "            if results['score_psi'] < 0.1:\\n",
    "                print(\\\"  Status: Stable (PSI < 0.1)\\\")\\n",
    "            elif results['score_psi'] < 0.25:\\n",
    "                print(\\\"  Status: Minor shift (0.1 <= PSI < 0.25)\\\")\\n",
    "            else:\\n",
    "                print(\\\"  Status: Major shift (PSI >= 0.25)\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\\n",
    "\\n",
    "# 1. Model AUC Comparison\\n",
    "if 'model_scores' in results:\\n",
    "    ax = axes[0, 0]\\n",
    "    scores_df = pd.DataFrame(results['model_scores']).T\\n",
    "    if 'test_auc' in scores_df.columns:\\n",
    "        scores_df['test_auc'].plot(kind='bar', ax=ax, color='steelblue')\\n",
    "        ax.set_title('Model AUC Comparison')\\n",
    "        ax.set_xlabel('Model')\\n",
    "        ax.set_ylabel('AUC')\\n",
    "        ax.axhline(y=0.7, color='r', linestyle='--', alpha=0.5)\\n",
    "        ax.grid(True, alpha=0.3)\\n",
    "\\n",
    "# 2. Risk Band Distribution\\n",
    "if 'risk_bands' in results:\\n",
    "    ax = axes[0, 1]\\n",
    "    risk_bands = results['risk_bands']\\n",
    "    ax.bar(risk_bands['band'], risk_bands['count'], color='coral')\\n",
    "    ax.set_title('Risk Band Distribution')\\n",
    "    ax.set_xlabel('Risk Band')\\n",
    "    ax.set_ylabel('Count')\\n",
    "    ax.grid(True, alpha=0.3)\\n",
    "\\n",
    "# 3. Default Rate by Risk Band\\n",
    "if 'risk_bands' in results:\\n",
    "    ax = axes[0, 2]\\n",
    "    ax.plot(risk_bands['band'], risk_bands['bad_rate'], 'o-', color='darkred', linewidth=2)\\n",
    "    ax.set_title('Default Rate by Risk Band')\\n",
    "    ax.set_xlabel('Risk Band')\\n",
    "    ax.set_ylabel('Default Rate')\\n",
    "    ax.grid(True, alpha=0.3)\\n",
    "\\n",
    "# 4. Feature Importance\\n",
    "if 'feature_importance' in results:\\n",
    "    ax = axes[1, 0]\\n",
    "    importance_df = pd.DataFrame(results['feature_importance']).head(10)\\n",
    "    ax.barh(importance_df['feature'], importance_df['importance'], color='green')\\n",
    "    ax.set_title('Top 10 Feature Importance')\\n",
    "    ax.set_xlabel('Importance')\\n",
    "    ax.invert_yaxis()\\n",
    "\\n",
    "# 5. Calibration Plot\\n",
    "if 'calibration_data' in results:\\n",
    "    ax = axes[1, 1]\\n",
    "    cal_data = results['calibration_data']\\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\\n",
    "    if 'predicted' in cal_data and 'actual' in cal_data:\\n",
    "        ax.plot(cal_data['predicted'], cal_data['actual'], 'o-', color='blue')\\n",
    "    ax.set_title('Calibration Plot')\\n",
    "    ax.set_xlabel('Predicted Probability')\\n",
    "    ax.set_ylabel('Actual Probability')\\n",
    "    ax.grid(True, alpha=0.3)\\n",
    "\\n",
    "# 6. PSI Distribution\\n",
    "if 'psi_results' in results:\\n",
    "    ax = axes[1, 2]\\n",
    "    psi_df = pd.DataFrame(results['psi_results'])\\n",
    "    if 'feature_psi' in psi_df.columns:\\n",
    "        psi_values = psi_df['feature_psi'].head(10)\\n",
    "        ax.bar(range(len(psi_values)), psi_values, color='purple')\\n",
    "        ax.axhline(y=0.1, color='g', linestyle='--', alpha=0.5, label='Stable')\\n",
    "        ax.axhline(y=0.25, color='r', linestyle='--', alpha=0.5, label='Shift')\\n",
    "        ax.set_title('Feature PSI Values')\\n",
    "        ax.set_xlabel('Feature Index')\\n",
    "        ax.set_ylabel('PSI')\\n",
    "        ax.legend()\\n",
    "        ax.grid(True, alpha=0.3)\\n",
    "\\n",
    "plt.tight_layout()\\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate SQL and Python Code for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate deployment code\\n",
    "print(\\\"Deployment Code Generation\\\")\\n",
    "print(\\\"=\\\" * 60)\\n",
    "\\n",
    "# Get SQL code for risk bands\\n",
    "if hasattr(pipeline.risk_band_optimizer, 'export_sql'):\\n",
    "    sql_code = pipeline.risk_band_optimizer.export_sql()\\n",
    "    print(\\\"SQL Code for Risk Bands:\\\")\\n",
    "    print(\\\"-\\\" * 40)\\n",
    "    print(sql_code[:500])  # Show first 500 chars\\n",
    "    if len(sql_code) > 500:\\n",
    "        print(\\\"...\\\\n[Truncated for display]\\\")\\n",
    "\\n",
    "# Get Python code\\n",
    "if hasattr(pipeline.risk_band_optimizer, 'export_python'):\\n",
    "    python_code = pipeline.risk_band_optimizer.export_python()\\n",
    "    print(\\\"\\\\nPython Code for Risk Bands:\\\")\\n",
    "    print(\\\"-\\\" * 40)\\n",
    "    print(python_code[:500])  # Show first 500 chars\\n",
    "    if len(python_code) > 500:\\n",
    "        print(\\\"...\\\\n[Truncated for display]\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Different Model Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual model types\\n",
    "model_types = ['XGBoost', 'LightGBM', 'CatBoost', 'GAM', 'ExtraTrees']\\n",
    "model_results = {}\\n",
    "\\n",
    "print(\\\"Testing Individual Model Types\\\")\\n",
    "print(\\\"=\\\" * 60)\\n",
    "\\n",
    "for model_type in model_types:\\n",
    "    try:\\n",
    "        # Update config for specific model\\n",
    "        config.model_type = model_type\\n",
    "        \\n",
    "        # Create and run pipeline\\n",
    "        pipeline_test = UnifiedRiskPipeline(config=config)\\n",
    "        \\n",
    "        # Run with minimal data for speed\\n",
    "        test_data = data.sample(n=2000, random_state=42)\\n",
    "        results_test = pipeline_test.fit(test_data)\\n",
    "        \\n",
    "        # Store results\\n",
    "        if 'best_auc' in results_test:\\n",
    "            model_results[model_type] = results_test['best_auc']\\n",
    "            print(f\\\"{model_type}: AUC = {results_test['best_auc']:.4f}\\\")\\n",
    "        else:\\n",
    "            print(f\\\"{model_type}: Training completed but no AUC available\\\")\\n",
    "    except Exception as e:\\n",
    "        print(f\\\"{model_type}: Not available ({str(e)[:50]}...)\\\")\\n",
    "\\n",
    "# Display best model\\n",
    "if model_results:\\n",
    "    best_model = max(model_results, key=model_results.get)\\n",
    "    print(f\\\"\\\\nBest Individual Model: {best_model} (AUC = {model_results[best_model]:.4f})\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\\n",
    "print(\\\"COMPREHENSIVE PIPELINE SUMMARY\\\")\\n",
    "print(\\\"=\\\" * 80)\\n",
    "\\n",
    "summary = []\\n",
    "\\n",
    "# Data summary\\n",
    "summary.append(\\\"DATA SUMMARY\\\")\\n",
    "summary.append(f\\\"  Total samples: {len(data)}\\\")\\n",
    "summary.append(f\\\"  Features: {len(data.columns) - 3}\\\")\\n",
    "summary.append(f\\\"  Default rate: {data['default'].mean():.2%}\\\")\\n",
    "summary.append(\\\"\\\")\\n",
    "\\n",
    "# Pipeline configuration\\n",
    "summary.append(\\\"PIPELINE CONFIGURATION\\\")\\n",
    "summary.append(f\\\"  Selection method: {config.selection_method}\\\")\\n",
    "summary.append(f\\\"  Binning method: {config.binning_method}\\\")\\n",
    "summary.append(f\\\"  Calibration: Stage 1 ({config.calibration_method}) + Stage 2 ({config.stage2_method})\\\")\\n",
    "summary.append(f\\\"  Risk bands: {config.n_risk_bands} bands using {config.band_method}\\\")\\n",
    "summary.append(\\\"\\\")\\n",
    "\\n",
    "# Results summary\\n",
    "if results:\\n",
    "    summary.append(\\\"MODEL PERFORMANCE\\\")\\n",
    "    if 'best_model_name' in results:\\n",
    "        summary.append(f\\\"  Best model: {results['best_model_name']}\\\")\\n",
    "    if 'best_auc' in results:\\n",
    "        summary.append(f\\\"  Best AUC: {results['best_auc']:.4f}\\\")\\n",
    "    if 'selected_features' in results:\\n",
    "        summary.append(f\\\"  Selected features: {len(results['selected_features'])}\\\")\\n",
    "    summary.append(\\\"\\\")\\n",
    "    \\n",
    "    # Risk band metrics\\n",
    "    if 'risk_band_metrics' in results:\\n",
    "        metrics = results['risk_band_metrics']\\n",
    "        summary.append(\\\"RISK BAND METRICS\\\")\\n",
    "        if 'herfindahl_index' in metrics:\\n",
    "            summary.append(f\\\"  Herfindahl Index: {metrics['herfindahl_index']:.4f}\\\")\\n",
    "        if 'entropy' in metrics:\\n",
    "            summary.append(f\\\"  Entropy: {metrics['entropy']:.4f}\\\")\\n",
    "        if 'gini_coefficient' in metrics:\\n",
    "            summary.append(f\\\"  Gini Coefficient: {metrics['gini_coefficient']:.4f}\\\")\\n",
    "        summary.append(\\\"\\\")\\n",
    "    \\n",
    "    # Calibration metrics\\n",
    "    if 'calibration_metrics' in results:\\n",
    "        summary.append(\\\"CALIBRATION METRICS\\\")\\n",
    "        cal_metrics = results['calibration_metrics']\\n",
    "        if 'stage1' in cal_metrics and 'ece' in cal_metrics['stage1']:\\n",
    "            summary.append(f\\\"  Stage 1 ECE: {cal_metrics['stage1']['ece']:.4f}\\\")\\n",
    "        if 'stage2' in cal_metrics and 'ece' in cal_metrics['stage2']:\\n",
    "            summary.append(f\\\"  Stage 2 ECE: {cal_metrics['stage2']['ece']:.4f}\\\")\\n",
    "        summary.append(\\\"\\\")\\n",
    "\\n",
    "# Features tested\\n",
    "summary.append(\\\"FEATURES TESTED\\\")\\n",
    "summary.append(\\\"  - Single unified pipeline with config control\\\")\\n",
    "summary.append(\\\"  - Data dictionary integration\\\")\\n",
    "summary.append(\\\"  - Stage 1 and Stage 2 calibration\\\")\\n",
    "summary.append(\\\"  - Complete selection methods (forward/backward/stepwise)\\\")\\n",
    "summary.append(\\\"  - Optimized binning for IV/Gini\\\")\\n",
    "summary.append(\\\"  - Extended model support (GAM, CatBoost, ExtraTrees)\\\")\\n",
    "summary.append(\\\"  - Equal default rate splits\\\")\\n",
    "summary.append(\\\"  - Risk band optimization with Herfindahl Index\\\")\\n",
    "summary.append(\\\"  - Comprehensive PSI analysis\\\")\\n",
    "summary.append(\\\"  - Deployment code generation\\\")\\n",
    "\\n",
    "# Print summary\\n",
    "for line in summary:\\n",
    "    print(line)\\n",
    "\\n",
    "print(\\\"\\\\n\\\" + \\\"=\\\" * 80)\\n",
    "print(\\\"Pipeline test completed successfully!\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}