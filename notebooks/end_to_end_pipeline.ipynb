{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Risk Model Pipeline\n",
    "## Complete Implementation with All Features\n",
    "\n",
    "This notebook demonstrates the complete risk model pipeline including:\n",
    "- Data loading and preprocessing\n",
    "- Feature engineering and selection\n",
    "- WOE transformation\n",
    "- Model training and evaluation\n",
    "- PSI monitoring\n",
    "- Calibration analysis\n",
    "- Risk band optimization\n",
    "- SHAP analysis\n",
    "- Comprehensive reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import sys\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# Install/Update risk-pipeline package from GitHub\nprint(\"Installing/Updating risk-pipeline from GitHub...\")\nprint(\"Uninstalling existing versions...\")\n!pip uninstall risk-pipeline risk-model-pipeline -y -q 2>/dev/null\nprint(\"Installing from GitHub development branch...\")\n!pip install git+https://github.com/selimoksuz/risk-model-pipeline.git@development --upgrade --force-reinstall -q\nprint(\"‚úÖ Package installed from development branch\")\n\n# Verify installation\ntry:\n    import risk_pipeline\n    print(f\"‚úÖ risk_pipeline version: {risk_pipeline.__version__ if hasattr(risk_pipeline, '__version__') else 'Unknown'}\")\nexcept ImportError as e:\n    print(f\"‚ùå Error importing risk_pipeline: {e}\")\n    print(\"Trying alternative import...\")\n\n# Import pipeline components\nfrom risk_pipeline.core.config import Config\nfrom risk_pipeline.core.data_processor import DataProcessor\nfrom risk_pipeline.core.splitter import DataSplitter\nfrom risk_pipeline.core.feature_engineer import FeatureEngineer\nfrom risk_pipeline.core.feature_selector import FeatureSelector\nfrom risk_pipeline.core.woe_transformer import WOETransformer\nfrom risk_pipeline.core.model_builder import ModelBuilder\nfrom risk_pipeline.core.psi_calculator import PSICalculator\nfrom risk_pipeline.core.calibration_analyzer import CalibrationAnalyzer\nfrom risk_pipeline.core.risk_band_optimizer import RiskBandOptimizer\nfrom risk_pipeline.core.reporter import Reporter\nfrom risk_pipeline.utils.metrics import calculate_metrics\nfrom risk_pipeline.utils.visualization import VisualizationHelper\nfrom risk_pipeline.utils.error_handler import ErrorHandler\n\n# Complete pipeline imports\nfrom risk_pipeline.complete_pipeline import CompletePipeline\nfrom risk_pipeline.advanced_pipeline import AdvancedRiskPipeline\n\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8-darkgrid')\n%matplotlib inline\n\nprint(f\"‚úÖ All modules imported successfully\")\nprint(f\"Pipeline initialized at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing/Updating risk-pipeline from GitHub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping risk-pipeline as it is not installed.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Eri≈üim engellendi: 'C:\\\\Users\\\\Acer\\\\anaconda3\\\\Lib\\\\site-packages\\\\numpy\\\\~~ibs\\\\libopenblas64__v0.3.21-gcc_10_3_0.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Package installed from development branch\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AdvancedPipeline' from 'risk_pipeline.advanced_pipeline' (C:\\Users\\Acer\\risk-model-pipeline\\src\\risk_pipeline\\advanced_pipeline.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20176\\1114792031.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Import pipeline components\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrisk_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrisk_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_processor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataProcessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrisk_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataSplitter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\risk-model-pipeline\\src\\risk_pipeline\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDualRiskModelPipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRiskModelPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcomplete_pipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCompletePipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0madvanced_pipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdvancedPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m __all__ = [\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'AdvancedPipeline' from 'risk_pipeline.advanced_pipeline' (C:\\Users\\Acer\\risk-model-pipeline\\src\\risk_pipeline\\advanced_pipeline.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Install/Update risk-pipeline package from GitHub\n",
    "print(\"Installing/Updating risk-pipeline from GitHub...\")\n",
    "!pip uninstall risk-pipeline -y -q\n",
    "!pip install git+https://github.com/selimoksuz/risk-model-pipeline.git@development --force-reinstall -q\n",
    "print(\"‚úÖ Package installed from development branch\")\n",
    "\n",
    "# Import pipeline components\n",
    "from risk_pipeline.core.config import Config\n",
    "from risk_pipeline.core.data_processor import DataProcessor\n",
    "from risk_pipeline.core.splitter import DataSplitter\n",
    "from risk_pipeline.core.feature_engineer import FeatureEngineer\n",
    "from risk_pipeline.core.feature_selector import FeatureSelector\n",
    "from risk_pipeline.core.woe_transformer import WOETransformer\n",
    "from risk_pipeline.core.model_builder import ModelBuilder\n",
    "from risk_pipeline.core.psi_calculator import PSICalculator\n",
    "from risk_pipeline.core.calibration_analyzer import CalibrationAnalyzer\n",
    "from risk_pipeline.core.risk_band_optimizer import RiskBandOptimizer\n",
    "from risk_pipeline.core.reporter import Reporter\n",
    "from risk_pipeline.utils.metrics import calculate_metrics\n",
    "from risk_pipeline.utils.visualization import VisualizationHelper\n",
    "from risk_pipeline.utils.error_handler import ErrorHandler\n",
    "\n",
    "# Complete pipeline imports\n",
    "from risk_pipeline.complete_pipeline import CompletePipeline\n",
    "from risk_pipeline.advanced_pipeline import AdvancedPipeline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Pipeline initialized at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration\n",
    "config = Config(\n",
    "    target_column='target',\n",
    "    test_size=0.2,\n",
    "    validation_size=0.1,\n",
    "    random_state=42,\n",
    "    cv_folds=5,\n",
    "    \n",
    "    # Feature engineering\n",
    "    create_polynomial=True,\n",
    "    create_interactions=True,\n",
    "    max_poly_degree=2,\n",
    "    \n",
    "    # Feature selection\n",
    "    selection_method='all',  # Use all methods\n",
    "    variance_threshold=0.01,\n",
    "    correlation_threshold=0.95,\n",
    "    top_k_features=50,\n",
    "    \n",
    "    # WOE parameters\n",
    "    max_bins=5,\n",
    "    min_samples_leaf=0.05,\n",
    "    \n",
    "    # Model parameters\n",
    "    scoring_metric='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    \n",
    "    # Advanced features\n",
    "    calculate_shap=True,\n",
    "    monitor_psi=True,\n",
    "    optimize_risk_bands=True,\n",
    "    perform_calibration=True,\n",
    "    \n",
    "    # Output\n",
    "    output_folder='outputs/end_to_end_pipeline',\n",
    "    save_plots=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Configuration set:\")\n",
    "print(f\"  - Target column: {config.target_column}\")\n",
    "print(f\"  - Test size: {config.test_size}\")\n",
    "print(f\"  - CV folds: {config.cv_folds}\")\n",
    "print(f\"  - Output folder: {config.output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = '../data/processed/model_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Data loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"\\nTarget rate: {df['target'].mean():.2%}\")\n",
    "\n",
    "# Basic info\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Missing values\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(missing[missing > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = DataProcessor(config)\n",
    "\n",
    "# Validate and freeze data\n",
    "df_processed = processor.validate_and_freeze(df)\n",
    "\n",
    "# Identify variable types\n",
    "numeric_vars = processor.get_numeric_columns(df_processed)\n",
    "categorical_vars = processor.get_categorical_columns(df_processed)\n",
    "\n",
    "print(f\"Numeric variables: {len(numeric_vars)}\")\n",
    "print(f\"Categorical variables: {len(categorical_vars)}\")\n",
    "\n",
    "# Handle missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "if numeric_vars:\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_processed[numeric_vars] = imputer.fit_transform(df_processed[numeric_vars])\n",
    "\n",
    "if categorical_vars:\n",
    "    df_processed[categorical_vars] = df_processed[categorical_vars].fillna('missing')\n",
    "\n",
    "print(\"\\nMissing values handled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Test/OOT Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "splitter = DataSplitter(config)\n",
    "\n",
    "# Check if we have a time column for OOT split\n",
    "if 'date' in df_processed.columns or 'created_at' in df_processed.columns:\n",
    "    time_col = 'date' if 'date' in df_processed.columns else 'created_at'\n",
    "    train, test, oot = splitter.split_with_oot(df_processed, time_column=time_col)\n",
    "else:\n",
    "    # Random split with validation as OOT\n",
    "    train, test, oot = splitter.split_train_test_validation(df_processed)\n",
    "\n",
    "print(f\"Train set: {train.shape[0]:,} rows ({train.shape[0]/df_processed.shape[0]:.1%})\")\n",
    "print(f\"Test set: {test.shape[0]:,} rows ({test.shape[0]/df_processed.shape[0]:.1%})\")\n",
    "print(f\"OOT set: {oot.shape[0]:,} rows ({oot.shape[0]/df_processed.shape[0]:.1%})\")\n",
    "\n",
    "print(\"\\nTarget rates:\")\n",
    "print(f\"  Train: {train['target'].mean():.2%}\")\n",
    "print(f\"  Test: {test['target'].mean():.2%}\")\n",
    "print(f\"  OOT: {oot['target'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "engineer = FeatureEngineer(config)\n",
    "\n",
    "# Create features\n",
    "print(\"Creating engineered features...\")\n",
    "train_eng = engineer.create_features(train)\n",
    "test_eng = engineer.transform(test)\n",
    "oot_eng = engineer.transform(oot)\n",
    "\n",
    "print(f\"\\nFeatures after engineering:\")\n",
    "print(f\"  Original: {train.shape[1]} features\")\n",
    "print(f\"  After engineering: {train_eng.shape[1]} features\")\n",
    "print(f\"  New features created: {train_eng.shape[1] - train.shape[1]}\")\n",
    "\n",
    "# Show some engineered features\n",
    "new_features = [col for col in train_eng.columns if col not in train.columns]\n",
    "if new_features:\n",
    "    print(f\"\\nSample of new features: {new_features[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize selector\n",
    "selector = FeatureSelector(config)\n",
    "\n",
    "# Select features\n",
    "selected_features = selector.select_features(\n",
    "    train_eng.drop(columns=['target']),\n",
    "    train_eng['target']\n",
    ")\n",
    "\n",
    "print(f\"Selected {len(selected_features)} features from {train_eng.shape[1]-1} candidates\")\n",
    "\n",
    "# Get feature importance\n",
    "if hasattr(selector, 'feature_importance_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': selector.feature_importance_[:len(selected_features)]\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 important features:\")\n",
    "    print(importance_df.head(10))\n",
    "\n",
    "# Apply selection\n",
    "train_selected = train_eng[selected_features + ['target']]\n",
    "test_selected = test_eng[selected_features + ['target']]\n",
    "oot_selected = oot_eng[selected_features + ['target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. WOE Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WOE transformer\n",
    "woe_transformer = WOETransformer(config)\n",
    "\n",
    "# Fit and transform\n",
    "train_woe = woe_transformer.fit_transform(\n",
    "    train_selected.drop(columns=['target']),\n",
    "    train_selected['target']\n",
    ")\n",
    "test_woe = woe_transformer.transform(test_selected.drop(columns=['target']))\n",
    "oot_woe = woe_transformer.transform(oot_selected.drop(columns=['target']))\n",
    "\n",
    "# Add target back\n",
    "train_woe['target'] = train_selected['target'].values\n",
    "test_woe['target'] = test_selected['target'].values\n",
    "oot_woe['target'] = oot_selected['target'].values\n",
    "\n",
    "print(f\"WOE transformation completed\")\n",
    "print(f\"  Train shape: {train_woe.shape}\")\n",
    "print(f\"  Test shape: {test_woe.shape}\")\n",
    "print(f\"  OOT shape: {oot_woe.shape}\")\n",
    "\n",
    "# Show WOE mapping for a sample variable\n",
    "if woe_transformer.woe_mapping_:\n",
    "    sample_var = list(woe_transformer.woe_mapping_.keys())[0]\n",
    "    print(f\"\\nWOE mapping for '{sample_var}':\")\n",
    "    print(woe_transformer.woe_mapping_[sample_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model builder\n",
    "model_builder = ModelBuilder(config)\n",
    "\n",
    "# Train models\n",
    "X_train = train_woe.drop(columns=['target'])\n",
    "y_train = train_woe['target']\n",
    "X_test = test_woe.drop(columns=['target'])\n",
    "y_test = test_woe['target']\n",
    "\n",
    "best_model, best_score, all_models = model_builder.build_models(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(f\"\\nBest model: {model_builder.best_model_name_}\")\n",
    "print(f\"Best CV score: {best_score:.4f}\")\n",
    "\n",
    "# Show all model scores\n",
    "print(\"\\nAll model scores:\")\n",
    "for name, score in model_builder.cv_scores_.items():\n",
    "    print(f\"  {name}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "train_pred = best_model.predict_proba(X_train)[:, 1]\n",
    "test_pred = best_model.predict_proba(X_test)[:, 1]\n",
    "oot_pred = best_model.predict_proba(oot_woe.drop(columns=['target']))[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "metrics = {\n",
    "    'Train': {\n",
    "        'AUC': roc_auc_score(y_train, train_pred),\n",
    "        'AP': average_precision_score(y_train, train_pred),\n",
    "        'Brier': brier_score_loss(y_train, train_pred)\n",
    "    },\n",
    "    'Test': {\n",
    "        'AUC': roc_auc_score(y_test, test_pred),\n",
    "        'AP': average_precision_score(y_test, test_pred),\n",
    "        'Brier': brier_score_loss(y_test, test_pred)\n",
    "    },\n",
    "    'OOT': {\n",
    "        'AUC': roc_auc_score(oot_woe['target'], oot_pred),\n",
    "        'AP': average_precision_score(oot_woe['target'], oot_pred),\n",
    "        'Brier': brier_score_loss(oot_woe['target'], oot_pred)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display metrics\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "print(\"Model Performance:\")\n",
    "print(metrics_df.round(4))\n",
    "\n",
    "# Check for overfitting\n",
    "overfit_score = metrics_df.loc['Train', 'AUC'] - metrics_df.loc['Test', 'AUC']\n",
    "print(f\"\\nOverfitting check (Train-Test AUC): {overfit_score:.4f}\")\n",
    "if overfit_score > 0.05:\n",
    "    print(\"  ‚ö†Ô∏è Warning: Potential overfitting detected\")\n",
    "else:\n",
    "    print(\"  ‚úÖ No significant overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. PSI Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PSI\n",
    "psi_calculator = PSICalculator()\n",
    "\n",
    "# Feature PSI\n",
    "feature_psi = {}\n",
    "for col in X_train.columns:\n",
    "    psi_test = psi_calculator.calculate(X_train[col], X_test[col])\n",
    "    psi_oot = psi_calculator.calculate(X_train[col], oot_woe.drop(columns=['target'])[col])\n",
    "    feature_psi[col] = {'test': psi_test, 'oot': psi_oot}\n",
    "\n",
    "# Score PSI\n",
    "score_psi_test = psi_calculator.calculate(train_pred, test_pred)\n",
    "score_psi_oot = psi_calculator.calculate(train_pred, oot_pred)\n",
    "\n",
    "print(\"PSI Analysis:\")\n",
    "print(f\"\\nScore PSI:\")\n",
    "print(f\"  Test: {score_psi_test:.4f}\")\n",
    "print(f\"  OOT: {score_psi_oot:.4f}\")\n",
    "\n",
    "# Check stability\n",
    "if score_psi_oot < 0.1:\n",
    "    print(\"  ‚úÖ Model is stable\")\n",
    "elif score_psi_oot < 0.25:\n",
    "    print(\"  ‚ö†Ô∏è Minor shift detected\")\n",
    "else:\n",
    "    print(\"  ‚ùå Significant shift detected\")\n",
    "\n",
    "# Top shifting features\n",
    "psi_df = pd.DataFrame(feature_psi).T\n",
    "unstable_features = psi_df[psi_df['oot'] > 0.25]\n",
    "if not unstable_features.empty:\n",
    "    print(f\"\\n‚ö†Ô∏è Unstable features (PSI > 0.25):\")\n",
    "    print(unstable_features.sort_values('oot', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration analysis\n",
    "calibration_analyzer = CalibrationAnalyzer()\n",
    "\n",
    "# Analyze calibration\n",
    "calibration_results = {\n",
    "    'test': calibration_analyzer.analyze_calibration(y_test, test_pred),\n",
    "    'oot': calibration_analyzer.analyze_calibration(oot_woe['target'], oot_pred)\n",
    "}\n",
    "\n",
    "print(\"Calibration Analysis:\")\n",
    "for dataset, results in calibration_results.items():\n",
    "    print(f\"\\n{dataset.upper()}:\")\n",
    "    print(f\"  ECE: {results['ece']:.4f}\")\n",
    "    print(f\"  MCE: {results['mce']:.4f}\")\n",
    "    print(f\"  Brier Score: {results['brier_score']:.4f}\")\n",
    "\n",
    "# Calibration plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, (dataset, results) in enumerate(calibration_results.items()):\n",
    "    ax = axes[idx]\n",
    "    bins = results['bins']\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "    ax.scatter(bins['mean_predicted'], bins['mean_actual'], s=100, alpha=0.7)\n",
    "    ax.plot(bins['mean_predicted'], bins['mean_actual'], 'b-', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Mean Predicted Probability')\n",
    "    ax.set_ylabel('Fraction of Positives')\n",
    "    ax.set_title(f'Calibration Plot - {dataset.upper()}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Apply calibration if needed\n",
    "if calibration_results['oot']['ece'] > 0.05:\n",
    "    print(\"\\nüìä Applying isotonic calibration...\")\n",
    "    test_pred_calibrated = calibration_analyzer.calibrate_predictions(\n",
    "        y_test, test_pred, oot_pred, method='isotonic'\n",
    "    )\n",
    "    print(f\"Calibrated OOT AUC: {roc_auc_score(oot_woe['target'], test_pred_calibrated):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Risk Band Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize risk bands\n",
    "risk_band_optimizer = RiskBandOptimizer()\n",
    "\n",
    "# Find optimal bands\n",
    "risk_bands = risk_band_optimizer.optimize_bands(\n",
    "    y_true=oot_woe['target'],\n",
    "    y_scores=oot_pred,\n",
    "    n_bands=5,\n",
    "    method='quantile'\n",
    ")\n",
    "\n",
    "print(\"Optimized Risk Bands:\")\n",
    "print(risk_bands[['band', 'min_score', 'max_score', 'bad_rate', 'volume_pct']].round(4))\n",
    "\n",
    "# Visualize risk bands\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Bad rate by band\n",
    "axes[0].bar(risk_bands['band'], risk_bands['bad_rate'], color='coral')\n",
    "axes[0].set_xlabel('Risk Band')\n",
    "axes[0].set_ylabel('Bad Rate')\n",
    "axes[0].set_title('Bad Rate by Risk Band')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume distribution\n",
    "axes[1].bar(risk_bands['band'], risk_bands['volume_pct'], color='skyblue')\n",
    "axes[1].set_xlabel('Risk Band')\n",
    "axes[1].set_ylabel('Volume %')\n",
    "axes[1].set_title('Volume Distribution by Risk Band')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate Gini from bands\n",
    "cumulative_bads = risk_bands['bad_rate'].cumsum() / risk_bands['bad_rate'].sum()\n",
    "cumulative_volume = risk_bands['volume_pct'].cumsum()\n",
    "gini_from_bands = 2 * np.trapz(cumulative_bads, cumulative_volume) - 1\n",
    "print(f\"\\nGini coefficient from bands: {gini_from_bands:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis for feature importance\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    # Create explainer\n",
    "    explainer = shap.Explainer(best_model, X_train)\n",
    "    \n",
    "    # Calculate SHAP values for test set\n",
    "    shap_values = explainer(X_test[:1000])  # Use subset for speed\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X_test[:1000], show=False)\n",
    "    plt.title('SHAP Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Get feature importance\n",
    "    shap_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': np.abs(shap_values.values).mean(axis=0)\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Features by SHAP:\")\n",
    "    print(shap_importance.head(10))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"SHAP analysis not available: {e}\")\n",
    "    print(\"Using permutation importance instead...\")\n",
    "    \n",
    "    from sklearn.inspection import permutation_importance\n",
    "    \n",
    "    perm_importance = permutation_importance(\n",
    "        best_model, X_test, y_test, n_repeats=10, random_state=42\n",
    "    )\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': perm_importance.importances_mean\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Features by Permutation Importance:\")\n",
    "    print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Complete Pipeline Run (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Run everything with CompletePipeline class\n",
    "print(\"Running Complete Pipeline Class...\\n\")\n",
    "\n",
    "# Initialize pipeline\n",
    "complete_pipeline = CompletePipeline(config)\n",
    "\n",
    "# Run pipeline\n",
    "results = complete_pipeline.run(\n",
    "    df=df,\n",
    "    test_df=None,  # Will be split automatically\n",
    "    oot_df=None    # Will be split automatically\n",
    ")\n",
    "\n",
    "print(\"\\nPipeline Results:\")\n",
    "print(f\"  Best Model: {results['best_model_name']}\")\n",
    "print(f\"  Best Score: {results['best_score']:.4f}\")\n",
    "print(f\"  Selected Features: {len(results['selected_features'])}\")\n",
    "print(f\"  Reports saved to: {config.output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Advanced pipeline with custom models\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Custom models\ncustom_models = {\n    'rf_custom': RandomForestClassifier(\n        n_estimators=200,\n        max_depth=10,\n        min_samples_split=50,\n        random_state=42\n    ),\n    'xgb_custom': XGBClassifier(\n        n_estimators=200,\n        max_depth=6,\n        learning_rate=0.01,\n        random_state=42\n    ),\n    'lgbm_custom': LGBMClassifier(\n        n_estimators=200,\n        max_depth=6,\n        learning_rate=0.01,\n        random_state=42,\n        verbose=-1\n    )\n}\n\n# Initialize advanced pipeline\nadvanced_pipeline = AdvancedRiskPipeline(config)\n\n# Set custom models\nadvanced_pipeline.model_builder.models.update(custom_models)\n\n# Run advanced pipeline\nadvanced_results = advanced_pipeline.run(\n    df=df,\n    external_validation_df=None\n)\n\nprint(\"\\nAdvanced Pipeline Results:\")\nprint(f\"  Best Model: {advanced_results['best_model_name']}\")\nprint(f\"  Best Score: {advanced_results['best_score']:.4f}\")\nprint(f\"  Model Comparison available: {advanced_results['model_comparison'] is not None}\")\nprint(f\"  Monitoring metrics calculated: {bool(advanced_results['monitoring_metrics'])}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced pipeline with custom models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Custom models\n",
    "custom_models = {\n",
    "    'rf_custom': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        min_samples_split=50,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'xgb_custom': XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.01,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'lgbm_custom': LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.01,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Initialize advanced pipeline\n",
    "advanced_pipeline = AdvancedPipeline(config)\n",
    "\n",
    "# Set custom models\n",
    "advanced_pipeline.model_builder.models.update(custom_models)\n",
    "\n",
    "# Run advanced pipeline\n",
    "advanced_results = advanced_pipeline.run(\n",
    "    df=df,\n",
    "    external_validation_df=None\n",
    ")\n",
    "\n",
    "print(\"\\nAdvanced Pipeline Results:\")\n",
    "print(f\"  Best Model: {advanced_results['best_model_name']}\")\n",
    "print(f\"  Best Score: {advanced_results['best_score']:.4f}\")\n",
    "print(f\"  Model Comparison available: {advanced_results['model_comparison'] is not None}\")\n",
    "print(f\"  Monitoring metrics calculated: {bool(advanced_results['monitoring_metrics'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Generate Comprehensive Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all reports\n",
    "reporter = Reporter(config)\n",
    "\n",
    "# Create comprehensive report\n",
    "report_data = {\n",
    "    'model_performance': metrics_df,\n",
    "    'feature_importance': shap_importance if 'shap_importance' in locals() else importance_df,\n",
    "    'risk_bands': risk_bands,\n",
    "    'psi_analysis': pd.DataFrame(feature_psi).T,\n",
    "    'calibration_metrics': pd.DataFrame(calibration_results).T,\n",
    "    'model_comparison': pd.DataFrame(model_builder.cv_scores_, index=['CV Score']).T\n",
    "}\n",
    "\n",
    "# Save Excel report\n",
    "report_path = reporter.save_excel_report(\n",
    "    report_data,\n",
    "    filename='end_to_end_pipeline_report.xlsx'\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Comprehensive report saved to: {report_path}\")\n",
    "\n",
    "# Generate model documentation\n",
    "model_docs = {\n",
    "    'Model Type': model_builder.best_model_name_,\n",
    "    'Training Date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'Training Samples': len(X_train),\n",
    "    'Features Used': len(selected_features),\n",
    "    'Cross-Validation Score': f\"{best_score:.4f}\",\n",
    "    'Test AUC': f\"{metrics_df.loc['Test', 'AUC']:.4f}\",\n",
    "    'OOT AUC': f\"{metrics_df.loc['OOT', 'AUC']:.4f}\",\n",
    "    'PSI (OOT)': f\"{score_psi_oot:.4f}\",\n",
    "    'ECE (OOT)': f\"{calibration_results['oot']['ece']:.4f}\",\n",
    "    'Number of Risk Bands': len(risk_bands)\n",
    "}\n",
    "\n",
    "print(\"\\nüìã Model Documentation:\")\n",
    "for key, value in model_docs.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Save Models and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'outputs/end_to_end_pipeline'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "model_path = os.path.join(output_dir, 'best_model.pkl')\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"‚úÖ Model saved to: {model_path}\")\n",
    "\n",
    "# Save transformers\n",
    "transformers = {\n",
    "    'feature_engineer': engineer,\n",
    "    'feature_selector': selector,\n",
    "    'woe_transformer': woe_transformer,\n",
    "    'imputer': imputer if 'imputer' in locals() else None\n",
    "}\n",
    "\n",
    "for name, transformer in transformers.items():\n",
    "    if transformer is not None:\n",
    "        transformer_path = os.path.join(output_dir, f'{name}.pkl')\n",
    "        joblib.dump(transformer, transformer_path)\n",
    "        print(f\"‚úÖ {name} saved to: {transformer_path}\")\n",
    "\n",
    "# Save configuration\n",
    "config_dict = {\n",
    "    'target_column': config.target_column,\n",
    "    'selected_features': selected_features,\n",
    "    'model_name': model_builder.best_model_name_,\n",
    "    'model_score': float(best_score),\n",
    "    'risk_bands': risk_bands.to_dict('records'),\n",
    "    'psi_thresholds': {'warning': 0.1, 'critical': 0.25},\n",
    "    'training_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "config_path = os.path.join(output_dir, 'pipeline_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "print(f\"‚úÖ Configuration saved to: {config_path}\")\n",
    "\n",
    "print(\"\\nüéâ End-to-End Pipeline Complete!\")\n",
    "print(f\"All artifacts saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Model Deployment Readiness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment readiness checklist\n",
    "print(\"üöÄ DEPLOYMENT READINESS CHECK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "checklist = {\n",
    "    'Model Performance': {\n",
    "        'Test AUC > 0.7': metrics_df.loc['Test', 'AUC'] > 0.7,\n",
    "        'OOT AUC > 0.7': metrics_df.loc['OOT', 'AUC'] > 0.7,\n",
    "        'Overfitting < 5%': overfit_score < 0.05\n",
    "    },\n",
    "    'Stability': {\n",
    "        'Score PSI < 0.25': score_psi_oot < 0.25,\n",
    "        'No unstable features': unstable_features.empty if 'unstable_features' in locals() else True\n",
    "    },\n",
    "    'Calibration': {\n",
    "        'ECE < 0.1': calibration_results['oot']['ece'] < 0.1,\n",
    "        'Brier Score < 0.25': calibration_results['oot']['brier_score'] < 0.25\n",
    "    },\n",
    "    'Documentation': {\n",
    "        'Model saved': os.path.exists(model_path),\n",
    "        'Config saved': os.path.exists(config_path),\n",
    "        'Report generated': os.path.exists(report_path) if 'report_path' in locals() else False\n",
    "    }\n",
    "}\n",
    "\n",
    "all_passed = True\n",
    "for category, checks in checklist.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for check, passed in checks.items():\n",
    "        status = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "        print(f\"  {status} {check}\")\n",
    "        if not passed:\n",
    "            all_passed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if all_passed:\n",
    "    print(\"‚úÖ MODEL IS READY FOR DEPLOYMENT\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some checks failed. Review before deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Quick Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_new_data(new_df, model_artifacts_dir='outputs/end_to_end_pipeline'):\n",
    "    \"\"\"\n",
    "    Score new data using saved pipeline artifacts\n",
    "    \"\"\"\n",
    "    # Load artifacts\n",
    "    model = joblib.load(os.path.join(model_artifacts_dir, 'best_model.pkl'))\n",
    "    feature_engineer = joblib.load(os.path.join(model_artifacts_dir, 'feature_engineer.pkl'))\n",
    "    feature_selector = joblib.load(os.path.join(model_artifacts_dir, 'feature_selector.pkl'))\n",
    "    woe_transformer = joblib.load(os.path.join(model_artifacts_dir, 'woe_transformer.pkl'))\n",
    "    \n",
    "    # Load config\n",
    "    with open(os.path.join(model_artifacts_dir, 'pipeline_config.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Process new data\n",
    "    df_eng = feature_engineer.transform(new_df)\n",
    "    df_selected = df_eng[config['selected_features']]\n",
    "    df_woe = woe_transformer.transform(df_selected)\n",
    "    \n",
    "    # Score\n",
    "    scores = model.predict_proba(df_woe)[:, 1]\n",
    "    \n",
    "    # Add risk bands\n",
    "    risk_bands = pd.DataFrame(config['risk_bands'])\n",
    "    \n",
    "    def assign_band(score):\n",
    "        for _, band in risk_bands.iterrows():\n",
    "            if band['min_score'] <= score <= band['max_score']:\n",
    "                return band['band']\n",
    "        return 'Unknown'\n",
    "    \n",
    "    # Create results\n",
    "    results = pd.DataFrame({\n",
    "        'score': scores,\n",
    "        'risk_band': [assign_band(s) for s in scores],\n",
    "        'risk_level': pd.cut(scores, bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                            labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test scoring function\n",
    "test_scores = score_new_data(test.head(100))\n",
    "print(\"Sample Scoring Results:\")\n",
    "print(test_scores.head(10))\n",
    "print(\"\\nRisk Distribution:\")\n",
    "print(test_scores['risk_level'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete end-to-end risk model pipeline including:\n",
    "\n",
    "‚úÖ **Data Processing**: Loading, validation, and preprocessing\n",
    "‚úÖ **Feature Engineering**: Creating polynomial and interaction features\n",
    "‚úÖ **Feature Selection**: Multiple selection methods\n",
    "‚úÖ **WOE Transformation**: Weight of Evidence encoding\n",
    "‚úÖ **Model Training**: Multiple algorithms with cross-validation\n",
    "‚úÖ **Evaluation**: Comprehensive metrics on train/test/OOT\n",
    "‚úÖ **PSI Monitoring**: Population stability tracking\n",
    "‚úÖ **Calibration**: Analysis and correction\n",
    "‚úÖ **Risk Bands**: Optimized segmentation\n",
    "‚úÖ **SHAP Analysis**: Feature importance and interpretability\n",
    "‚úÖ **Reporting**: Comprehensive Excel reports\n",
    "‚úÖ **Model Persistence**: Saving all artifacts for deployment\n",
    "\n",
    "The pipeline is now ready for production deployment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}