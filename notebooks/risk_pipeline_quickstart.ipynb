{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f465c8a",
   "metadata": {},
   "source": [
    "# Risk Pipeline Quickstart Notebook\n",
    "\n",
    "This notebook demonstrates how to run the **Unified Risk Pipeline** end-to-end on the bundled synthetic dataset. It is designed as a minimal yet comprehensive validation harness so you can exercise the entire modelling flow (splitting, WOE, feature selection, model training, risk banding and reporting) with just a few cells.\n",
    "\n",
    "> Tip: If you are running this notebook outside of the repository checkout, install the latest development branch first.\n",
    "> `\bash\n",
    "> pip install \"git+https://github.com/selimoksuz/risk-model-pipeline.git@development\"\n",
    "> `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230ff55",
   "metadata": {},
   "source": [
    "## 1. Imports and paths\n",
    "\n",
    "The quickstart dataset and data dictionary live under examples/data/quickstart. All outputs will be written to output/notebook_quickstart by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from risk_pipeline.core.config import Config\n",
    "from risk_pipeline.unified_pipeline import UnifiedRiskPipeline\n",
    "\n",
    "DATA_DIR = Path('examples/data/quickstart')\n",
    "INPUT_CSV = DATA_DIR / 'loan_applications.csv'\n",
    "DICTIONARY_CSV = DATA_DIR / 'data_dictionary.csv'\n",
    "OUTPUT_DIR = Path('output/notebook_quickstart')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "data_dictionary = pd.read_csv(DICTIONARY_CSV)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c45d2",
   "metadata": {},
   "source": [
    "## 2. Quick sanity checks\n",
    "\n",
    "Inspect the class balance and a few categorical distributions before running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40971e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts(normalize=True).rename('default_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee482f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['region', 'segment']].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0e01e0",
   "metadata": {},
   "source": [
    "## 3. Configure the pipeline\n",
    "\n",
    "The configuration below keeps the full modelling sequence but with light settings so the notebook finishes in a couple of minutes on a laptop. You can tweak any parameter if you want to stress-test specific stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    target_column='target',\n",
    "    id_column='app_id',\n",
    "    time_column='app_dt',\n",
    "    create_test_split=True,\n",
    "    test_size=0.25,\n",
    "    stratify_test=True,\n",
    "    oot_months=1,\n",
    "    enable_dual=False,\n",
    "    enable_tsfresh_features=False,\n",
    "    enable_scoring=True,\n",
    "    output_folder=str(OUTPUT_DIR),\n",
    "    enable_stage2_calibration=False,\n",
    "    n_risk_bands=5,\n",
    "    risk_band_method='quantile',\n",
    "    selection_steps=['psi', 'univariate', 'iv', 'correlation', 'stepwise'],\n",
    "    max_psi=0.6,\n",
    "    algorithms=['logistic'],\n",
    "    use_optuna=False,\n",
    "    calculate_shap=False,\n",
    "    use_noise_sentinel=False,\n",
    "    random_state=42,\n",
    ")\n",
    "cfg.model_type = 'logistic'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156916f",
   "metadata": {},
   "source": [
    "## 4. Run the unified pipeline\n",
    "\n",
    "The pipeline prints progress for each stage. Expect to see data processing, splitting, WOE, feature selection, modelling, calibration (Stage 1 only), risk band optimisation and scoring outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758cece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = UnifiedRiskPipeline(cfg)\n",
    "results = pipe.fit(df, data_dictionary=data_dictionary, score_df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f585a68",
   "metadata": {},
   "source": [
    "## 5. Inspect results\n",
    "\n",
    "The \n",
    "esults dictionary collects the artefacts most downstream processes need. Below are a few high-level summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550bea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = results.get('best_model_name')\n",
    "model_scores = results.get('model_results', {}).get('scores', {})\n",
    "print(f'Best model: {best_model}')\n",
    "pd.DataFrame(model_scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891dd883",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_report = pipe.reporter.reports_.get('features')\n",
    "feature_report.head() if feature_report is not None else 'No feature report available.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511feb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.reporter.reports_.get('risk_bands_summary', {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c0365",
   "metadata": {},
   "source": [
    "## 6. Generated files\n",
    "\n",
    "All artefacts are stored under the configured output directory. Use the list below for a quick peek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589dfe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(p.name for p in OUTPUT_DIR.glob('**/*') if p.is_file())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a724d701",
   "metadata": {},
   "source": [
    "## 7. Automating via script (optional)\n",
    "\n",
    "The repository also ships with examples/quickstart_demo.py. You can execute it from the console or from this notebook to validate the pipeline in CI environments.\n",
    "\n",
    "`python\n",
    "from examples.quickstart_demo import run_quickstart\n",
    "run_quickstart('output/quickstart_script')\n",
    "`\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
