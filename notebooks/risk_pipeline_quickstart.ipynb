{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "244c9779",
      "metadata": {},
      "source": [
        "# Credit Risk Pipeline Quickstart\n",
        "\n",
        "This notebook runs the **Unified Risk Pipeline** end-to-end on the bundled synthetic dataset.\n",
        "The sample includes stratified monthly observations, calibration hold-outs, stage-2 data, and a future scoring batch\n",
        "so each major step can be validated quickly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f55c9a1a",
      "metadata": {},
      "source": [
        "## 1. Environment & Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import importlib\n",
        "import importlib.metadata as metadata\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def _locate_project_root() -> Path:\n",
        "    cwd = Path.cwd().resolve()\n",
        "    if (cwd / 'src' / 'risk_pipeline').exists():\n",
        "        return cwd\n",
        "    candidate = cwd / 'risk-model-pipeline-dev'\n",
        "    if (candidate / 'src' / 'risk_pipeline').exists():\n",
        "        return candidate\n",
        "    for parent in cwd.parents:\n",
        "        maybe = parent / 'risk-model-pipeline-dev'\n",
        "        if (maybe / 'src' / 'risk_pipeline').exists():\n",
        "            return maybe\n",
        "    return cwd\n",
        "\n",
        "\n",
        "PROJECT_ROOT = _locate_project_root()\n",
        "SRC_PATH = PROJECT_ROOT / 'src'\n",
        "PACKAGE_PATH = SRC_PATH / 'risk_pipeline'\n",
        "MODULE_INIT = PACKAGE_PATH / '__init__.py'\n",
        "if SRC_PATH.exists() and str(SRC_PATH) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_PATH))\n",
        "\n",
        "TARGET_VERSION = '0.4.1'\n",
        "GIT_SPEC = 'risk-pipeline[ml,notebook] @ git+https://github.com/selimoksuz/risk-model-pipeline.git@development'\n",
        "PREREQ_PACKAGES = [\n",
        "    'numba==0.59.1',\n",
        "    'llvmlite==0.42.0',\n",
        "    'scipy==1.11.4',\n",
        "    'pandas==2.3.2',\n",
        "    'tsfresh==0.20.1',\n",
        "    'matrixprofile==1.1.10',\n",
        "    'shap==0.48.0',\n",
        "    'stumpy==1.13.0',\n",
        "]\n",
        "\n",
        "\n",
        "def _parse_version(value: str):\n",
        "    parts = []\n",
        "    for part in value.split('.'):\n",
        "        if not part.isdigit():\n",
        "            break\n",
        "        parts.append(int(part))\n",
        "    return tuple(parts)\n",
        "\n",
        "\n",
        "def _run_pip(args):\n",
        "    subprocess.check_call([\n",
        "        sys.executable,\n",
        "        '-m',\n",
        "        'pip',\n",
        "        'install',\n",
        "        '--no-cache-dir',\n",
        "        '--upgrade',\n",
        "        '--force-reinstall',\n",
        "        *args,\n",
        "    ])\n",
        "\n",
        "\n",
        "def _install_prerequisites():\n",
        "    print(f\"Installing prerequisite stack: {', '.join(PREREQ_PACKAGES)}\")\n",
        "    _run_pip(PREREQ_PACKAGES)\n",
        "\n",
        "\n",
        "def _sanity_check():\n",
        "    import shap  # noqa: F401\n",
        "    from llvmlite import binding as _ll_binding\n",
        "    _ = _ll_binding.ffi.lib\n",
        "    from numba import njit\n",
        "\n",
        "    @njit\n",
        "    def _probe(x):\n",
        "        return x + 1\n",
        "\n",
        "    assert _probe(1) == 2\n",
        "\n",
        "\n",
        "def _tsfresh_smoke_test():\n",
        "    import pandas as pd\n",
        "    from tsfresh import extract_features\n",
        "    from tsfresh.feature_extraction import EfficientFCParameters\n",
        "\n",
        "    data = pd.DataFrame(\n",
        "        {\n",
        "            'id': ['a', 'a', 'a', 'b', 'b', 'b'],\n",
        "            'time': [0, 1, 2, 0, 1, 2],\n",
        "            'value': [1.0, 2.0, 3.0, 4.0, 9.0, 16.0],\n",
        "        }\n",
        "    )\n",
        "    features = extract_features(\n",
        "        data,\n",
        "        column_id='id',\n",
        "        column_sort='time',\n",
        "        column_value='value',\n",
        "        default_fc_parameters=EfficientFCParameters(),\n",
        "        disable_progressbar=True,\n",
        "        n_jobs=0,\n",
        "    )\n",
        "    if not any('entropy' in col for col in features.columns):\n",
        "        raise RuntimeError('tsfresh smoke test did not produce entropy features')\n",
        "\n",
        "\n",
        "def _resolve_installed_version(module):\n",
        "    module_path = Path(getattr(module, '__file__', '')).resolve()\n",
        "    if SRC_PATH in module_path.parents:\n",
        "        return TARGET_VERSION\n",
        "    try:\n",
        "        return metadata.version('risk-pipeline')\n",
        "    except metadata.PackageNotFoundError:\n",
        "        return '0.0.0'\n",
        "\n",
        "\n",
        "def _load_local_package():\n",
        "    if not MODULE_INIT.exists():\n",
        "        return None\n",
        "    spec = importlib.util.spec_from_file_location('risk_pipeline', MODULE_INIT)\n",
        "    if spec and spec.loader:\n",
        "        module = importlib.util.module_from_spec(spec)\n",
        "        sys.modules['risk_pipeline'] = module\n",
        "        spec.loader.exec_module(module)\n",
        "        return module\n",
        "    return None\n",
        "\n",
        "\n",
        "def ensure_risk_pipeline():\n",
        "    print(f\"Resolved project root: {PROJECT_ROOT}\")\n",
        "    try:\n",
        "        module = _load_local_package()\n",
        "        if module is None:\n",
        "            module = importlib.import_module('risk_pipeline')\n",
        "        installed = _resolve_installed_version(module)\n",
        "        if _parse_version(installed) < _parse_version(TARGET_VERSION):\n",
        "            raise ModuleNotFoundError(f'risk-pipeline {installed} < {TARGET_VERSION}')\n",
        "        print(f'risk-pipeline {installed} available (path: {module.__file__}).')\n",
        "        _sanity_check()\n",
        "        _tsfresh_smoke_test()\n",
        "    except Exception as exc:\n",
        "        print(f'risk-pipeline import failed: {exc}')\n",
        "        try:\n",
        "            _install_prerequisites()\n",
        "            print(f'Attempting GitHub install: {GIT_SPEC}')\n",
        "            _run_pip([GIT_SPEC])\n",
        "            print('GitHub install succeeded.')\n",
        "            raise SystemExit('Installation complete. Restart the kernel and rerun this cell.')\n",
        "        except subprocess.CalledProcessError as err:\n",
        "            print(f'GitHub install failed: {err}')\n",
        "            raise SystemExit('Installation failed. Review the errors above.')\n",
        "    else:\n",
        "        print('Numba/llvmlite sanity check passed.')\n",
        "        print('tsfresh smoke test passed (entropy features available).')\n",
        "\n",
        "\n",
        "ensure_risk_pipeline()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "from risk_pipeline.core.config import Config\n",
        "from risk_pipeline.unified_pipeline import UnifiedRiskPipeline\n",
        "from risk_pipeline.data.sample import load_credit_risk_sample\n",
        "\n",
        "# ensure pipeline placeholders exist for diagnostic cells during step-by-step execution\n",
        "if 'pipe' not in globals():\n",
        "    pipe = None\n",
        "if 'results' not in globals():\n",
        "    results = {}\n",
        "if 'full_results' not in globals():\n",
        "    full_results = {}\n",
        "\n",
        "sample = load_credit_risk_sample()\n",
        "OUTPUT_DIR = Path('output/credit_risk_sample_notebook')\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "dev_df = sample.development.copy()\n",
        "cal_long_df = sample.calibration_longrun.copy()\n",
        "cal_recent_df = sample.calibration_recent.copy()\n",
        "score_df = sample.scoring_future.copy()\n",
        "data_dictionary = sample.data_dictionary.copy() if hasattr(sample.data_dictionary, 'copy') else sample.data_dictionary\n",
        "\n",
        "# ensure demo missingness as before\n",
        "_exclusion_cols = {'target', 'snapshot_month', 'customer_id', 'app_id', 'application_id', 'app_dt', 'decision_dt'}\n",
        "_rng = np.random.default_rng(42)\n",
        "\n",
        "def _inject_demo_missing(frame, rate=0.01, max_features=5):\n",
        "    if frame.isna().sum().sum() > 0:\n",
        "        return frame\n",
        "    numeric_candidates = [\n",
        "        col for col in frame.select_dtypes(include=['number']).columns\n",
        "        if col.lower() not in _exclusion_cols and not col.lower().endswith('_id')\n",
        "    ]\n",
        "    if not numeric_candidates:\n",
        "        return frame\n",
        "    for col in numeric_candidates[:max_features]:\n",
        "        mask = _rng.random(len(frame)) < rate\n",
        "        if mask.any():\n",
        "            frame.loc[mask, col] = np.nan\n",
        "    return frame\n",
        "\n",
        "for dataset in (dev_df, cal_long_df, cal_recent_df, score_df):\n",
        "    _inject_demo_missing(dataset)\n",
        "\n",
        "dev_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def _get_pipeline_context():\n",
        "    pipe_candidates = [\n",
        "        globals().get('pipe'),\n",
        "        globals().get('full_pipe'),\n",
        "        globals().get('raw_pipe'),\n",
        "    ]\n",
        "    pipe = next((p for p in pipe_candidates if p is not None), None)\n",
        "    results = globals().get('results')\n",
        "    if not isinstance(results, dict):\n",
        "        results = {}\n",
        "    if not results and pipe is not None:\n",
        "        results = getattr(pipe, 'results_', {}) or {}\n",
        "    if not results and isinstance(globals().get('full_results'), dict):\n",
        "        results = globals()['full_results']\n",
        "    return pipe, results if isinstance(results, dict) else {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure the Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import importlib\n",
        "\n",
        "import risk_pipeline.core.config as config_module\n",
        "import risk_pipeline.unified_pipeline as pipeline_module\n",
        "\n",
        "Config = importlib.reload(config_module).Config\n",
        "UnifiedRiskPipeline = importlib.reload(pipeline_module).UnifiedRiskPipeline\n",
        "\n",
        "cfg_params = {\n",
        "    # Core identifiers\n",
        "    'target_column': 'target',\n",
        "    'id_column': 'customer_id',\n",
        "    'time_column': 'app_dt',\n",
        "\n",
        "    # Split configuration\n",
        "    'create_test_split': True,\n",
        "    'stratify_test': True,\n",
        "    'train_ratio': 0.8,\n",
        "    'test_ratio': 0.2,\n",
        "    'oot_ratio': 0.0,\n",
        "    'oot_size': 0.0,\n",
        "    'oot_months': 3,\n",
        "\n",
        "    # TSFresh controls\n",
        "    'enable_tsfresh_features': True,\n",
        "    'tsfresh_feature_set': 'efficient',\n",
        "    'tsfresh_n_jobs': 4,\n",
        "\n",
        "    # Feature selection strategy\n",
        "    'selection_steps': [\n",
        "        'univariate',\n",
        "        'psi',\n",
        "        'vif',\n",
        "        'correlation',\n",
        "        'iv',\n",
        "        'boruta',\n",
        "        'stepwise',\n",
        "    ],\n",
        "    'min_univariate_gini': 0.05,\n",
        "    'psi_threshold': 0.25,\n",
        "    'monthly_psi_threshold': 0.15,\n",
        "    'oot_psi_threshold': 0.25,\n",
        "    'max_vif': 5.0,\n",
        "    'correlation_threshold': 0.9,\n",
        "    'iv_threshold': 0.02,\n",
        "    'stepwise_method': 'forward',\n",
        "    'stepwise_max_features': 25,\n",
        "\n",
        "    # Model training preferences\n",
        "    'algorithms': [\n",
        "        'logistic',\n",
        "        'lightgbm',\n",
        "        'xgboost',\n",
        "        'catboost',\n",
        "        'randomforest',\n",
        "        'extratrees',\n",
        "        'woe_boost',\n",
        "        'woe_li',\n",
        "        'shao',\n",
        "        'xbooster',\n",
        "    ],\n",
        "    'model_selection_method': 'gini_oot',\n",
        "    'model_stability_weight': 0.2,\n",
        "    'min_gini_threshold': 0.5,\n",
        "    'max_train_oot_gap': 0.03,\n",
        "    'use_optuna': True,\n",
        "    'hpo_trials': 75,\n",
        "    'hpo_timeout_sec': 1800,\n",
        "\n",
        "    # Diagnostics & toggles\n",
        "    'use_noise_sentinel': True,\n",
        "    'enable_dual': True,\n",
        "    'enable_woe_boost_scorecard': True,\n",
        "    'calculate_shap': True,\n",
        "    'enable_scoring': True,\n",
        "    'enable_stage2_calibration': True,\n",
        "\n",
        "    # Risk band settings\n",
        "    'n_risk_bands': 10,\n",
        "    'risk_band_method': 'pd_constraints',\n",
        "    'risk_band_min_bins': 7,\n",
        "    'risk_band_max_bins': 10,\n",
        "    'risk_band_hhi_threshold': 0.15,\n",
        "    'risk_band_binomial_pass_weight': 0.85,\n",
        "\n",
        "    # Runtime controls\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "}\n",
        "\n",
        "cfg_field_names = set(Config.__dataclass_fields__.keys())\n",
        "supported_params = {k: v for k, v in cfg_params.items() if k in cfg_field_names}\n",
        "unsupported = sorted(set(cfg_params.keys()) - set(supported_params.keys()))\n",
        "if unsupported:\n",
        "    print(f\"[WARN] Config ignores unsupported parameters: {unsupported}\")\n",
        "\n",
        "cfg = Config(**supported_params)\n",
        "pipe = UnifiedRiskPipeline(cfg)\n",
        "results = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. TSFresh Feature Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "if pipe is None:\n",
        "    raise RuntimeError('Pipeline instance is not initialized yet. Run the configuration cell first.')\n",
        "processed = pipe.run_process(dev_df, create_map=True, force=True)\n",
        "results_ref['processed_data'] = processed\n",
        "results = results_ref\n",
        "print(f\"Processed feature space: {processed.shape[1]} columns\")\n",
        "\n",
        "if pipe.data_.get('tsfresh_metadata') is not None and not pipe.data_['tsfresh_metadata'].empty:\n",
        "    display(pipe.data_['tsfresh_metadata'].head())\n",
        "else:\n",
        "    print('No TSFresh features were generated (configuration disabled).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Raw Numeric Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "if pipe is None:\n",
        "    raise RuntimeError('Pipeline instance is not initialized yet. Run the configuration cell first.')\n",
        "splits = pipe.run_split(processed, force=True)\n",
        "results_ref['splits'] = splits\n",
        "results = results_ref\n",
        "raw_layers = pipe.results_.get('raw_numeric_layers', {})\n",
        "print(f\"Identified numeric features: {len(pipe.data_.get('numeric_features', []))}\")\n",
        "if raw_layers:\n",
        "    train_raw = raw_layers.get('train_raw_prepped')\n",
        "    if train_raw is not None:\n",
        "        display(train_raw[pipe.data_.get('numeric_features', [])].head())\n",
        "else:\n",
        "    print('No numeric preprocessing layer was created.')\n",
        "\n",
        "impute_stats = getattr(pipe.data_processor, 'imputation_stats_', {})\n",
        "if impute_stats:\n",
        "    display(pd.DataFrame(impute_stats).T.head())\n",
        "\n",
        "# Summarise configuration choices for quick inspection\n",
        "config_summary = pd.DataFrame([\n",
        "    (\"Target column\", cfg.target_column),\n",
        "    (\"ID column\", cfg.id_column),\n",
        "    (\"Time column\", cfg.time_column),\n",
        "    (\"Train/Test/OOT split\", f\"{cfg.train_ratio:.0%}/{cfg.test_ratio:.0%}/{cfg.oot_ratio:.0%}\"),\n",
        "    (\"OOT holdout months\", cfg.oot_months),\n",
        "    (\"Risk bands\", f\"{cfg.n_risk_bands} (method={cfg.risk_band_method})\"),\n",
        "    (\"Calibration chain\", f\"{cfg.calibration_stage1_method} -> {cfg.calibration_stage2_method}\"),\n",
        "], columns=[\"Parameter\", \"Configured value\"])\n",
        "display(config_summary)\n",
        "\n",
        "flag_toggles = pd.DataFrame({\n",
        "    \"Feature\": [\n",
        "        \"Dual RAW+WOE flow\",\n",
        "        \"TSFresh feature mining\",\n",
        "        \"Scoring on hold-out data\",\n",
        "        \"Stage 2 calibration\",\n",
        "        \"Optuna HPO\",\n",
        "        \"Noise sentinel\",\n",
        "        \"SHAP importance\",\n",
        "    ],\n",
        "    \"Enabled\": [\n",
        "        getattr(cfg, 'enable_dual', False),\n",
        "        getattr(cfg, 'enable_tsfresh_features', False),\n",
        "        getattr(cfg, 'enable_scoring', False),\n",
        "        getattr(cfg, 'enable_stage2_calibration', False),\n",
        "        getattr(cfg, 'use_optuna', False),\n",
        "        getattr(cfg, 'use_noise_sentinel', False),\n",
        "        getattr(cfg, 'calculate_shap', False),\n",
        "    ],\n",
        "})\n",
        "flag_toggles['Enabled'] = flag_toggles['Enabled'].map({True: 'Yes', False: 'No'})\n",
        "display(flag_toggles)\n",
        "\n",
        "thresholds = pd.DataFrame({\n",
        "    \"Threshold\": [\n",
        "        \"PSI\",\n",
        "        \"IV\",\n",
        "        \"Univariate Gini\",\n",
        "        \"Correlation ceiling\",\n",
        "        \"VIF ceiling\",\n",
        "        \"|Train-OOT| Gini gap\",\n",
        "    ],\n",
        "    \"Value\": [\n",
        "        cfg.psi_threshold,\n",
        "        cfg.iv_threshold,\n",
        "        cfg.univariate_gini_threshold,\n",
        "        cfg.correlation_threshold,\n",
        "        cfg.vif_threshold,\n",
        "        cfg.max_train_oot_gap,\n",
        "    ],\n",
        "})\n",
        "display(thresholds)\n",
        "\n",
        "selection_order = pd.DataFrame({\"Selection step\": cfg.selection_steps})\n",
        "selection_order.index = selection_order.index + 1\n",
        "display(selection_order)\n",
        "\n",
        "algorithms_df = pd.DataFrame({\"Algorithm\": cfg.algorithms})\n",
        "algorithms_df.index = algorithms_df.index + 1\n",
        "display(algorithms_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. WOE Transformation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "if pipe is None:\n",
        "    print('WOE transformation skipped: pipeline instance not available yet.')\n",
        "else:\n",
        "    woe_results = pipe.run_woe(splits=splits, force=True)\n",
        "    results_ref['woe_results'] = woe_results\n",
        "    results = results_ref\n",
        "    woe_values = woe_results.get('woe_values', {})\n",
        "    print(f\"WOE computed for {len(woe_values)} features.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Diagnostic Summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "if pipe is None:\n",
        "    print('Feature selection skipped: pipeline instance not available yet.')\n",
        "else:\n",
        "    woe_results = results_ref.get('woe_results')\n",
        "    selection_mode = 'WOE' if getattr(pipe.config, 'enable_woe', True) else 'RAW'\n",
        "    selection_results = pipe.run_selection(\n",
        "        mode=selection_mode,\n",
        "        splits=splits,\n",
        "        woe_results=woe_results,\n",
        "        force=True,\n",
        "    )\n",
        "    results_ref['selection_results'] = selection_results\n",
        "    results_ref['selected_features'] = selection_results.get('selected_features', [])\n",
        "    results = results_ref\n",
        "    selected = results_ref.get('selected_features') or []\n",
        "    print(f\"Selected {len(selected)} features using {selection_mode} flow.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Raw vs prepped numeric diagnostics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "processed_df = getattr(pipe, 'data_', {}).get('processed') if pipe is not None else None\n",
        "if processed_df is None or processed_df.empty:\n",
        "    print(\"Processed dataset snapshot is not available.\")\n",
        "else:\n",
        "    numeric_cols = (\n",
        "        dev_df.select_dtypes(include=['number'])\n",
        "        .columns.difference([cfg.target_column])\n",
        "    )\n",
        "    diagnostics = []\n",
        "    for col in numeric_cols:\n",
        "        raw_series = dev_df[col]\n",
        "        proc_series = processed_df[col]\n",
        "        diagnostics.append({\n",
        "            'feature': col,\n",
        "            'raw_missing': int(raw_series.isna().sum()),\n",
        "            'processed_missing': int(proc_series.isna().sum()),\n",
        "            'raw_mean': float(raw_series.mean()),\n",
        "            'processed_mean': float(proc_series.mean()),\n",
        "        })\n",
        "    diag_df = pd.DataFrame(diagnostics)\n",
        "    if diag_df.empty:\n",
        "        print(\"No numeric columns found for diagnostics.\")\n",
        "    else:\n",
        "        diag_df['missing_delta'] = diag_df['raw_missing'] - diag_df['processed_missing']\n",
        "        diag_df['mean_shift'] = diag_df['processed_mean'] - diag_df['raw_mean']\n",
        "        display(diag_df.sort_values(['missing_delta', 'mean_shift'], ascending=[False, False]).head(12))\n",
        "        top_cols = diag_df.sort_values(['missing_delta', 'mean_shift'], ascending=[False, False])['feature'].head(4).tolist()\n",
        "        if top_cols:\n",
        "            comparison = pd.concat({'raw': dev_df[top_cols], 'prepped': processed_df[top_cols]}, axis=1)\n",
        "            display(comparison.head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 TSFresh feature contributions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "tsfresh_meta = (\n",
        "    results_ref.get('tsfresh_metadata')\n",
        "    or results_ref.get('selection_results', {}).get('tsfresh_metadata')\n",
        "    if isinstance(results_ref, dict) else None\n",
        ")\n",
        "if tsfresh_meta is None and pipe is not None:\n",
        "    reporter = getattr(pipe, 'reporter', None)\n",
        "    if reporter is not None:\n",
        "        tsfresh_meta = reporter.reports_.get('tsfresh_metadata')\n",
        "    if tsfresh_meta is None:\n",
        "        processor = getattr(pipe, 'data_processor', None)\n",
        "        tsfresh_meta = getattr(processor, 'tsfresh_metadata_', None) if processor is not None else None\n",
        "    if tsfresh_meta is None:\n",
        "        tsfresh_meta = getattr(getattr(pipe, 'data_', {}), 'get', lambda *_: None)('tsfresh_metadata')\n",
        "\n",
        "if tsfresh_meta is None:\n",
        "    print('No TSFresh metadata available ? run the TSFresh feature extraction cell first.')\n",
        "else:\n",
        "    tsfresh_meta = pd.DataFrame(tsfresh_meta).copy()\n",
        "    if 'source_variable' in tsfresh_meta.columns:\n",
        "        tsfresh_meta['source_variable'] = tsfresh_meta['source_variable'].astype(str)\n",
        "    else:\n",
        "        tsfresh_meta['source_variable'] = 'unknown'\n",
        "    if 'statistic' in tsfresh_meta.columns:\n",
        "        tsfresh_meta['statistic'] = tsfresh_meta['statistic'].fillna('unknown').astype(str)\n",
        "    else:\n",
        "        tsfresh_meta['statistic'] = 'unknown'\n",
        "    total_features = int(tsfresh_meta['feature'].nunique())\n",
        "    total_bases = int(tsfresh_meta['source_variable'].nunique())\n",
        "    print(f\"Generated {total_features} TSFresh features across {total_bases} base variables.\")\n",
        "    source_summary = (\n",
        "        tsfresh_meta.groupby('source_variable')\n",
        "        .size()\n",
        "        .rename('feature_count')\n",
        "        .sort_values(ascending=False)\n",
        "        .reset_index()\n",
        "    )\n",
        "    if not source_summary.empty:\n",
        "        source_summary['share'] = (\n",
        "            source_summary['feature_count'] / source_summary['feature_count'].sum()\n",
        "        ).round(4)\n",
        "        display(source_summary.head(15))\n",
        "    stat_mix = (\n",
        "        tsfresh_meta.groupby('statistic')\n",
        "        .size()\n",
        "        .rename('feature_count')\n",
        "        .sort_values(ascending=False)\n",
        "        .reset_index()\n",
        "    )\n",
        "    if not stat_mix.empty:\n",
        "        display(stat_mix)\n",
        "    display(tsfresh_meta.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 WOE transformation quality\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "woe_results = results_ref.get('woe_results') if isinstance(results_ref, dict) else {}\n",
        "if not woe_results:\n",
        "    print(\"WOE diagnostics skipped: run the WOE transformation cell first.\")\n",
        "else:\n",
        "    woe_values = woe_results.get('woe_values', {}) or {}\n",
        "    gini_map = woe_results.get('univariate_gini', {}) or {}\n",
        "    summary_rows = []\n",
        "    exclude_cols = set(getattr(getattr(pipe, 'config', None), 'exclude_woe_features', []) or []) if pipe is not None else set()\n",
        "    exclude_cols.add(getattr(pipe, 'noise_sentinel_name', 'noise_sentinel') if pipe is not None else 'noise_sentinel')\n",
        "    for feature, info in woe_values.items():\n",
        "        if not isinstance(info, dict) or feature in exclude_cols:\n",
        "            continue\n",
        "        stats = info.get('stats') or []\n",
        "        feature_type = info.get('type', 'unknown')\n",
        "        if feature_type == 'numeric':\n",
        "            bin_count = len(info.get('bins', [])) - 1 if info.get('bins') is not None else len(stats)\n",
        "        else:\n",
        "            categories = info.get('categories')\n",
        "            bin_count = len(categories) if categories else len(stats)\n",
        "        gini_info = gini_map.get(feature, {}) or {}\n",
        "        summary_rows.append({\n",
        "            'feature': feature,\n",
        "            'type': feature_type,\n",
        "            'iv': info.get('iv'),\n",
        "            'bin_count': bin_count,\n",
        "            'gini_raw': gini_info.get('gini_raw'),\n",
        "            'gini_woe': gini_info.get('gini_woe'),\n",
        "        })\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "    if summary_df.empty:\n",
        "        print('No WOE summary rows to display.')\n",
        "    else:\n",
        "        summary_df['gini_uplift'] = summary_df['gini_woe'].fillna(0) - summary_df['gini_raw'].fillna(0)\n",
        "        display(summary_df.sort_values('iv', ascending=False).head(15))\n",
        "        display(summary_df.sort_values('gini_uplift').head(10))\n",
        "        top_feature = summary_df.sort_values('iv', ascending=False)['feature'].iloc[0]\n",
        "        top_stats = woe_values.get(top_feature, {}).get('stats')\n",
        "        if top_stats:\n",
        "            display(pd.DataFrame(top_stats).head(15))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.4 Feature selection progression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "selection_results = results_ref.get('selection_results') if isinstance(results_ref, dict) else {}\n",
        "if not selection_results:\n",
        "    print(\"Selection history is empty.\")\n",
        "else:\n",
        "    selection_history = pipe.reporter.reports_.get('selection_history') if pipe is not None else None\n",
        "    if selection_history is None or selection_history.empty:\n",
        "        print(\"Selection history is empty.\")\n",
        "    else:\n",
        "        selection_history = selection_history.copy()\n",
        "        if 'details' in selection_history.columns:\n",
        "            details_series = selection_history['details']\n",
        "            selection_history = selection_history.drop(columns='details')\n",
        "        else:\n",
        "            details_series = pd.Series([{} for _ in range(len(selection_history))])\n",
        "        expanded = details_series.apply(lambda val: val if isinstance(val, dict) else {}).apply(pd.Series)\n",
        "        history_df = pd.concat([selection_history, expanded], axis=1)\n",
        "        display(history_df)\n",
        "selected_features = results_ref.get('selected_features') or []\n",
        "if selected_features:\n",
        "    feature_list = pd.DataFrame({'rank': range(1, len(selected_features) + 1), 'feature': selected_features})\n",
        "    display(feature_list.head(40))\n",
        "    print(f\"Total selected features: {len(selected_features)}\")\n",
        "dropped = selection_results.get('dropped_features') if isinstance(selection_results, dict) else None\n",
        "if dropped:\n",
        "    dropped_df = pd.DataFrame(list(dropped.items()), columns=['feature', 'reason']) if isinstance(dropped, dict) else pd.DataFrame(dropped)\n",
        "    if not dropped_df.empty:\n",
        "        display(dropped_df.head(40))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.5 Model performance comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "model_results = results_ref.get('model_results', {}) if isinstance(results_ref, dict) else {}\n",
        "model_scores = model_results.get('scores', {}) or {}\n",
        "if not model_scores:\n",
        "    print(\"Model scores not available.\")\n",
        "else:\n",
        "    scores_df = pd.DataFrame(model_scores).T\n",
        "    metric_priority = [col for col in ['oot_auc', 'test_auc', 'train_auc'] if col in scores_df.columns]\n",
        "    metric_cols = [col for col in ['train_auc', 'test_auc', 'oot_auc', 'train_gini', 'test_gini', 'oot_gini', 'train_oot_gap'] if col in scores_df.columns]\n",
        "    display(scores_df[metric_cols].sort_values(by=metric_priority, ascending=False))\n",
        "    if 'train_auc' in scores_df.columns and any(col in scores_df.columns for col in ['oot_auc', 'test_auc']):\n",
        "        ref_cols = [col for col in ['oot_auc', 'test_auc'] if col in scores_df.columns]\n",
        "        scores_df['overfit_gap'] = scores_df['train_auc'] - scores_df[ref_cols].max(axis=1)\n",
        "        display(scores_df['overfit_gap'].sort_values())\n",
        "best_model = model_results.get('best_model_name') or results_ref.get('best_model_name')\n",
        "if best_model and model_scores:\n",
        "    print(f\"Best model: {best_model}\")\n",
        "    if best_model in model_scores:\n",
        "        display(pd.Series(model_scores[best_model]).dropna())\n",
        "feature_importance = model_results.get('feature_importance', {})\n",
        "best_importance = feature_importance.get(best_model)\n",
        "if isinstance(best_importance, (dict, pd.DataFrame)):\n",
        "    fi_df = pd.DataFrame(best_importance)\n",
        "    if not fi_df.empty:\n",
        "        display(fi_df.head(25))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.6 Calibration metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "stage1 = results_ref.get('calibration_stage1') or {}\n",
        "stage2 = results_ref.get('calibration_stage2') or {}\n",
        "stage1_metrics = stage1.get('calibration_metrics') if isinstance(stage1, dict) else None\n",
        "if stage1_metrics:\n",
        "    stage1_df = pd.DataFrame(stage1_metrics, index=['Stage 1']).T\n",
        "    display(stage1_df)\n",
        "    if stage1.get('stage1_details'):\n",
        "        display(pd.DataFrame(stage1['stage1_details'], index=['Stage 1']).T)\n",
        "stage2_metrics = stage2.get('stage2_metrics') if isinstance(stage2, dict) else None\n",
        "if stage2_metrics:\n",
        "    stage2_df = pd.DataFrame(stage2_metrics, index=['Stage 2']).T\n",
        "    display(stage2_df)\n",
        "stage2_details = stage2.get('stage2_details') if isinstance(stage2, dict) else None\n",
        "if stage2_details:\n",
        "    display(pd.DataFrame(stage2_details, index=['Stage 2']).T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.7 Risk band optimisation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "risk_band_results = results_ref.get('risk_bands') if isinstance(results_ref, dict) else {}\n",
        "if not risk_band_results:\n",
        "    print(\"Risk band optimizer did not return results.\")\n",
        "else:\n",
        "    band_table = risk_band_results.get('bands')\n",
        "    if isinstance(band_table, pd.DataFrame) and band_table.empty:\n",
        "        band_table = None\n",
        "    if band_table is None:\n",
        "        band_table = risk_band_results.get('band_table')\n",
        "    if isinstance(band_table, dict):\n",
        "        band_table = pd.DataFrame.from_dict(band_table)\n",
        "    if isinstance(band_table, pd.DataFrame) and not band_table.empty:\n",
        "        display(band_table)\n",
        "    metrics = risk_band_results.get('metrics') or {}\n",
        "    if metrics:\n",
        "        metrics_df = pd.DataFrame(metrics, index=['metrics']) if not isinstance(metrics, pd.DataFrame) else metrics\n",
        "        display(metrics_df.T if 'metrics' in metrics_df.index else metrics_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.8 Recent scoring diagnostics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "scoring_output = results_ref.get('scoring_output') if isinstance(results_ref, dict) else {}\n",
        "scoring_metrics = scoring_output.get('metrics') or results_ref.get('scoring_metrics') or {}\n",
        "if scoring_metrics:\n",
        "    flat_metrics = {\n",
        "        key: value\n",
        "        for key, value in scoring_metrics.items()\n",
        "        if not isinstance(value, (dict, list))\n",
        "    }\n",
        "    if flat_metrics:\n",
        "        summary_df = pd.DataFrame(flat_metrics, index=['value']).T\n",
        "        display(summary_df)\n",
        "    if 'with_target' in scoring_metrics and scoring_metrics['with_target']:\n",
        "        display(pd.DataFrame(scoring_metrics['with_target'], index=['with_target']).T)\n",
        "    if 'without_target' in scoring_metrics and scoring_metrics['without_target']:\n",
        "        display(pd.DataFrame(scoring_metrics['without_target'], index=['without_target']).T)\n",
        "reports = scoring_output.get('reports') or results_ref.get('scoring_reports') or {}\n",
        "for name, report in reports.items():\n",
        "    if isinstance(report, pd.DataFrame) and not report.empty:\n",
        "        print(f\"Report: {name}\")\n",
        "        display(report)\n",
        "scored_df = scoring_output.get('dataframe') or results_ref.get('scoring_results')\n",
        "if isinstance(scored_df, pd.DataFrame) and not scored_df.empty:\n",
        "    scored_view = scored_df.copy()\n",
        "    if 'risk_score' not in scored_view.columns:\n",
        "        scored_view['risk_score'] = np.nan\n",
        "    band_summary = scored_view.groupby('risk_band').agg(\n",
        "        records=('risk_band', 'size'),\n",
        "        avg_score=('risk_score', 'mean'),\n",
        "        min_score=('risk_score', 'min'),\n",
        "        max_score=('risk_score', 'max')\n",
        "    )\n",
        "    if 'target' in scored_view.columns:\n",
        "        band_summary['bads'] = scored_view.groupby('risk_band')['target'].sum()\n",
        "        band_summary['bad_rate'] = scored_view.groupby('risk_band')['target'].mean()\n",
        "    display(band_summary)\n",
        "    display(scored_view.head(10))\n",
        "else:\n",
        "    print(\"Scoring dataframe is not available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Stage 1 & Stage 2 Calibration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "if pipe is None:\n",
        "    print('Calibration skipped: pipeline instance not available yet.')\n",
        "else:\n",
        "    stage1 = pipe.run_stage1_calibration(model_results=pipe.results_['model_results'], calibration_df=cal_long_df, force=True)\n",
        "    stage2 = pipe.run_stage2_calibration(stage1_results=stage1, recent_df=cal_recent_df, force=True)\n",
        "    pipe.results_['calibration_stage1'] = stage1\n",
        "    pipe.results_['calibration_stage2'] = stage2\n",
        "    results_ref['calibration_stage1'] = stage1\n",
        "    results_ref['calibration_stage2'] = stage2\n",
        "    results = results_ref\n",
        "    print('Stage 1 and Stage 2 calibration completed.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Risk Band Optimisation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "if pipe is None:\n",
        "    print('Risk band optimisation skipped: pipeline instance not available yet.')\n",
        "else:\n",
        "    bands = pipe.run_risk_bands(stage2_results=pipe.results_.get('calibration_stage2'), splits=pipe.results_.get('splits'), force=True)\n",
        "    pipe.results_['risk_bands'] = bands\n",
        "    results_ref['risk_bands'] = bands\n",
        "    results = results_ref\n",
        "    if isinstance(bands, dict):\n",
        "        metrics = bands.get('metrics')\n",
        "        if isinstance(metrics, dict):\n",
        "            display(pd.DataFrame(metrics, index=['value']).T)\n",
        "        else:\n",
        "            print('Risk band metrics not available.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Consolidated Pipeline Run\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "full_pipe = UnifiedRiskPipeline(cfg)\n",
        "full_results = full_pipe.fit(\n",
        "    dev_df,\n",
        "    data_dictionary=data_dictionary,\n",
        "    calibration_df=cal_long_df,\n",
        "    stage2_df=cal_recent_df,\n",
        "    score_df=score_df,\n",
        ")\n",
        "results = full_results\n",
        "print(f\"Best mode: {full_results.get('best_model_mode')} | Best model: {full_results.get('best_model_name')}\")\n",
        "print('Model registry (top rows):')\n",
        "model_registry = pd.DataFrame(full_results.get('model_registry', []))\n",
        "if not model_registry.empty:\n",
        "    display(model_registry.sort_values(['mode', 'oot_auc'], ascending=[True, False]).head())\n",
        "else:\n",
        "    print('Model registry is empty.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Recent Raw Data Scoring\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe, results_ref = _get_pipeline_context()\n",
        "if pipe is None:\n",
        "    print('Scoring skipped: pipeline instance not available yet.')\n",
        "else:\n",
        "    scoring_output = pipe.run_scoring(score_df, force=True)\n",
        "    results_ref['scoring_output'] = scoring_output\n",
        "    results = results_ref\n",
        "    scored_df = scoring_output.get('dataframe')\n",
        "    if scored_df is not None:\n",
        "        display(scored_df.head())\n",
        "    metrics = scoring_output.get('metrics')\n",
        "    if metrics:\n",
        "        print('Scoring metrics:')\n",
        "        display(pd.DataFrame(metrics, index=[0]).T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84c253f",
      "metadata": {},
      "source": [
        "For automation examples, see examples/quickstart_demo.py."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}