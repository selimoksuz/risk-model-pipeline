{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244c9779",
   "metadata": {},
   "source": [
    "# Credit Risk Pipeline Quickstart\n",
    "\n",
    "This notebook runs the **Unified Risk Pipeline** end-to-end on the bundled synthetic dataset.\n",
    "The sample includes stratified monthly observations, calibration hold-outs, stage-2 data, and a future scoring batch\n",
    "so each major step can be validated quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c9a1a",
   "metadata": {},
   "source": [
    "## 1. Environment & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d95991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import importlib.metadata as metadata\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _locate_project_root() -> Path:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    if (cwd / 'src' / 'risk_pipeline').exists():\n",
    "        return cwd\n",
    "    candidate = cwd / 'risk-model-pipeline-dev'\n",
    "    if (candidate / 'src' / 'risk_pipeline').exists():\n",
    "        return candidate\n",
    "    for parent in cwd.parents:\n",
    "        maybe = parent / 'risk-model-pipeline-dev'\n",
    "        if (maybe / 'src' / 'risk_pipeline').exists():\n",
    "            return maybe\n",
    "    return cwd\n",
    "\n",
    "\n",
    "PROJECT_ROOT = _locate_project_root()\n",
    "SRC_PATH = PROJECT_ROOT / 'src'\n",
    "PACKAGE_PATH = SRC_PATH / 'risk_pipeline'\n",
    "MODULE_INIT = PACKAGE_PATH / '__init__.py'\n",
    "if SRC_PATH.exists() and str(SRC_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_PATH))\n",
    "\n",
    "TARGET_VERSION = '0.4.1'\n",
    "GIT_SPEC = 'risk-pipeline[ml,notebook] @ git+https://github.com/selimoksuz/risk-model-pipeline.git@development'\n",
    "PREREQ_PACKAGES = [\n",
    "    'numba==0.59.1',\n",
    "    'llvmlite==0.42.0',\n",
    "    'scipy==1.11.4',\n",
    "    'pandas==2.3.2',\n",
    "    'tsfresh==0.20.1',\n",
    "    'matrixprofile==1.1.10',\n",
    "    'shap==0.48.0',\n",
    "    'stumpy==1.13.0',\n",
    "]\n",
    "\n",
    "\n",
    "def _parse_version(value: str):\n",
    "    parts = []\n",
    "    for part in value.split('.'):\n",
    "        if not part.isdigit():\n",
    "            break\n",
    "        parts.append(int(part))\n",
    "    return tuple(parts)\n",
    "\n",
    "\n",
    "def _run_pip(args):\n",
    "    subprocess.check_call([\n",
    "        sys.executable,\n",
    "        '-m',\n",
    "        'pip',\n",
    "        'install',\n",
    "        '--no-cache-dir',\n",
    "        '--upgrade',\n",
    "        '--force-reinstall',\n",
    "        *args,\n",
    "    ])\n",
    "\n",
    "\n",
    "def _install_prerequisites():\n",
    "    print(f\"Installing prerequisite stack: {', '.join(PREREQ_PACKAGES)}\")\n",
    "    _run_pip(PREREQ_PACKAGES)\n",
    "\n",
    "\n",
    "def _sanity_check():\n",
    "    import shap  # noqa: F401\n",
    "    from llvmlite import binding as _ll_binding\n",
    "    _ = _ll_binding.ffi.lib\n",
    "    from numba import njit\n",
    "\n",
    "    @njit\n",
    "    def _probe(x):\n",
    "        return x + 1\n",
    "\n",
    "    assert _probe(1) == 2\n",
    "\n",
    "\n",
    "def _tsfresh_smoke_test():\n",
    "    import pandas as pd\n",
    "    from tsfresh import extract_features\n",
    "    from tsfresh.feature_extraction import EfficientFCParameters\n",
    "\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            'id': ['a', 'a', 'a', 'b', 'b', 'b'],\n",
    "            'time': [0, 1, 2, 0, 1, 2],\n",
    "            'value': [1.0, 2.0, 3.0, 4.0, 9.0, 16.0],\n",
    "        }\n",
    "    )\n",
    "    features = extract_features(\n",
    "        data,\n",
    "        column_id='id',\n",
    "        column_sort='time',\n",
    "        column_value='value',\n",
    "        default_fc_parameters=EfficientFCParameters(),\n",
    "        disable_progressbar=True,\n",
    "        n_jobs=0,\n",
    "    )\n",
    "    if not any('entropy' in col for col in features.columns):\n",
    "        raise RuntimeError('tsfresh smoke test did not produce entropy features')\n",
    "\n",
    "\n",
    "def _resolve_installed_version(module):\n",
    "    module_path = Path(getattr(module, '__file__', '')).resolve()\n",
    "    if SRC_PATH in module_path.parents:\n",
    "        return TARGET_VERSION\n",
    "    try:\n",
    "        return metadata.version('risk-pipeline')\n",
    "    except metadata.PackageNotFoundError:\n",
    "        return '0.0.0'\n",
    "\n",
    "\n",
    "def _load_local_package():\n",
    "    if not MODULE_INIT.exists():\n",
    "        return None\n",
    "    spec = importlib.util.spec_from_file_location('risk_pipeline', MODULE_INIT)\n",
    "    if spec and spec.loader:\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        sys.modules['risk_pipeline'] = module\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "    return None\n",
    "\n",
    "\n",
    "def ensure_risk_pipeline():\n",
    "    print(f\"Resolved project root: {PROJECT_ROOT}\")\n",
    "    try:\n",
    "        module = _load_local_package()\n",
    "        if module is None:\n",
    "            module = importlib.import_module('risk_pipeline')\n",
    "        installed = _resolve_installed_version(module)\n",
    "        if _parse_version(installed) < _parse_version(TARGET_VERSION):\n",
    "            raise ModuleNotFoundError(f'risk-pipeline {installed} < {TARGET_VERSION}')\n",
    "        print(f'risk-pipeline {installed} available (path: {module.__file__}).')\n",
    "        _sanity_check()\n",
    "        _tsfresh_smoke_test()\n",
    "    except Exception as exc:\n",
    "        print(f'risk-pipeline import failed: {exc}')\n",
    "        try:\n",
    "            _install_prerequisites()\n",
    "            print(f'Attempting GitHub install: {GIT_SPEC}')\n",
    "            _run_pip([GIT_SPEC])\n",
    "            print('GitHub install succeeded.')\n",
    "            raise SystemExit('Installation complete. Restart the kernel and rerun this cell.')\n",
    "        except subprocess.CalledProcessError as err:\n",
    "            print(f'GitHub install failed: {err}')\n",
    "            raise SystemExit('Installation failed. Review the errors above.')\n",
    "    else:\n",
    "        print('Numba/llvmlite sanity check passed.')\n",
    "        print('tsfresh smoke test passed (entropy features available).')\n",
    "\n",
    "\n",
    "ensure_risk_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a762023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from risk_pipeline.data.sample import load_credit_risk_sample\n",
    "\n",
    "sample = load_credit_risk_sample()\n",
    "OUTPUT_DIR = Path('output/credit_risk_sample_notebook')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dev_df = sample.development.copy()\n",
    "cal_long_df = sample.calibration_longrun.copy()\n",
    "cal_recent_df = sample.calibration_recent.copy()\n",
    "score_df = sample.scoring_future.copy()\n",
    "data_dictionary = sample.data_dictionary.copy()\n",
    "\n",
    "print(f\"Development dataset: {dev_df.shape[0]:,} rows, {dev_df.shape[1]} columns\")\n",
    "print(f\"Stage 1 calibration dataset: {cal_long_df.shape[0]:,} rows\")\n",
    "print(f\"Stage 2 calibration dataset: {cal_recent_df.shape[0]:,} rows\")\n",
    "print(f\"Scoring dataset: {score_df.shape[0]:,} rows\")\n",
    "\n",
    "display(dev_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e810cc0",
   "metadata": {},
   "source": [
    "## 2. Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10426cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import risk_pipeline.core.config as config_module\n",
    "import risk_pipeline.unified_pipeline as pipeline_module\n",
    "\n",
    "Config = importlib.reload(config_module).Config\n",
    "UnifiedRiskPipeline = importlib.reload(pipeline_module).UnifiedRiskPipeline\n",
    "\n",
    "cfg_params = {\n",
    "    # Core identifiers\n",
    "    'target_column': 'target',\n",
    "    'id_column': 'customer_id',\n",
    "    'time_column': 'app_dt',\n",
    "\n",
    "    # Split configuration\n",
    "    'create_test_split': True,\n",
    "    'stratify_test': True,\n",
    "    'train_ratio': 0.8,\n",
    "    'test_ratio': 0.2,\n",
    "    'oot_ratio': 0.0,\n",
    "    'oot_size': 0.0,\n",
    "    'oot_months': 3,\n",
    "\n",
    "    # TSFresh controls\n",
    "    'enable_tsfresh_features': True,\n",
    "    'tsfresh_feature_set': 'efficient',\n",
    "    'tsfresh_n_jobs': 4,\n",
    "\n",
    "    # Feature selection strategy\n",
    "    'selection_steps': [\n",
    "        'univariate',\n",
    "        'psi',\n",
    "        'vif',\n",
    "        'correlation',\n",
    "        'iv',\n",
    "        'boruta',\n",
    "        'stepwise',\n",
    "    ],\n",
    "    'min_univariate_gini': 0.05,\n",
    "    'psi_threshold': 0.25,\n",
    "    'monthly_psi_threshold': 0.15,\n",
    "    'oot_psi_threshold': 0.25,\n",
    "    'max_vif': 5.0,\n",
    "    'correlation_threshold': 0.9,\n",
    "    'iv_threshold': 0.02,\n",
    "    'stepwise_method': 'forward',\n",
    "    'stepwise_max_features': 25,\n",
    "\n",
    "    # Model training preferences\n",
    "    'algorithms': [\n",
    "        'logistic',\n",
    "        'lightgbm',\n",
    "        'xgboost',\n",
    "        'catboost',\n",
    "        'randomforest',\n",
    "        'extratrees',\n",
    "        'woe_boost',\n",
    "        'woe_li',\n",
    "        'shao',\n",
    "        'xbooster',\n",
    "    ],\n",
    "    'model_selection_method': 'gini_oot',\n",
    "    'model_stability_weight': 0.2,\n",
    "    'min_gini_threshold': 0.5,\n",
    "    'max_train_oot_gap': 0.03,\n",
    "    'use_optuna': True,\n",
    "    'hpo_trials': 75,\n",
    "    'hpo_timeout_sec': 1800,\n",
    "\n",
    "    # Diagnostics & toggles\n",
    "    'use_noise_sentinel': True,\n",
    "    'enable_dual': True,\n",
    "    'enable_woe_boost_scorecard': True,\n",
    "    'calculate_shap': True,\n",
    "    'enable_scoring': True,\n",
    "    'enable_stage2_calibration': True,\n",
    "\n",
    "    # Risk band settings\n",
    "    'n_risk_bands': 10,\n",
    "    'risk_band_method': 'pd_constraints',\n",
    "    'risk_band_min_bins': 7,\n",
    "    'risk_band_max_bins': 10,\n",
    "    'risk_band_hhi_threshold': 0.15,\n",
    "    'risk_band_binomial_pass_weight': 0.85,\n",
    "\n",
    "    # Runtime controls\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "cfg_field_names = set(Config.__dataclass_fields__.keys())\n",
    "supported_params = {k: v for k, v in cfg_params.items() if k in cfg_field_names}\n",
    "unsupported = sorted(set(cfg_params.keys()) - set(supported_params.keys()))\n",
    "if unsupported:\n",
    "    print(f\"[WARN] Config ignores unsupported parameters: {unsupported}\")\n",
    "\n",
    "cfg = Config(**supported_params)\n",
    "pipe = UnifiedRiskPipeline(cfg)\n",
    "\n",
    "config_summary = pd.DataFrame([\n",
    "    ('train_ratio', cfg.train_ratio),\n",
    "    ('test_ratio', cfg.test_ratio),\n",
    "    ('oot_ratio', getattr(cfg, 'oot_ratio', None)),\n",
    "    ('oot_months', getattr(cfg, 'oot_months', None)),\n",
    "    ('dual_pipeline', getattr(cfg, 'enable_dual', False)),\n",
    "    ('enable_scoring', cfg.enable_scoring),\n",
    "    ('stage2_calibration', cfg.enable_stage2_calibration),\n",
    "    ('tsfresh_enabled', cfg.enable_tsfresh_features),\n",
    "    ('noise_sentinel', cfg.use_noise_sentinel),\n",
    "    ('model_selection', cfg.model_selection_method),\n",
    "    ('vif_threshold', getattr(cfg, 'max_vif', None)),\n",
    "], columns=['parameter', 'value'])\n",
    "\n",
    "print('Configured pipeline parameters:')\n",
    "display(config_summary)\n",
    "print('\n",
    "Algorithms ({}): {}'.format(len(cfg.algorithms), ', '.join(cfg.algorithms)))\n",
    "print('Feature selection order: {}'.format(' -> '.join(cfg.selection_steps)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1555b8",
   "metadata": {},
   "source": [
    "## 3. TSFresh Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbab735",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = pipe.run_process(dev_df, create_map=True, force=True)\n",
    "print(f\"Processed feature space: {processed.shape[1]} columns\")\n",
    "\n",
    "original_columns = set(dev_df.columns)\n",
    "tsfresh_columns = sorted([col for col in processed.columns if col not in original_columns])\n",
    "print(f\"Original features: {len(original_columns)} | TSFresh-derived: {len(tsfresh_columns)}\")\n",
    "if tsfresh_columns:\n",
    "    display(pd.DataFrame({'tsfresh_feature': tsfresh_columns}).head(10))\n",
    "else:\n",
    "    print('No TSFresh features were generated (configuration disabled).')\n",
    "\n",
    "metadata = pipe.data_.get('tsfresh_metadata')\n",
    "if metadata is not None and not metadata.empty:\n",
    "    display(metadata.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cad32",
   "metadata": {},
   "source": [
    "## 4. Raw Numeric Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1fea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = pipe.run_split(processed, force=True)\n",
    "raw_layers = pipe.results_.get('raw_numeric_layers', {})\n",
    "numeric_features = pipe.data_.get('numeric_features', [])\n",
    "print(f\"Identified numeric features: {len(numeric_features)}\")\n",
    "\n",
    "train_raw = splits.get('train')\n",
    "train_prepped = raw_layers.get('train_raw_prepped')\n",
    "if train_raw is not None and train_prepped is not None and numeric_features:\n",
    "    raw_preview = train_raw[numeric_features]\n",
    "    prepped_preview = train_prepped[numeric_features]\n",
    "\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'raw_missing': raw_preview.isna().sum(),\n",
    "        'prepped_missing': prepped_preview.isna().sum()\n",
    "    })\n",
    "    display(missing_summary.head(10))\n",
    "\n",
    "    range_summary = pd.DataFrame({\n",
    "        'raw_min': raw_preview.min(),\n",
    "        'prepped_min': prepped_preview.min(),\n",
    "        'raw_max': raw_preview.max(),\n",
    "        'prepped_max': prepped_preview.max()\n",
    "    })\n",
    "    display(range_summary.head(10))\n",
    "\n",
    "    print('Raw vs prepped preview:')\n",
    "    comparison = pd.concat([\n",
    "        raw_preview.head().assign(_view='raw'),\n",
    "        prepped_preview.head().assign(_view='prepped')\n",
    "    ])\n",
    "    display(comparison.set_index('_view', append=True).swaplevel(0,1).head(10))\n",
    "else:\n",
    "    print('Numeric preprocessing layer not available.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6e1e4",
   "metadata": {},
   "source": [
    "## 5. WOE Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc3bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "woe_results = pipe.run_woe(splits, force=True)\n",
    "woe_values = woe_results.get('woe_values', {})\n",
    "print(f\"WOE maps generated for {len(woe_values)} variables\")\n",
    "if woe_values:\n",
    "    summary_rows = []\n",
    "    for name, info in woe_values.items():\n",
    "        entry = {\n",
    "            'feature': name,\n",
    "            'type': info.get('type'),\n",
    "            'iv': info.get('iv')\n",
    "        }\n",
    "        stats = info.get('stats') or []\n",
    "        if stats:\n",
    "            woe_vals = [s.get('woe') for s in stats if isinstance(s, dict) and s.get('woe') is not None]\n",
    "            entry['woe_min'] = min(woe_vals) if woe_vals else None\n",
    "            entry['woe_max'] = max(woe_vals) if woe_vals else None\n",
    "            entry['bins'] = len(stats)\n",
    "        summary_rows.append(entry)\n",
    "    woe_summary = pd.DataFrame(summary_rows)\n",
    "    display(woe_summary.sort_values('iv', ascending=False).head(10))\n",
    "else:\n",
    "    print('WOE transformation returned no mappings.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc5aa0",
   "metadata": {},
   "source": [
    "## 6. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_raw = pipe.run_selection(mode='RAW', splits=splits, woe_results=woe_results, force=True)\n",
    "selection_woe = pipe.run_selection(mode='WOE', splits=splits, woe_results=woe_results, force=True)\n",
    "\n",
    "selection_summary = pd.DataFrame([\n",
    "    {'mode': 'RAW', 'n_features': len(selection_raw.get('selected_features', []))},\n",
    "    {'mode': 'WOE', 'n_features': len(selection_woe.get('selected_features', []))},\n",
    "])\n",
    "print('Selected feature counts by mode:')\n",
    "display(selection_summary)\n",
    "\n",
    "for mode_label, payload in [('RAW', selection_raw), ('WOE', selection_woe)]:\n",
    "    history = pd.DataFrame(payload.get('selection_history', []))\n",
    "    if history.empty:\n",
    "        continue\n",
    "    print(f\"\n",
    "{mode_label} selection history:\")\n",
    "    display(history[['method', 'before', 'after', 'removed']])\n",
    "\n",
    "    detail_records = []\n",
    "    for record in payload.get('selection_history', []):\n",
    "        detail = record.get('details')\n",
    "        if isinstance(detail, dict):\n",
    "            detail_records.append({'method': record.get('method'), 'details': detail})\n",
    "    if detail_records:\n",
    "        detail_df = pd.json_normalize(detail_records, 'details', ['method'], errors='ignore')\n",
    "        if not detail_df.empty:\n",
    "            display(detail_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0af1f05",
   "metadata": {},
   "source": [
    "## 7. Modeling (RAW vs WOE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _best_auc(model_results):\n",
    "    scores = model_results.get('scores', {}) or {}\n",
    "    preferred = model_results.get('best_model_name')\n",
    "\n",
    "    def _score(metrics):\n",
    "        if not metrics:\n",
    "            return float('-inf')\n",
    "        for key in ('oot_auc', 'test_auc', 'train_auc'):\n",
    "            value = metrics.get(key)\n",
    "            if value is not None:\n",
    "                return value\n",
    "        return float('-inf')\n",
    "\n",
    "    if preferred and preferred in scores:\n",
    "        return _score(scores[preferred])\n",
    "    if scores:\n",
    "        return max((_score(metrics) for metrics in scores.values()), default=float('-inf'))\n",
    "    return float('-inf')\n",
    "\n",
    "models_raw = pipe.run_modeling(mode='RAW', splits=splits, selection_results=selection_raw, force=True)\n",
    "models_woe = pipe.run_modeling(mode='WOE', splits=splits, selection_results=selection_woe, force=True)\n",
    "\n",
    "score_frames = []\n",
    "for mode_label, payload in [('RAW', models_raw), ('WOE', models_woe)]:\n",
    "    scores = payload.get('scores', {})\n",
    "    if scores:\n",
    "        frame = pd.DataFrame(scores).T\n",
    "        frame['mode'] = mode_label\n",
    "        score_frames.append(frame)\n",
    "\n",
    "if score_frames:\n",
    "    combined = pd.concat(score_frames).reset_index().rename(columns={'index': 'model'})\n",
    "    display(combined.sort_values(['mode', 'oot_auc'], ascending=[True, False]))\n",
    "else:\n",
    "    print('No models were trained.')\n",
    "\n",
    "flows = {\n",
    "    'RAW': {'selection_results': selection_raw, 'model_results': models_raw, 'best_auc': _best_auc(models_raw)},\n",
    "    'WOE': {'selection_results': selection_woe, 'model_results': models_woe, 'best_auc': _best_auc(models_woe)},\n",
    "}\n",
    "\n",
    "best_mode = max(flows.items(), key=lambda item: item[1]['best_auc'])[0]\n",
    "best_flow = flows[best_mode]\n",
    "\n",
    "pipe.results_['flows'] = flows\n",
    "pipe.results_['best_mode'] = best_mode\n",
    "pipe.results_['selection_results'] = best_flow['selection_results']\n",
    "pipe.results_['model_results'] = best_flow['model_results']\n",
    "pipe.config.enable_woe = (best_mode == 'WOE')\n",
    "pipe.models_ = best_flow['model_results'].get('models', {})\n",
    "pipe.selected_features_ = best_flow['selection_results'].get('selected_features', [])\n",
    "\n",
    "print(f\"Best mode: {best_mode} | Best model: {best_flow['model_results'].get('best_model_name')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28851631",
   "metadata": {},
   "source": [
    "## 8. Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a607124",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1 = pipe.run_stage1_calibration(model_results=pipe.results_['model_results'], calibration_df=cal_long_df, force=True)\n",
    "stage2 = pipe.run_stage2_calibration(stage1_results=stage1, recent_df=cal_recent_df, force=True)\n",
    "\n",
    "if isinstance(stage1, dict):\n",
    "    metrics1 = stage1.get('calibration_metrics')\n",
    "    if metrics1:\n",
    "        print('Stage 1 metrics:')\n",
    "        display(pd.DataFrame(metrics1, index=['value']).T)\n",
    "    curve1 = stage1.get('calibration_curve')\n",
    "    if hasattr(curve1, 'head'):\n",
    "        print('Stage 1 calibration curve preview:')\n",
    "        display(curve1.head())\n",
    "\n",
    "if isinstance(stage2, dict):\n",
    "    metrics2 = stage2.get('stage2_metrics')\n",
    "    if metrics2:\n",
    "        print('Stage 2 metrics:')\n",
    "        display(pd.DataFrame(metrics2, index=['value']).T)\n",
    "    details2 = stage2.get('stage2_details')\n",
    "    if details2:\n",
    "        print('Stage 2 details:')\n",
    "        display(pd.DataFrame(details2, index=['value']).T)\n",
    "\n",
    "pipe.results_['calibration_stage1'] = stage1\n",
    "pipe.results_['calibration_stage2'] = stage2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff055f",
   "metadata": {},
   "source": [
    "## 9. Risk Band Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e6d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = pipe.run_risk_bands(stage2_results=stage2, splits=pipe.results_['splits'], force=True)\n",
    "pipe.results_['risk_bands'] = bands\n",
    "\n",
    "if isinstance(bands, dict):\n",
    "    metrics = bands.get('metrics')\n",
    "    if isinstance(metrics, dict) and metrics:\n",
    "        print('Risk band metrics:')\n",
    "        display(pd.DataFrame(metrics, index=['value']).T)\n",
    "    band_frame = bands.get('bands') or bands.get('band_stats')\n",
    "    if isinstance(band_frame, pd.DataFrame) and not band_frame.empty:\n",
    "        print('Risk band table preview:')\n",
    "        display(band_frame.head(10))\n",
    "else:\n",
    "    print('Risk band optimisation returned no results.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347a97c",
   "metadata": {},
   "source": [
    "## 10. Consolidated Pipeline Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7605948",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipe = UnifiedRiskPipeline(cfg)\n",
    "full_results = full_pipe.fit(\n",
    "    dev_df,\n",
    "    data_dictionary=data_dictionary,\n",
    "    calibration_df=cal_long_df,\n",
    "    stage2_df=cal_recent_df,\n",
    "    score_df=score_df,\n",
    ")\n",
    "\n",
    "summary_items = {\n",
    "    'best_mode': full_results.get('best_model_mode'),\n",
    "    'best_model': full_results.get('best_model_name'),\n",
    "    'best_auc': full_results.get('chosen_auc'),\n",
    "    'stage2_adjustment': (full_results.get('calibration_stage2') or {}).get('adjustment_factor'),\n",
    "    'risk_band_source': (full_results.get('risk_bands') or {}).get('source'),\n",
    "}\n",
    "print('Consolidated run summary:')\n",
    "display(pd.Series(summary_items, name='value'))\n",
    "\n",
    "model_registry = full_results.get('model_registry')\n",
    "if isinstance(model_registry, dict) and model_registry:\n",
    "    registry_rows = []\n",
    "    for mode_label, models in model_registry.items():\n",
    "        frame = pd.DataFrame(models).T\n",
    "        frame['mode'] = mode_label\n",
    "        registry_rows.append(frame)\n",
    "    if registry_rows:\n",
    "        combined_registry = pd.concat(registry_rows).reset_index().rename(columns={'index': 'model'})\n",
    "        display(combined_registry.sort_values(['mode', 'oot_auc'], ascending=[True, False]).head())\n",
    "else:\n",
    "    print('Model registry is empty.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f6e6e",
   "metadata": {},
   "source": [
    "## 11. Recent Raw Data Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe3d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_output = pipe.run_scoring(score_df, force=True)\n",
    "scored_df = scoring_output.get('dataframe')\n",
    "if scored_df is not None:\n",
    "    print('Scored dataframe preview:')\n",
    "    display(scored_df.head())\n",
    "\n",
    "metrics = scoring_output.get('metrics')\n",
    "if metrics:\n",
    "    flat_metrics = {k: v for k, v in metrics.items() if not isinstance(v, (dict, list))}\n",
    "    print('Overall scoring metrics:')\n",
    "    display(pd.Series(flat_metrics, name='value'))\n",
    "\n",
    "    with_target = metrics.get('with_target')\n",
    "    if with_target:\n",
    "        print('Metrics (records with target):')\n",
    "        display(pd.Series(with_target, name='value'))\n",
    "\n",
    "    without_target = metrics.get('without_target')\n",
    "    if without_target:\n",
    "        print('Metrics (records without target):')\n",
    "        display(pd.Series(without_target, name='value'))\n",
    "\n",
    "    noise_diag = metrics.get('noise_sentinel')\n",
    "    if noise_diag:\n",
    "        print('Noise sentinel diagnostics:')\n",
    "        display(pd.Series(noise_diag, name='value'))\n",
    "else:\n",
    "    print('No scoring metrics were generated.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c253f",
   "metadata": {},
   "source": [
    "For automation examples, see examples/quickstart_demo.py."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
