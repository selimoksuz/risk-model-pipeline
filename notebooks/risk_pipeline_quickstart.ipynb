{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244c9779",
   "metadata": {},
   "source": [
    "# Credit Risk Pipeline Quickstart\n",
    "\n",
    "This notebook runs the **Unified Risk Pipeline** end-to-end on the bundled synthetic dataset.\n",
    "The sample includes stratified monthly observations, calibration hold-outs, stage-2 data, and a future scoring batch\n",
    "so each major step can be validated quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c9a1a",
   "metadata": {},
   "source": [
    "## 1. Environment & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f1bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import importlib.util\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _locate_project_root() -> Path:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    if (cwd / 'src' / 'risk_pipeline').exists():\n",
    "        return cwd\n",
    "    candidate = cwd / 'risk-model-pipeline-dev'\n",
    "    if (candidate / 'src' / 'risk_pipeline').exists():\n",
    "        return candidate\n",
    "    for parent in cwd.parents:\n",
    "        maybe = parent / 'risk-model-pipeline-dev'\n",
    "        if (maybe / 'src' / 'risk_pipeline').exists():\n",
    "            return maybe\n",
    "    return cwd\n",
    "\n",
    "\n",
    "PROJECT_ROOT = _locate_project_root()\n",
    "SRC_PATH = PROJECT_ROOT / 'src'\n",
    "PACKAGE_PATH = SRC_PATH / 'risk_pipeline'\n",
    "MODULE_INIT = PACKAGE_PATH / '__init__.py'\n",
    "if SRC_PATH.exists() and str(SRC_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_PATH))\n",
    "\n",
    "\n",
    "def _load_local_package():\n",
    "    if not MODULE_INIT.exists():\n",
    "        return None\n",
    "    spec = importlib.util.spec_from_file_location('risk_pipeline', MODULE_INIT)\n",
    "    if spec and spec.loader:\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        sys.modules['risk_pipeline'] = module\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "    return None\n",
    "\n",
    "\n",
    "def ensure_risk_pipeline():\n",
    "    module = _load_local_package()\n",
    "    if module is None:\n",
    "        module = importlib.import_module('risk_pipeline')\n",
    "    version = getattr(module, '__version__', 'local-dev')\n",
    "    location = Path(getattr(module, '__file__', 'unknown')).resolve()\n",
    "    print(f'risk-pipeline loaded (version {version}, path={location})')\n",
    "    return module\n",
    "\n",
    "\n",
    "TSFRESH_AVAILABLE = importlib.util.find_spec('tsfresh') is not None\n",
    "if TSFRESH_AVAILABLE:\n",
    "    print('tsfresh available (advanced time-series features can be enabled via config).')\n",
    "else:\n",
    "    print('tsfresh is not installed; pipeline will fall back to lightweight aggregate features when needed.')\n",
    "\n",
    "risk_pipeline_module = ensure_risk_pipeline()\n",
    "NOTEBOOK_FLAGS = globals().setdefault('_NOTEBOOK_FLAGS', {})\n",
    "NOTEBOOK_FLAGS['tsfresh_available'] = TSFRESH_AVAILABLE\n",
    "NOTEBOOK_FLAGS['project_root'] = PROJECT_ROOT\n",
    "NOTEBOOK_FLAGS['src_path'] = SRC_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "from risk_pipeline.core.config import Config\n",
    "from risk_pipeline.unified_pipeline import UnifiedRiskPipeline\n",
    "from risk_pipeline.data.sample import load_credit_risk_sample\n",
    "\n",
    "NOTEBOOK_CONTEXT = globals().setdefault('_NOTEBOOK_CONTEXT', {'data': {}, 'artifacts': {}, 'paths': {}, 'options': {}})\n",
    "\n",
    "# ensure pipeline placeholders exist for diagnostic cells during step-by-step execution\n",
    "if 'pipe' not in globals():\n",
    "    pipe = None\n",
    "if 'results' not in globals():\n",
    "    results = {}\n",
    "if 'full_results' not in globals():\n",
    "    full_results = {}\n",
    "\n",
    "sample = load_credit_risk_sample()\n",
    "OUTPUT_DIR = Path('output/credit_risk_sample_notebook')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dev_df = sample.development.copy()\n",
    "cal_long_df = sample.calibration_longrun.copy()\n",
    "cal_recent_df = sample.calibration_recent.copy()\n",
    "score_df = sample.scoring_future.copy()\n",
    "data_dictionary = sample.data_dictionary.copy() if hasattr(sample.data_dictionary, 'copy') else sample.data_dictionary\n",
    "\n",
    "NOTEBOOK_CONTEXT['data'].update({\n",
    "    'development': dev_df,\n",
    "    'calibration_longrun': cal_long_df,\n",
    "    'calibration_recent': cal_recent_df,\n",
    "    'scoring': score_df,\n",
    "    'dictionary': data_dictionary,\n",
    "})\n",
    "NOTEBOOK_CONTEXT['paths']['output'] = OUTPUT_DIR\n",
    "\n",
    "# ensure demo missingness as before\n",
    "_exclusion_cols = {'target', 'snapshot_month', 'customer_id', 'app_id', 'application_id', 'app_dt', 'decision_dt'}\n",
    "_rng = np.random.default_rng(42)\n",
    "\n",
    "def _inject_demo_missing(frame, rate=0.01, max_features=5):\n",
    "    if frame.isna().sum().sum() > 0:\n",
    "        return frame\n",
    "    numeric_candidates = [\n",
    "        col for col in frame.select_dtypes(include=['number']).columns\n",
    "        if col.lower() not in _exclusion_cols and not col.lower().endswith('_id')\n",
    "    ]\n",
    "    if not numeric_candidates:\n",
    "        return frame\n",
    "    for col in numeric_candidates[:max_features]:\n",
    "        mask = _rng.random(len(frame)) < rate\n",
    "        if mask.any():\n",
    "            frame.loc[mask, col] = np.nan\n",
    "    return frame\n",
    "\n",
    "for dataset in (dev_df, cal_long_df, cal_recent_df, score_df):\n",
    "    _inject_demo_missing(dataset)\n",
    "\n",
    "dev_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87230fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "from risk_pipeline.core.config import Config\n",
    "from risk_pipeline.unified_pipeline import UnifiedRiskPipeline\n",
    "from risk_pipeline.data.sample import load_credit_risk_sample\n",
    "\n",
    "NOTEBOOK_CONTEXT = globals().setdefault('_NOTEBOOK_CONTEXT', {'data': {}, 'artifacts': {}, 'paths': {}, 'options': {}})\n",
    "\n",
    "def _get_pipeline_context():\n",
    "    pipe_candidates = [\n",
    "        globals().get('pipe'),\n",
    "        globals().get('full_pipe'),\n",
    "        globals().get('raw_pipe'),\n",
    "    ]\n",
    "    pipe = next((p for p in pipe_candidates if p is not None), None)\n",
    "    results = globals().get('results')\n",
    "    if not isinstance(results, dict):\n",
    "        results = {}\n",
    "    if not results and pipe is not None:\n",
    "        results = getattr(pipe, 'results_', {}) or {}\n",
    "    if not results and isinstance(globals().get('full_results'), dict):\n",
    "        results = globals()['full_results']\n",
    "    return pipe, results if isinstance(results, dict) else {}\n",
    "\n",
    "def _store_artifact(name, value):\n",
    "    context = globals().get('_NOTEBOOK_CONTEXT') or {}\n",
    "    context.setdefault('artifacts', {})[name] = value\n",
    "    return value\n",
    "\n",
    "def _update_results(results_ref, **artifacts):\n",
    "    if not isinstance(results_ref, dict):\n",
    "        return results_ref\n",
    "    results_ref.update(artifacts)\n",
    "    globals()['results'] = results_ref\n",
    "    return results_ref\n",
    "\n",
    "def _ensure_dev_df():\n",
    "    context = globals().get('_NOTEBOOK_CONTEXT') or {}\n",
    "    data = context.get('data', {})\n",
    "    df = data.get('development')\n",
    "    if df is None:\n",
    "        df = globals().get('dev_df')\n",
    "    return df\n",
    "\n",
    "def _ensure_processed(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('processed_data')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('processed_data')\n",
    "        if cached is not None:\n",
    "            _store_artifact('processed_data', cached)\n",
    "            return cached\n",
    "    source_df = _ensure_dev_df()\n",
    "    if source_df is None:\n",
    "        raise RuntimeError('Development dataframe is not loaded yet. Run the data preparation cell first.')\n",
    "    processed = pipe.run_process(source_df, create_map=True, include_noise=False, force=bool(force))\n",
    "    _update_results(results_ref, processed_data=processed)\n",
    "    _store_artifact('processed_data', processed)\n",
    "    return processed\n",
    "\n",
    "def _ensure_splits(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('splits')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('splits')\n",
    "        if cached is not None:\n",
    "            _store_artifact('splits', cached)\n",
    "            return cached\n",
    "    processed = _ensure_processed(force=False)\n",
    "    splits = pipe.run_split(processed, force=True)\n",
    "    _update_results(results_ref, splits=splits)\n",
    "    _store_artifact('splits', splits)\n",
    "    return splits\n",
    "\n",
    "def _ensure_woe(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('woe_results')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('woe_results')\n",
    "        if cached is not None:\n",
    "            _store_artifact('woe_results', cached)\n",
    "            return cached\n",
    "    splits = _ensure_splits(force=False)\n",
    "    if splits is None:\n",
    "        raise RuntimeError('Splits are unavailable; run the split cell first.')\n",
    "    woe_results = pipe.run_woe(splits=splits, force=True)\n",
    "    _update_results(results_ref, woe_results=woe_results)\n",
    "    _store_artifact('woe_results', woe_results)\n",
    "    return woe_results\n",
    "\n",
    "def _ensure_selection(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('selection_results')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('selection_results')\n",
    "        if cached:\n",
    "            _store_artifact('selection_results', cached)\n",
    "            return cached\n",
    "    splits = _ensure_splits(force=False)\n",
    "    woe_results = _ensure_woe(force=False)\n",
    "    selection_mode = 'WOE' if getattr(pipe.config, 'enable_woe', True) else 'RAW'\n",
    "    selection_results = pipe.run_selection(\n",
    "        mode=selection_mode,\n",
    "        splits=splits,\n",
    "        woe_results=woe_results,\n",
    "        force=True,\n",
    "    )\n",
    "    selected = selection_results.get('selected_features', [])\n",
    "    _update_results(results_ref, selection_results=selection_results, selected_features=selected)\n",
    "    _store_artifact('selection_results', selection_results)\n",
    "    return selection_results\n",
    "\n",
    "def _ensure_model_results(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('model_results')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('model_results')\n",
    "        if cached:\n",
    "            _store_artifact('model_results', cached)\n",
    "            return cached\n",
    "    selection_results = _ensure_selection(force=False)\n",
    "    splits = _ensure_splits(force=False)\n",
    "    selection_mode = selection_results.get('mode') if isinstance(selection_results, dict) else 'WOE'\n",
    "    selection_mode = selection_mode or ('WOE' if getattr(pipe.config, 'enable_woe', True) else 'RAW')\n",
    "    model_results = pipe.run_modeling(\n",
    "        mode=selection_mode,\n",
    "        splits=splits,\n",
    "        selection_results=selection_results,\n",
    "        force=True,\n",
    "    )\n",
    "    _update_results(results_ref, model_results=model_results)\n",
    "    _store_artifact('model_results', model_results)\n",
    "    return model_results\n",
    "\n",
    "def _ensure_stage1(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('calibration_stage1')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('calibration_stage1')\n",
    "        if cached:\n",
    "            _store_artifact('calibration_stage1', cached)\n",
    "            return cached\n",
    "    model_results = _ensure_model_results(force=False)\n",
    "    calibration_df = NOTEBOOK_CONTEXT.get('data', {}).get('calibration_longrun')\n",
    "    if calibration_df is None:\n",
    "        calibration_df = NOTEBOOK_CONTEXT.get('data', {}).get('development')\n",
    "    stage1 = pipe.run_stage1_calibration(model_results=model_results, calibration_df=calibration_df, force=True)\n",
    "    _update_results(results_ref, calibration_stage1=stage1)\n",
    "    _store_artifact('calibration_stage1', stage1)\n",
    "    return stage1\n",
    "\n",
    "def _ensure_stage2(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('calibration_stage2')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('calibration_stage2')\n",
    "        if cached:\n",
    "            _store_artifact('calibration_stage2', cached)\n",
    "            return cached\n",
    "    stage1 = _ensure_stage1(force=False)\n",
    "    recent_df = NOTEBOOK_CONTEXT.get('data', {}).get('calibration_recent')\n",
    "    stage2 = pipe.run_stage2_calibration(stage1_results=stage1, recent_df=recent_df, force=True)\n",
    "    _update_results(results_ref, calibration_stage2=stage2)\n",
    "    _store_artifact('calibration_stage2', stage2)\n",
    "    return stage2\n",
    "\n",
    "def _ensure_risk_bands(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('risk_bands')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('risk_bands')\n",
    "        if cached:\n",
    "            _store_artifact('risk_bands', cached)\n",
    "            return cached\n",
    "    stage2 = _ensure_stage2(force=False)\n",
    "    splits = _ensure_splits(force=False)\n",
    "    bands = pipe.run_risk_bands(stage2_results=stage2, splits=splits, force=True)\n",
    "    _update_results(results_ref, risk_bands=bands)\n",
    "    _store_artifact('risk_bands', bands)\n",
    "    return bands\n",
    "\n",
    "def _ensure_scoring(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('scoring_output')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('scoring_output')\n",
    "        if cached:\n",
    "            _store_artifact('scoring_output', cached)\n",
    "            return cached\n",
    "    score_df = NOTEBOOK_CONTEXT.get('data', {}).get('scoring') or globals().get('score_df')\n",
    "    if score_df is None:\n",
    "        raise RuntimeError('Scoring dataset is not loaded.')\n",
    "    stage2 = _ensure_stage2(force=False)\n",
    "    selection = _ensure_selection(force=False)\n",
    "    woe_results = _ensure_woe(force=False)\n",
    "    model_results = _ensure_model_results(force=False)\n",
    "    splits = _ensure_splits(force=False)\n",
    "    scoring_output = pipe.run_scoring(\n",
    "        score_df,\n",
    "        stage2_results=stage2,\n",
    "        selection_results=selection,\n",
    "        woe_results=woe_results,\n",
    "        model_results=model_results,\n",
    "        splits=splits,\n",
    "        force=True,\n",
    "    )\n",
    "    _update_results(results_ref, scoring_output=scoring_output)\n",
    "    _store_artifact('scoring_output', scoring_output)\n",
    "    return scoring_output\n",
    "\n",
    "def _ensure_reports(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('reports')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('reports')\n",
    "        if cached:\n",
    "            _store_artifact('reports', cached)\n",
    "            return cached\n",
    "    reports = pipe.run_reporting(force=True)\n",
    "    _update_results(results_ref, reports=reports)\n",
    "    _store_artifact('reports', reports)\n",
    "    return reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5f4f0",
   "metadata": {},
   "source": [
    "## 2. Configure the Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20771cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "\n",
    "import risk_pipeline.core.feature_selector_enhanced as fs_module\n",
    "import risk_pipeline.core.config as config_module\n",
    "import risk_pipeline.unified_pipeline as pipeline_module\n",
    "\n",
    "AdvancedFeatureSelector = importlib.reload(fs_module).AdvancedFeatureSelector\n",
    "Config = importlib.reload(config_module).Config\n",
    "UnifiedRiskPipeline = importlib.reload(pipeline_module).UnifiedRiskPipeline\n",
    "\n",
    "cfg_params = {\n",
    "    # Core identifiers\n",
    "    'target_column': 'target',\n",
    "    'id_column': 'customer_id',\n",
    "    'time_column': 'app_dt',\n",
    "\n",
    "    # Split configuration\n",
    "    'create_test_split': True,\n",
    "    'stratify_test': True,\n",
    "    'train_ratio': 0.8,\n",
    "    'test_ratio': 0.2,\n",
    "    'oot_ratio': 0.0,\n",
    "    'oot_months': 3,\n",
    "\n",
    "    # Output controls\n",
    "    'output_folder': str(NOTEBOOK_CONTEXT['paths']['output']),\n",
    "    'output_excel_path': str(NOTEBOOK_CONTEXT['paths']['output'] / 'risk_pipeline_report.xlsx'),\n",
    "\n",
    "    # TSFresh controls (auto-disabled if package missing)\n",
    "    'enable_tsfresh_features': bool(NOTEBOOK_FLAGS.get('tsfresh_available', False)),\n",
    "    'tsfresh_feature_set': 'efficient',\n",
    "    'tsfresh_n_jobs': 4,\n",
    "\n",
    "    # Feature selection strategy\n",
    "    'selection_steps': [\n",
    "        'univariate',\n",
    "        'psi',\n",
    "        'vif',\n",
    "        'correlation',\n",
    "        'iv',\n",
    "        'boruta',\n",
    "        'stepwise',\n",
    "    ],\n",
    "    'min_univariate_gini': 0.05,\n",
    "    'psi_threshold': 0.25,\n",
    "    'monthly_psi_threshold': 0.15,\n",
    "    'oot_psi_threshold': 0.25,\n",
    "    'vif_threshold': 5.0,\n",
    "    'correlation_threshold': 0.9,\n",
    "    'iv_threshold': 0.02,\n",
    "    'stepwise_method': 'forward',\n",
    "    'stepwise_max_features': 25,\n",
    "\n",
    "    # Model training preferences\n",
    "    'algorithms': [\n",
    "        'logistic',\n",
    "        'lightgbm',\n",
    "        'xgboost',\n",
    "        'catboost',\n",
    "        'randomforest',\n",
    "        'extratrees',\n",
    "        'woe_boost',\n",
    "        'woe_li',\n",
    "        'shao',\n",
    "        'xbooster',\n",
    "    ],\n",
    "    'model_selection_method': 'gini_oot',\n",
    "    'model_stability_weight': 0.2,\n",
    "    'min_gini_threshold': 0.5,\n",
    "    'max_train_oot_gap': 0.03,\n",
    "    'use_optuna': True,\n",
    "    'hpo_trials': 75,\n",
    "    'hpo_timeout_sec': 1800,\n",
    "\n",
    "    # Diagnostics & toggles\n",
    "    'use_noise_sentinel': True,\n",
    "    'enable_dual': True,\n",
    "    'enable_woe_boost_scorecard': True,\n",
    "    'calculate_shap': True,\n",
    "    'enable_scoring': True,\n",
    "    'enable_stage2_calibration': True,\n",
    "\n",
    "    # Risk band settings\n",
    "    'n_risk_bands': 10,\n",
    "    'risk_band_method': 'pd_constraints',\n",
    "    'risk_band_min_bins': 7,\n",
    "    'risk_band_max_bins': 10,\n",
    "    'risk_band_hhi_threshold': 0.15,\n",
    "    'risk_band_binomial_pass_weight': 0.85,\n",
    "\n",
    "    # Runtime controls\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "if not NOTEBOOK_FLAGS.get('tsfresh_available', False):\n",
    "    print('Notebook config: tsfresh features disabled automatically (package not detected).')\n",
    "\n",
    "cfg_field_names = set(Config.__dataclass_fields__.keys())\n",
    "supported_params = {k: v for k, v in cfg_params.items() if k in cfg_field_names}\n",
    "unsupported = sorted(set(cfg_params.keys()) - set(supported_params.keys()))\n",
    "if unsupported:\n",
    "    print(f\"[WARN] Config ignores unsupported parameters: {unsupported}\")\n",
    "\n",
    "cfg = Config(**supported_params)\n",
    "pipe = UnifiedRiskPipeline(cfg)\n",
    "results = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356e0a6",
   "metadata": {},
   "source": [
    "## 3. TSFresh Feature Extraction\n",
    "\n",
    "Use `pipe.run_process` to derive TSFresh features and persist processed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e36384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "if pipe is None:\n",
    "    raise RuntimeError('Pipeline instance is not initialized yet. Run the configuration cell first.')\n",
    "processed = _ensure_processed(force=False)\n",
    "_update_results(results_ref, processed_data=processed)\n",
    "results = results_ref\n",
    "print(f\"Processed feature space: {processed.shape[1]} columns\")\n",
    "\n",
    "if pipe.data_.get('tsfresh_metadata') is not None and not pipe.data_['tsfresh_metadata'].empty:\n",
    "    display(pipe.data_['tsfresh_metadata'].head())\n",
    "else:\n",
    "    flag = 'disabled via config' if not getattr(cfg, 'enable_tsfresh_features', False) else 'not generated'\n",
    "    print(f'No TSFresh features were generated ({flag}).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c84fb",
   "metadata": {},
   "source": [
    "## 4. Raw Numeric Processing\n",
    "\n",
    "Split the processed dataset into train/test/OOT partitions and review preprocessing statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85605ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "if pipe is None:\n",
    "    raise RuntimeError('Pipeline instance is not initialized yet. Run the configuration cell first.')\n",
    "processed = _ensure_processed(force=False)\n",
    "splits = _ensure_splits(force=True)\n",
    "_update_results(results_ref, processed_data=processed, splits=splits)\n",
    "results = results_ref\n",
    "\n",
    "raw_layers = pipe.results_.get('raw_numeric_layers', {})\n",
    "print(f\"Identified numeric features: {len(pipe.data_.get('numeric_features', []))}\")\n",
    "if raw_layers:\n",
    "    train_raw = raw_layers.get('train_raw_prepped')\n",
    "    if train_raw is not None:\n",
    "        display(train_raw[pipe.data_.get('numeric_features', [])].head())\n",
    "else:\n",
    "    print('No numeric preprocessing layer was created.')\n",
    "\n",
    "impute_stats = getattr(pipe.data_processor, 'imputation_stats_', {})\n",
    "if impute_stats:\n",
    "    display(pd.DataFrame(impute_stats).T.head())\n",
    "\n",
    "# Summarise configuration choices for quick inspection\n",
    "config_summary = pd.DataFrame([\n",
    "    (\"Target column\", cfg.target_column),\n",
    "    (\"ID column\", cfg.id_column),\n",
    "    (\"Time column\", cfg.time_column),\n",
    "    (\"Train/Test/OOT split\", f\"{cfg.train_ratio:.0%}/{cfg.test_ratio:.0%}/{cfg.oot_ratio:.0%}\"),\n",
    "    (\"OOT holdout months\", cfg.oot_months),\n",
    "    (\"Risk bands\", f\"{cfg.n_risk_bands} (method={cfg.risk_band_method})\"),\n",
    "    (\"Calibration chain\", f\"{cfg.calibration_stage1_method} -> {cfg.calibration_stage2_method}\"),\n",
    "], columns=[\"Parameter\", \"Configured value\"])\n",
    "display(config_summary)\n",
    "\n",
    "flag_toggles = pd.DataFrame({\n",
    "    \"Feature\": [\n",
    "        \"Dual RAW+WOE flow\",\n",
    "        \"TSFresh feature mining\",\n",
    "        \"Scoring on hold-out data\",\n",
    "        \"Stage 2 calibration\",\n",
    "        \"Optuna HPO\",\n",
    "        \"Noise sentinel\",\n",
    "        \"SHAP importance\",\n",
    "    ],\n",
    "    \"Enabled\": [\n",
    "        getattr(cfg, 'enable_dual', False),\n",
    "        getattr(cfg, 'enable_tsfresh_features', False),\n",
    "        getattr(cfg, 'enable_scoring', False),\n",
    "        getattr(cfg, 'enable_stage2_calibration', False),\n",
    "        getattr(cfg, 'use_optuna', False),\n",
    "        getattr(cfg, 'use_noise_sentinel', False),\n",
    "        getattr(cfg, 'calculate_shap', False),\n",
    "    ],\n",
    "})\n",
    "flag_toggles['Enabled'] = flag_toggles['Enabled'].map({True: 'Yes', False: 'No'})\n",
    "display(flag_toggles)\n",
    "\n",
    "thresholds = pd.DataFrame({\n",
    "    \"Threshold\": [\n",
    "        \"PSI\",\n",
    "        \"IV\",\n",
    "        \"Univariate Gini\",\n",
    "        \"Correlation ceiling\",\n",
    "        \"VIF ceiling\",\n",
    "        \"|Train-OOT| Gini gap\",\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        cfg.psi_threshold,\n",
    "        cfg.iv_threshold,\n",
    "        cfg.min_univariate_gini,\n",
    "        cfg.correlation_threshold,\n",
    "        cfg.vif_threshold,\n",
    "        cfg.max_train_oot_gap,\n",
    "    ],\n",
    "})\n",
    "display(thresholds)\n",
    "\n",
    "selection_order = pd.DataFrame({\"Selection step\": cfg.selection_steps})\n",
    "selection_order.index = selection_order.index + 1\n",
    "display(selection_order)\n",
    "\n",
    "algorithms_df = pd.DataFrame({\"Algorithm\": cfg.algorithms})\n",
    "algorithms_df.index = algorithms_df.index + 1\n",
    "display(algorithms_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f0ba9",
   "metadata": {},
   "source": [
    "## 5. WOE Transformation\n",
    "\n",
    "Apply the WOE transformer on the prepared splits and capture bin statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54556c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "if pipe is None:\n",
    "    print('WOE transformation skipped: pipeline instance not available yet.')\n",
    "else:\n",
    "    splits = _ensure_splits(force=False)\n",
    "    woe_results = pipe.run_woe(splits=splits, force=True)\n",
    "    _update_results(results_ref, splits=splits, woe_results=woe_results)\n",
    "    results = results_ref\n",
    "    woe_values = woe_results.get('woe_values', {})\n",
    "    print(f\"WOE computed for {len(woe_values)} features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0c422",
   "metadata": {},
   "source": [
    "## 6. Feature Selection\n",
    "\n",
    "Execute the configured feature selection pipeline (univariate, PSI, IV, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e03082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "if pipe is None:\n",
    "    print('Feature selection skipped: pipeline instance not available yet.')\n",
    "else:\n",
    "    splits = _ensure_splits(force=False)\n",
    "    woe_results = _ensure_woe(force=False)\n",
    "    selection_mode = 'WOE' if getattr(pipe.config, 'enable_woe', True) else 'RAW'\n",
    "    selection_results = pipe.run_selection(\n",
    "        mode=selection_mode,\n",
    "        splits=splits,\n",
    "        woe_results=woe_results,\n",
    "        force=True,\n",
    "    )\n",
    "    selected = selection_results.get('selected_features', [])\n",
    "    _update_results(results_ref, selection_results=selection_results, selected_features=selected)\n",
    "    results = results_ref\n",
    "    print(f\"Selected {len(selected)} features using {selection_mode} flow.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a812e",
   "metadata": {},
   "source": [
    "## 7. Diagnostic Summaries\n",
    "\n",
    "Inspect post-selection diagnostics, stability metrics, and monitoring summaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836dfae",
   "metadata": {},
   "source": [
    "### 7.1 Raw vs prepped numeric diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fbb56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "processed_df = getattr(pipe, 'data_', {}).get('processed') if pipe is not None else None\n",
    "if processed_df is None or processed_df.empty:\n",
    "    processed_df = _ensure_processed(force=False)\n",
    "if processed_df is None or processed_df.empty:\n",
    "    print(\"Processed dataset snapshot is not available.\")\n",
    "else:\n",
    "    numeric_cols = (\n",
    "        dev_df.select_dtypes(include=['number'])\n",
    "        .columns.difference([cfg.target_column])\n",
    "    )\n",
    "    diagnostics = []\n",
    "    for col in numeric_cols:\n",
    "        raw_series = dev_df[col]\n",
    "        proc_series = processed_df[col]\n",
    "        diagnostics.append({\n",
    "            'feature': col,\n",
    "            'raw_missing': int(raw_series.isna().sum()),\n",
    "            'processed_missing': int(proc_series.isna().sum()),\n",
    "            'raw_mean': float(raw_series.mean()),\n",
    "            'processed_mean': float(proc_series.mean()),\n",
    "        })\n",
    "    diag_df = pd.DataFrame(diagnostics)\n",
    "    if diag_df.empty:\n",
    "        print(\"No numeric columns found for diagnostics.\")\n",
    "    else:\n",
    "        diag_df['missing_delta'] = diag_df['raw_missing'] - diag_df['processed_missing']\n",
    "        diag_df['mean_shift'] = diag_df['processed_mean'] - diag_df['raw_mean']\n",
    "        display(diag_df.sort_values(['missing_delta', 'mean_shift'], ascending=[False, False]).head(12))\n",
    "        top_cols = diag_df.sort_values(['missing_delta', 'mean_shift'], ascending=[False, False])['feature'].head(4).tolist()\n",
    "        if top_cols:\n",
    "            comparison = pd.concat({'raw': dev_df[top_cols], 'prepped': processed_df[top_cols]}, axis=1)\n",
    "            display(comparison.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8fac4",
   "metadata": {},
   "source": [
    "### 7.2 TSFresh feature contributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "tsfresh_meta = (\n",
    "    results_ref.get('tsfresh_metadata')\n",
    "    or results_ref.get('selection_results', {}).get('tsfresh_metadata')\n",
    "    if isinstance(results_ref.get('selection_results'), dict)\n",
    "    else None\n",
    ")\n",
    "if tsfresh_meta is None:\n",
    "    tsfresh_meta = pipe.data_.get('tsfresh_metadata') if pipe is not None else None\n",
    "if isinstance(tsfresh_meta, pd.DataFrame) and not tsfresh_meta.empty:\n",
    "    display(tsfresh_meta.head())\n",
    "else:\n",
    "    print('TSFresh metadata is empty.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1794661",
   "metadata": {},
   "source": [
    "### 7.3 WOE transformation quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562875c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "selection_results = _ensure_selection(force=False) or {}\n",
    "woe_results = _ensure_woe(force=False) or {}\n",
    "if not woe_results:\n",
    "    print(\"WOE diagnostics skipped: run the WOE transformation cell first.\")\n",
    "else:\n",
    "    woe_values = woe_results.get('woe_values', {})\n",
    "    feature = next(iter(selection_results.get('selected_features', woe_values.keys())), None)\n",
    "    if feature is None:\n",
    "        print('No features available for WOE diagnostic display.')\n",
    "    else:\n",
    "        info = woe_values.get(feature, {})\n",
    "        print(f'Details for feature: {feature}')\n",
    "        if isinstance(info, dict) and info.get('stats'):\n",
    "            display(pd.DataFrame(info['stats']).head())\n",
    "        else:\n",
    "            print('  WOE stats not available for this feature.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b7fb3",
   "metadata": {},
   "source": [
    "### 7.4 Feature selection progression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4810bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "selection_results = _ensure_selection(force=False)\n",
    "if not selection_results:\n",
    "    print(\"Selection history is not available yet.\")\n",
    "else:\n",
    "    history = selection_results.get('selection_history')\n",
    "    if not history:\n",
    "        print('Selection history is empty.')\n",
    "    else:\n",
    "        rows = []\n",
    "        for step in history:\n",
    "            if not isinstance(step, dict):\n",
    "                continue\n",
    "            rows.append({\n",
    "                'method': step.get('method'),\n",
    "                'before': step.get('before'),\n",
    "                'after': step.get('after'),\n",
    "                'removed': ', '.join(sorted(step.get('removed', []))) if step.get('removed') else '',\n",
    "            })\n",
    "        display(pd.DataFrame(rows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e027d",
   "metadata": {},
   "source": [
    "### 7.5 Model performance comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "model_results = _ensure_model_results(force=False) or {}\n",
    "model_scores = model_results.get('scores', {}) or {}\n",
    "if not model_scores:\n",
    "    print(\"Model registry is empty.\")\n",
    "else:\n",
    "    df_scores = pd.DataFrame(model_scores).T\n",
    "    display(df_scores.sort_values(['oot_auc', 'test_auc', 'train_auc'], ascending=[False, False, False]))\n",
    "\n",
    "best_model_name = model_results.get('best_model_name')\n",
    "if best_model_name:\n",
    "    print(f\"Best model: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb4cb14",
   "metadata": {},
   "source": [
    "### 7.6 Calibration metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ac7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "stage1 = _ensure_stage1(force=True)\n",
    "stage2 = _ensure_stage2(force=False)\n",
    "stage1_metrics = stage1.get('calibration_metrics') if isinstance(stage1, dict) else None\n",
    "stage2_metrics = stage2.get('stage2_metrics') if isinstance(stage2, dict) else None\n",
    "if stage1_metrics:\n",
    "    print('Stage-1 calibration metrics:')\n",
    "    display(pd.DataFrame([stage1_metrics]))\n",
    "else:\n",
    "    print('Stage-1 calibration metrics are unavailable.')\n",
    "if stage2_metrics:\n",
    "    print('Stage-2 calibration metrics:')\n",
    "    display(pd.DataFrame([stage2_metrics]))\n",
    "else:\n",
    "    print('Stage-2 calibration metrics are unavailable.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b6554",
   "metadata": {},
   "source": [
    "### 7.7 Risk band optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38140c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "risk_band_results = _ensure_risk_bands(force=False)\n",
    "if not risk_band_results:\n",
    "    print(\"Risk band optimizer did not produce results yet.\")\n",
    "else:\n",
    "    band_stats = risk_band_results.get('band_stats') or risk_band_results.get('bands')\n",
    "    if isinstance(band_stats, pd.DataFrame) and not band_stats.empty:\n",
    "        display(band_stats.head())\n",
    "    else:\n",
    "        print('Risk band statistics dataframe is empty.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b50b0",
   "metadata": {},
   "source": [
    "### 7.8 Recent scoring diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f61855",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "scoring_output = _ensure_scoring(force=False)\n",
    "scoring_metrics = scoring_output.get('metrics') if isinstance(scoring_output, dict) else None\n",
    "if scoring_metrics:\n",
    "    display(pd.DataFrame([scoring_metrics]))\n",
    "else:\n",
    "    print('Scoring metrics not available.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15fe0ed",
   "metadata": {},
   "source": [
    "## 8. Stage 1 & Stage 2 Calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "if pipe is None:\n",
    "    print('Calibration skipped: pipeline instance not available yet.')\n",
    "else:\n",
    "    stage1 = pipe.run_stage1_calibration(model_results=pipe.results_.get('model_results'), calibration_df=NOTEBOOK_CONTEXT['data'].get('calibration_longrun'), force=True)\n",
    "    stage2 = pipe.run_stage2_calibration(stage1_results=stage1, recent_df=NOTEBOOK_CONTEXT['data'].get('calibration_recent'), force=True)\n",
    "    _update_results(results_ref, calibration_stage1=stage1, calibration_stage2=stage2)\n",
    "    results = results_ref\n",
    "    print('Calibration refreshed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24908e1f",
   "metadata": {},
   "source": [
    "## 9. Risk Band Optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671439b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "if pipe is None:\n",
    "    print('Risk band optimisation skipped: pipeline instance not available yet.')\n",
    "else:\n",
    "    bands = pipe.run_risk_bands(stage2_results=pipe.results_.get('calibration_stage2'), splits=pipe.results_.get('splits'), force=True)\n",
    "    _update_results(results_ref, risk_bands=bands)\n",
    "    results = results_ref\n",
    "    print('Risk bands recomputed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586df369",
   "metadata": {},
   "source": [
    "## 10. Consolidated Pipeline Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c344cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "if pipe is None:\n",
    "    pipe = UnifiedRiskPipeline(cfg)\n",
    "    globals()['pipe'] = pipe\n",
    "full_results = pipe.fit(\n",
    "    NOTEBOOK_CONTEXT['data']['development'],\n",
    "    data_dictionary=NOTEBOOK_CONTEXT['data']['dictionary'],\n",
    "    calibration_df=NOTEBOOK_CONTEXT['data']['calibration_longrun'],\n",
    "    stage2_df=NOTEBOOK_CONTEXT['data']['calibration_recent'],\n",
    "    score_df=NOTEBOOK_CONTEXT['data']['scoring'],\n",
    ")\n",
    "results = full_results\n",
    "NOTEBOOK_CONTEXT['artifacts'].clear()\n",
    "NOTEBOOK_CONTEXT['artifacts'].update(full_results)\n",
    "print(f\"Best mode: {full_results.get('best_model_mode')} | Best model: {full_results.get('best_model_name')}\")\n",
    "print('Model registry (top rows):')\n",
    "model_registry = pd.DataFrame(full_results.get('model_registry', []))\n",
    "if not model_registry.empty:\n",
    "    display(model_registry.sort_values(['mode', 'oot_auc'], ascending=[True, False]).head())\n",
    "else:\n",
    "    print('Model registry is empty.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cef4aa",
   "metadata": {},
   "source": [
    "## 11. Recent Raw Data Scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e618a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "if pipe is None:\n",
    "    print('Scoring skipped: pipeline instance not available yet.')\n",
    "else:\n",
    "    scoring_output = pipe.run_scoring(NOTEBOOK_CONTEXT['data']['scoring'], force=True)\n",
    "    _update_results(results_ref, scoring_output=scoring_output)\n",
    "    results = results_ref\n",
    "    reports = _ensure_reports(force=True)\n",
    "    excel_path = reports.get('excel_path') if isinstance(reports, dict) else None\n",
    "    if excel_path:\n",
    "        print(f\"Latest reporting workbook: {excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18139d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "reports = _ensure_reports(force=True)\n",
    "if not reports:\n",
    "    print('Reporting artifacts are not available yet.')\n",
    "else:\n",
    "    excel_path = reports.get('excel_path')\n",
    "    if excel_path:\n",
    "        print(f\"Excel workbook generated: {excel_path}\")\n",
    "    available_keys = sorted(reports.keys())\n",
    "    display(pd.DataFrame({'report_key': available_keys}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c253f",
   "metadata": {},
   "source": [
    "For automation examples, see examples/quickstart_demo.py."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
