{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "244c9779",
      "metadata": {},
      "source": [
        "# Credit Risk Pipeline Quickstart\n",
        "\n",
        "This notebook runs the **Unified Risk Pipeline** end-to-end on the bundled synthetic dataset.\n",
        "The sample includes stratified monthly observations, calibration hold-outs, stage-2 data, and a future scoring batch\n",
        "so each major step can be validated quickly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f55c9a1a",
      "metadata": {},
      "source": [
        "## 1. Environment & Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d95991",
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import importlib.metadata as metadata\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def _locate_project_root() -> Path:\n",
        "    cwd = Path.cwd().resolve()\n",
        "    if (cwd / 'src' / 'risk_pipeline').exists():\n",
        "        return cwd\n",
        "    candidate = cwd / 'risk-model-pipeline-dev'\n",
        "    if (candidate / 'src' / 'risk_pipeline').exists():\n",
        "        return candidate\n",
        "    for parent in cwd.parents:\n",
        "        maybe = parent / 'risk-model-pipeline-dev'\n",
        "        if (maybe / 'src' / 'risk_pipeline').exists():\n",
        "            return maybe\n",
        "    return cwd\n",
        "\n",
        "\n",
        "PROJECT_ROOT = _locate_project_root()\n",
        "SRC_PATH = PROJECT_ROOT / 'src'\n",
        "PACKAGE_PATH = SRC_PATH / 'risk_pipeline'\n",
        "MODULE_INIT = PACKAGE_PATH / '__init__.py'\n",
        "if SRC_PATH.exists() and str(SRC_PATH) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_PATH))\n",
        "\n",
        "TARGET_VERSION = '0.4.1'\n",
        "GIT_SPEC = 'risk-pipeline[ml,notebook] @ git+https://github.com/selimoksuz/risk-model-pipeline.git@development'\n",
        "PREREQ_PACKAGES = [\n",
        "    'numba==0.59.1',\n",
        "    'llvmlite==0.42.0',\n",
        "    'scipy==1.11.4',\n",
        "    'pandas==2.3.2',\n",
        "    'tsfresh==0.20.1',\n",
        "    'matrixprofile==1.1.10',\n",
        "    'shap==0.48.0',\n",
        "    'stumpy==1.13.0',\n",
        "]\n",
        "\n",
        "\n",
        "def _parse_version(value: str):\n",
        "    parts = []\n",
        "    for part in value.split('.'):\n",
        "        if not part.isdigit():\n",
        "            break\n",
        "        parts.append(int(part))\n",
        "    return tuple(parts)\n",
        "\n",
        "\n",
        "def _run_pip(args):\n",
        "    subprocess.check_call([\n",
        "        sys.executable,\n",
        "        '-m',\n",
        "        'pip',\n",
        "        'install',\n",
        "        '--no-cache-dir',\n",
        "        '--upgrade',\n",
        "        '--force-reinstall',\n",
        "        *args,\n",
        "    ])\n",
        "\n",
        "\n",
        "def _install_prerequisites():\n",
        "    print(f\"Installing prerequisite stack: {', '.join(PREREQ_PACKAGES)}\")\n",
        "    _run_pip(PREREQ_PACKAGES)\n",
        "\n",
        "\n",
        "def _sanity_check():\n",
        "    import shap  # noqa: F401\n",
        "    from llvmlite import binding as _ll_binding\n",
        "    _ = _ll_binding.ffi.lib\n",
        "    from numba import njit\n",
        "\n",
        "    @njit\n",
        "    def _probe(x):\n",
        "        return x + 1\n",
        "\n",
        "    assert _probe(1) == 2\n",
        "\n",
        "\n",
        "def _tsfresh_smoke_test():\n",
        "    import pandas as pd\n",
        "    from tsfresh import extract_features\n",
        "    from tsfresh.feature_extraction import EfficientFCParameters\n",
        "\n",
        "    data = pd.DataFrame(\n",
        "        {\n",
        "            'id': ['a', 'a', 'a', 'b', 'b', 'b'],\n",
        "            'time': [0, 1, 2, 0, 1, 2],\n",
        "            'value': [1.0, 2.0, 3.0, 4.0, 9.0, 16.0],\n",
        "        }\n",
        "    )\n",
        "    features = extract_features(\n",
        "        data,\n",
        "        column_id='id',\n",
        "        column_sort='time',\n",
        "        column_value='value',\n",
        "        default_fc_parameters=EfficientFCParameters(),\n",
        "        disable_progressbar=True,\n",
        "        n_jobs=0,\n",
        "    )\n",
        "    if not any('entropy' in col for col in features.columns):\n",
        "        raise RuntimeError('tsfresh smoke test did not produce entropy features')\n",
        "\n",
        "\n",
        "def _resolve_installed_version(module):\n",
        "    module_path = Path(getattr(module, '__file__', '')).resolve()\n",
        "    if SRC_PATH in module_path.parents:\n",
        "        return TARGET_VERSION\n",
        "    try:\n",
        "        return metadata.version('risk-pipeline')\n",
        "    except metadata.PackageNotFoundError:\n",
        "        return '0.0.0'\n",
        "\n",
        "\n",
        "def _load_local_package():\n",
        "    if not MODULE_INIT.exists():\n",
        "        return None\n",
        "    spec = importlib.util.spec_from_file_location('risk_pipeline', MODULE_INIT)\n",
        "    if spec and spec.loader:\n",
        "        module = importlib.util.module_from_spec(spec)\n",
        "        sys.modules['risk_pipeline'] = module\n",
        "        spec.loader.exec_module(module)\n",
        "        return module\n",
        "    return None\n",
        "\n",
        "\n",
        "def ensure_risk_pipeline():\n",
        "    print(f\"Resolved project root: {PROJECT_ROOT}\")\n",
        "    try:\n",
        "        module = _load_local_package()\n",
        "        if module is None:\n",
        "            module = importlib.import_module('risk_pipeline')\n",
        "        installed = _resolve_installed_version(module)\n",
        "        if _parse_version(installed) < _parse_version(TARGET_VERSION):\n",
        "            raise ModuleNotFoundError(f'risk-pipeline {installed} < {TARGET_VERSION}')\n",
        "        print(f'risk-pipeline {installed} available (path: {module.__file__}).')\n",
        "        _sanity_check()\n",
        "        _tsfresh_smoke_test()\n",
        "    except Exception as exc:\n",
        "        print(f'risk-pipeline import failed: {exc}')\n",
        "        try:\n",
        "            _install_prerequisites()\n",
        "            print(f'Attempting GitHub install: {GIT_SPEC}')\n",
        "            _run_pip([GIT_SPEC])\n",
        "            print('GitHub install succeeded.')\n",
        "            raise SystemExit('Installation complete. Restart the kernel and rerun this cell.')\n",
        "        except subprocess.CalledProcessError as err:\n",
        "            print(f'GitHub install failed: {err}')\n",
        "            raise SystemExit('Installation failed. Review the errors above.')\n",
        "    else:\n",
        "        print('Numba/llvmlite sanity check passed.')\n",
        "        print('tsfresh smoke test passed (entropy features available).')\n",
        "\n",
        "\n",
        "ensure_risk_pipeline()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a762023",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "from risk_pipeline.data.sample import load_credit_risk_sample\n",
        "\n",
        "sample = load_credit_risk_sample()\n",
        "OUTPUT_DIR = Path('output/credit_risk_sample_notebook')\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "dev_df = sample.development.copy()\n",
        "cal_long_df = sample.calibration_longrun.copy()\n",
        "cal_recent_df = sample.calibration_recent.copy()\n",
        "score_df = sample.scoring_future.copy()\n",
        "data_dictionary = sample.data_dictionary.copy()\n",
        "\n",
        "print(f\"Development dataset: {dev_df.shape[0]:,} rows, {dev_df.shape[1]} columns\")\n",
        "print(f\"Stage 1 calibration dataset: {cal_long_df.shape[0]:,} rows\")\n",
        "print(f\"Stage 2 calibration dataset: {cal_recent_df.shape[0]:,} rows\")\n",
        "print(f\"Scoring dataset: {score_df.shape[0]:,} rows\")\n",
        "\n",
        "display(dev_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "from risk_pipeline.core.config import Config\n",
        "from risk_pipeline.unified_pipeline import UnifiedRiskPipeline\n",
        "from risk_pipeline.data.sample import load_credit_risk_sample\n",
        "\n",
        "sample = load_credit_risk_sample()\n",
        "OUTPUT_DIR = Path('output/credit_risk_sample_notebook')\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "dev_df = sample.development\n",
        "cal_long_df = sample.calibration_longrun\n",
        "cal_recent_df = sample.calibration_recent\n",
        "score_df = sample.scoring_future\n",
        "data_dictionary = sample.data_dictionary\n",
        "\n",
        "dev_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10426cfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "\n",
        "import risk_pipeline.core.config as config_module\n",
        "import risk_pipeline.unified_pipeline as pipeline_module\n",
        "\n",
        "Config = importlib.reload(config_module).Config\n",
        "UnifiedRiskPipeline = importlib.reload(pipeline_module).UnifiedRiskPipeline\n",
        "\n",
        "cfg_params = {\n",
        "    # Core identifiers\n",
        "    'target_column': 'target',\n",
        "    'id_column': 'customer_id',\n",
        "    'time_column': 'app_dt',\n",
        "\n",
        "    # Split configuration\n",
        "    'create_test_split': True,\n",
        "    'stratify_test': True,\n",
        "    'train_ratio': 0.8,\n",
        "    'test_ratio': 0.2,\n",
        "    'oot_ratio': 0.0,\n",
        "    'oot_size': 0.0,\n",
        "    'oot_months': 3,\n",
        "\n",
        "    # TSFresh controls\n",
        "    'enable_tsfresh_features': True,\n",
        "    'tsfresh_feature_set': 'efficient',\n",
        "    'tsfresh_n_jobs': 4,\n",
        "\n",
        "    # Feature selection strategy\n",
        "    'selection_steps': [\n",
        "        'univariate',\n",
        "        'psi',\n",
        "        'vif',\n",
        "        'correlation',\n",
        "        'iv',\n",
        "        'boruta',\n",
        "        'stepwise',\n",
        "    ],\n",
        "    'min_univariate_gini': 0.05,\n",
        "    'psi_threshold': 0.25,\n",
        "    'monthly_psi_threshold': 0.15,\n",
        "    'oot_psi_threshold': 0.25,\n",
        "    'max_vif': 5.0,\n",
        "    'correlation_threshold': 0.9,\n",
        "    'iv_threshold': 0.02,\n",
        "    'stepwise_method': 'forward',\n",
        "    'stepwise_max_features': 25,\n",
        "\n",
        "    # Model training preferences\n",
        "    'algorithms': [\n",
        "        'logistic',\n",
        "        'lightgbm',\n",
        "        'xgboost',\n",
        "        'catboost',\n",
        "        'randomforest',\n",
        "        'extratrees',\n",
        "        'woe_boost',\n",
        "        'woe_li',\n",
        "        'shao',\n",
        "        'xbooster',\n",
        "    ],\n",
        "    'model_selection_method': 'gini_oot',\n",
        "    'model_stability_weight': 0.2,\n",
        "    'min_gini_threshold': 0.5,\n",
        "    'max_train_oot_gap': 0.03,\n",
        "    'use_optuna': True,\n",
        "    'hpo_trials': 75,\n",
        "    'hpo_timeout_sec': 1800,\n",
        "\n",
        "    # Diagnostics & toggles\n",
        "    'use_noise_sentinel': True,\n",
        "    'enable_dual': True,\n",
        "    'enable_woe_boost_scorecard': True,\n",
        "    'calculate_shap': True,\n",
        "    'enable_scoring': True,\n",
        "    'enable_stage2_calibration': True,\n",
        "\n",
        "    # Risk band settings\n",
        "    'n_risk_bands': 10,\n",
        "    'risk_band_method': 'pd_constraints',\n",
        "    'risk_band_min_bins': 7,\n",
        "    'risk_band_max_bins': 10,\n",
        "    'risk_band_hhi_threshold': 0.15,\n",
        "    'risk_band_binomial_pass_weight': 0.85,\n",
        "\n",
        "    # Runtime controls\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "}\n",
        "\n",
        "cfg_field_names = set(Config.__dataclass_fields__.keys())\n",
        "supported_params = {k: v for k, v in cfg_params.items() if k in cfg_field_names}\n",
        "unsupported = sorted(set(cfg_params.keys()) - set(supported_params.keys()))\n",
        "if unsupported:\n",
        "    print(f\"[WARN] Config ignores unsupported parameters: {unsupported}\")\n",
        "\n",
        "cfg = Config(**supported_params)\n",
        "\n",
        "pipe = UnifiedRiskPipeline(cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed1555b8",
      "metadata": {},
      "source": [
        "## 3. TSFresh Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fbab735",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "processed = pipe.run_process(dev_df, create_map=True, force=True)\n",
        "print(f\"Processed feature space: {processed.shape[1]} columns\")\n",
        "\n",
        "if pipe.data_.get('tsfresh_metadata') is not None and not pipe.data_['tsfresh_metadata'].empty:\n",
        "    display(pipe.data_['tsfresh_metadata'].head())\n",
        "else:\n",
        "    print('No TSFresh features were generated (configuration disabled).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "251cad32",
      "metadata": {},
      "source": [
        "## 4. Raw Numeric Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af1fea4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "splits = pipe.run_split(processed, force=True)\n",
        "raw_layers = pipe.results_.get('raw_numeric_layers', {})\n",
        "print(f\"Identified numeric features: {len(pipe.data_.get('numeric_features', []))}\")\n",
        "if raw_layers:\n",
        "    train_raw = raw_layers.get('train_raw_prepped')\n",
        "    if train_raw is not None:\n",
        "        display(train_raw[pipe.data_.get('numeric_features', [])].head())\n",
        "else:\n",
        "    print('No numeric preprocessing layer was created.')\n",
        "\n",
        "impute_stats = getattr(pipe.data_processor, 'imputation_stats_', {})\n",
        "if impute_stats:\n",
        "    display(pd.DataFrame(impute_stats).T.head())\n",
        "\n",
        "# Summarise configuration choices for quick inspection\n",
        "config_summary = pd.DataFrame([\n",
        "    (\"Target column\", cfg.target_column),\n",
        "    (\"ID column\", cfg.id_column),\n",
        "    (\"Time column\", cfg.time_column),\n",
        "    (\"Train/Test/OOT split\", f\"{cfg.train_ratio:.0%}/{cfg.test_ratio:.0%}/{cfg.oot_ratio:.0%}\"),\n",
        "    (\"OOT holdout months\", cfg.oot_months),\n",
        "    (\"Risk bands\", f\"{cfg.n_risk_bands} (method={cfg.risk_band_method})\"),\n",
        "    (\"Calibration chain\", f\"{cfg.calibration_stage1_method} -> {cfg.calibration_stage2_method}\"),\n",
        "], columns=[\"Parameter\", \"Configured value\"])\n",
        "display(config_summary)\n",
        "\n",
        "flag_toggles = pd.DataFrame({\n",
        "    \"Feature\": [\n",
        "        \"Dual RAW+WOE flow\",\n",
        "        \"TSFresh feature mining\",\n",
        "        \"Scoring on hold-out data\",\n",
        "        \"Stage 2 calibration\",\n",
        "        \"Optuna HPO\",\n",
        "        \"Noise sentinel\",\n",
        "        \"SHAP importance\",\n",
        "    ],\n",
        "    \"Enabled\": [\n",
        "        getattr(cfg, 'enable_dual', False),\n",
        "        getattr(cfg, 'enable_tsfresh_features', False),\n",
        "        getattr(cfg, 'enable_scoring', False),\n",
        "        getattr(cfg, 'enable_stage2_calibration', False),\n",
        "        getattr(cfg, 'use_optuna', False),\n",
        "        getattr(cfg, 'use_noise_sentinel', False),\n",
        "        getattr(cfg, 'calculate_shap', False),\n",
        "    ],\n",
        "})\n",
        "flag_toggles['Enabled'] = flag_toggles['Enabled'].map({True: 'Yes', False: 'No'})\n",
        "display(flag_toggles)\n",
        "\n",
        "thresholds = pd.DataFrame({\n",
        "    \"Threshold\": [\n",
        "        \"PSI\",\n",
        "        \"IV\",\n",
        "        \"Univariate Gini\",\n",
        "        \"Correlation ceiling\",\n",
        "        \"VIF ceiling\",\n",
        "        \"|Train-OOT| Gini gap\",\n",
        "    ],\n",
        "    \"Value\": [\n",
        "        cfg.psi_threshold,\n",
        "        cfg.iv_threshold,\n",
        "        cfg.univariate_gini_threshold,\n",
        "        cfg.correlation_threshold,\n",
        "        cfg.vif_threshold,\n",
        "        cfg.max_train_oot_gap,\n",
        "    ],\n",
        "})\n",
        "display(thresholds)\n",
        "\n",
        "selection_order = pd.DataFrame({\"Selection step\": cfg.selection_steps})\n",
        "selection_order.index = selection_order.index + 1\n",
        "display(selection_order)\n",
        "\n",
        "algorithms_df = pd.DataFrame({\"Algorithm\": cfg.algorithms})\n",
        "algorithms_df.index = algorithms_df.index + 1\n",
        "display(algorithms_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fe6e1e4",
      "metadata": {},
      "source": [
        "## 5. WOE Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc3bbb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "woe_results = pipe.run_woe(splits, force=True)\n",
        "woe_values = woe_results.get('woe_values', {})\n",
        "print(f\"WOE maps generated for {len(woe_values)} variables\")\n",
        "if woe_values:\n",
        "    preview = pd.DataFrame([\n",
        "        {\n",
        "            'variable': name,\n",
        "            'type': info.get('type'),\n",
        "            'iv': info.get('iv'),\n",
        "        }\n",
        "        for name, info in list(woe_values.items())[:5]\n",
        "    ])\n",
        "    display(preview)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60fc5aa0",
      "metadata": {},
      "source": [
        "## 6. Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Raw vs prepped numeric diagnostics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "processed_df = pipe.data_.get('processed')\n",
        "if processed_df is None or processed_df.empty:\n",
        "    print(\"Processed dataset snapshot is not available.\")\n",
        "else:\n",
        "    numeric_cols = (\n",
        "        dev_df.select_dtypes(include=['number']).columns\n",
        "        .difference([cfg.target_column])\n",
        "    )\n",
        "    diagnostics = []\n",
        "    for col in numeric_cols:\n",
        "        raw_series = dev_df[col]\n",
        "        proc_series = processed_df[col]\n",
        "        missing_raw = int(raw_series.isna().sum())\n",
        "        missing_proc = int(proc_series.isna().sum())\n",
        "        diagnostics.append({\n",
        "            'feature': col,\n",
        "            'raw_missing': missing_raw,\n",
        "            'processed_missing': missing_proc,\n",
        "            'missing_delta': missing_raw - missing_proc,\n",
        "            'raw_p01': float(raw_series.quantile(0.01)),\n",
        "            'proc_p01': float(proc_series.quantile(0.01)),\n",
        "            'raw_p99': float(raw_series.quantile(0.99)),\n",
        "            'proc_p99': float(proc_series.quantile(0.99)),\n",
        "            'raw_mean': float(raw_series.mean()),\n",
        "            'proc_mean': float(proc_series.mean()),\n",
        "        })\n",
        "    diagnostics_df = pd.DataFrame(diagnostics)\n",
        "    if diagnostics_df.empty:\n",
        "        print(\"No numeric columns found for diagnostics.\")\n",
        "    else:\n",
        "        diagnostics_df['clip_delta_low'] = diagnostics_df['proc_p01'] - diagnostics_df['raw_p01']\n",
        "        diagnostics_df['clip_delta_high'] = diagnostics_df['proc_p99'] - diagnostics_df['raw_p99']\n",
        "        diagnostics_df['mean_shift'] = diagnostics_df['proc_mean'] - diagnostics_df['raw_mean']\n",
        "        display(\n",
        "            diagnostics_df.sort_values(['missing_delta', 'clip_delta_low'], ascending=[False, False])\n",
        "            .head(12)\n",
        "        )\n",
        "        top_cols = (\n",
        "            diagnostics_df.sort_values(['missing_delta', 'clip_delta_low'], ascending=[False, False])\n",
        "            ['feature'].head(4).tolist()\n",
        "        )\n",
        "        if top_cols:\n",
        "            comparison = pd.concat(\n",
        "                {\n",
        "                    'raw': dev_df[top_cols],\n",
        "                    'prepped': processed_df[top_cols]\n",
        "                }, axis=1\n",
        "            )\n",
        "            display(comparison.head(5))\n",
        "    get_summary = getattr(pipe.splitter, 'get_split_summary', None)\n",
        "    if callable(get_summary):\n",
        "        split_summary = get_summary()\n",
        "        if isinstance(split_summary, pd.DataFrame) and not split_summary.empty:\n",
        "            display(split_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 TSFresh feature contributions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "tsfresh_meta = pipe.reporter.reports_.get('tsfresh_metadata')\n",
        "if tsfresh_meta is None or tsfresh_meta.empty:\n",
        "    print(\"No TSFresh metadata captured (feature generation may be disabled).\")\n",
        "else:\n",
        "    tsfresh_meta = tsfresh_meta.copy()\n",
        "    if 'source_variable' in tsfresh_meta.columns:\n",
        "        tsfresh_meta['source_variable'] = tsfresh_meta['source_variable'].astype(str)\n",
        "    else:\n",
        "        tsfresh_meta['source_variable'] = 'unknown'\n",
        "    if 'statistic' in tsfresh_meta.columns:\n",
        "        tsfresh_meta['statistic'] = tsfresh_meta['statistic'].fillna('unknown').astype(str)\n",
        "    else:\n",
        "        tsfresh_meta['statistic'] = 'unknown'\n",
        "    total_features = int(tsfresh_meta['feature'].nunique())\n",
        "    total_bases = int(tsfresh_meta['source_variable'].nunique())\n",
        "    print(f\"Generated {total_features} TSFresh features across {total_bases} base variables.\")\n",
        "    source_summary = (\n",
        "        tsfresh_meta.groupby('source_variable')\n",
        "        .size()\n",
        "        .rename('feature_count')\n",
        "        .sort_values(ascending=False)\n",
        "        .reset_index()\n",
        "    )\n",
        "    if not source_summary.empty:\n",
        "        source_summary['share'] = (\n",
        "            source_summary['feature_count'] / source_summary['feature_count'].sum()\n",
        "        ).round(4)\n",
        "        display(source_summary.head(15))\n",
        "    stat_mix = (\n",
        "        tsfresh_meta.groupby('statistic')\n",
        "        .size()\n",
        "        .rename('feature_count')\n",
        "        .sort_values(ascending=False)\n",
        "        .reset_index()\n",
        "    )\n",
        "    if not stat_mix.empty:\n",
        "        display(stat_mix)\n",
        "    display(tsfresh_meta.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 WOE transformation quality\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "woe_results = results.get('woe_results') or {}\n",
        "if not woe_results:\n",
        "    print(\"WOE results dictionary is empty.\")\n",
        "else:\n",
        "    woe_values = woe_results.get('woe_values', {}) or {}\n",
        "    gini_map = woe_results.get('univariate_gini', {}) or {}\n",
        "    summary_rows = []\n",
        "    for feature, info in woe_values.items():\n",
        "        stats = info.get('stats') or []\n",
        "        feature_type = info.get('type', 'unknown')\n",
        "        if feature_type == 'numeric':\n",
        "            if info.get('bins') is not None:\n",
        "                bin_count = max(len(info['bins']) - 1, len(stats))\n",
        "            else:\n",
        "                bin_count = len(stats)\n",
        "        else:\n",
        "            categories = info.get('categories')\n",
        "            if categories:\n",
        "                bin_count = len(categories)\n",
        "            else:\n",
        "                bin_count = len(stats)\n",
        "        gini_info = gini_map.get(feature, {}) or {}\n",
        "        summary_rows.append({\n",
        "            'feature': feature,\n",
        "            'type': feature_type,\n",
        "            'iv': info.get('iv'),\n",
        "            'bin_count': bin_count,\n",
        "            'gini_raw': gini_info.get('gini_raw'),\n",
        "            'gini_woe': gini_info.get('gini_woe'),\n",
        "        })\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "    if summary_df.empty:\n",
        "        print(\"No WOE summary rows to display.\")\n",
        "    else:\n",
        "        summary_df['gini_uplift'] = summary_df['gini_woe'].fillna(0) - summary_df['gini_raw'].fillna(0)\n",
        "        display(summary_df.sort_values('iv', ascending=False).head(15))\n",
        "        display(summary_df.sort_values('gini_uplift').head(10))\n",
        "        top_feature = summary_df.sort_values('iv', ascending=False)['feature'].iloc[0]\n",
        "        top_stats = woe_values.get(top_feature, {}).get('stats')\n",
        "        if top_stats:\n",
        "            display(pd.DataFrame(top_stats).head(15))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Feature selection progression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "selection_results = results.get('selection_results') or {}\n",
        "selection_history = pipe.reporter.reports_.get('selection_history')\n",
        "if selection_history is None or selection_history.empty:\n",
        "    print(\"Selection history is empty.\")\n",
        "else:\n",
        "    selection_history = selection_history.copy()\n",
        "    if 'details' in selection_history.columns:\n",
        "        details_series = selection_history['details']\n",
        "        selection_history = selection_history.drop(columns='details')\n",
        "    else:\n",
        "        details_series = pd.Series([{} for _ in range(len(selection_history))])\n",
        "    expanded = details_series.apply(lambda val: val if isinstance(val, dict) else {}).apply(pd.Series)\n",
        "    history_df = pd.concat([selection_history, expanded], axis=1)\n",
        "    display(history_df)\n",
        "selected_features = selection_results.get('selected_features') or results.get('selected_features') or []\n",
        "if selected_features:\n",
        "    feature_list = pd.DataFrame({'rank': np.arange(1, len(selected_features) + 1), 'feature': selected_features})\n",
        "    display(feature_list.head(40))\n",
        "    print(f\"Total selected features: {len(selected_features)}\")\n",
        "feature_report = pipe.reporter.reports_.get('features')\n",
        "if isinstance(feature_report, pd.DataFrame) and not feature_report.empty:\n",
        "    display(feature_report.head(25))\n",
        "dropped = selection_results.get('dropped_features')\n",
        "if dropped:\n",
        "    if isinstance(dropped, dict):\n",
        "        dropped_df = pd.DataFrame(list(dropped.items()), columns=['feature', 'reason'])\n",
        "    else:\n",
        "        dropped_df = pd.DataFrame(dropped)\n",
        "    if not dropped_df.empty:\n",
        "        display(dropped_df.head(40))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 Model performance comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "model_results = results.get('model_results', {})\n",
        "model_scores = model_results.get('scores', {}) or {}\n",
        "if not model_scores:\n",
        "    print(\"Model scores not available.\")\n",
        "else:\n",
        "    scores_df = pd.DataFrame(model_scores).T\n",
        "    metric_priority = [col for col in ['oot_auc', 'test_auc', 'train_auc'] if col in scores_df.columns]\n",
        "    metric_cols = [col for col in ['train_auc', 'test_auc', 'oot_auc', 'train_gini', 'test_gini', 'oot_gini', 'train_oot_gap'] if col in scores_df.columns]\n",
        "    display(scores_df[metric_cols].sort_values(by=metric_priority, ascending=False))\n",
        "    if 'train_auc' in scores_df.columns and any(col in scores_df.columns for col in ['oot_auc', 'test_auc']):\n",
        "        ref_cols = [col for col in ['oot_auc', 'test_auc'] if col in scores_df.columns]\n",
        "        scores_df['overfit_gap'] = scores_df['train_auc'] - scores_df[ref_cols].max(axis=1)\n",
        "        display(scores_df['overfit_gap'].sort_values())\n",
        "best_model = model_results.get('best_model_name') or results.get('best_model_name')\n",
        "chosen_flow = results.get('chosen_flow')\n",
        "if best_model and model_scores:\n",
        "    print(f\"Best model: {best_model} (flow: {chosen_flow})\")\n",
        "    if best_model in model_scores:\n",
        "        display(pd.Series(model_scores[best_model]).dropna())\n",
        "feature_importance = model_results.get('feature_importance', {})\n",
        "if feature_importance and best_model in feature_importance:\n",
        "    fi_df = pd.DataFrame(feature_importance[best_model])\n",
        "    if not fi_df.empty:\n",
        "        display(fi_df.head(25))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.6 Calibration metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "stage1 = results.get('calibration_stage1') or {}\n",
        "stage2 = results.get('calibration_stage2') or {}\n",
        "stage1_metrics = stage1.get('calibration_metrics')\n",
        "if stage1_metrics:\n",
        "    stage1_df = pd.DataFrame(stage1_metrics, index=['Stage 1']).T\n",
        "    display(stage1_df)\n",
        "    if stage1.get('stage1_details'):\n",
        "        display(pd.DataFrame(stage1['stage1_details'], index=['Stage 1']).T)\n",
        "stage2_metrics = stage2.get('stage2_metrics')\n",
        "if stage2_metrics:\n",
        "    stage2_df = pd.DataFrame(stage2_metrics, index=['Stage 2']).T\n",
        "    display(stage2_df)\n",
        "stage2_details = stage2.get('stage2_details')\n",
        "if stage2_details:\n",
        "    display(pd.DataFrame(stage2_details, index=['Stage 2']).T)\n",
        "calibration_report = pipe.reporter.reports_.get('calibration')\n",
        "if calibration_report is not None:\n",
        "    display(calibration_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.7 Risk band optimisation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "risk_band_results = results.get('risk_bands') or {}\n",
        "if not risk_band_results:\n",
        "    print(\"Risk band optimizer did not return results.\")\n",
        "else:\n",
        "    band_table = risk_band_results.get('bands') or risk_band_results.get('band_table')\n",
        "    if isinstance(band_table, pd.DataFrame) and not band_table.empty:\n",
        "        display(band_table)\n",
        "    metrics = risk_band_results.get('metrics') or {}\n",
        "    if metrics:\n",
        "        display(pd.DataFrame(metrics, index=['metrics']).T)\n",
        "risk_band_summary = pipe.reporter.reports_.get('risk_bands_summary')\n",
        "if isinstance(risk_band_summary, pd.DataFrame) and not risk_band_summary.empty:\n",
        "    display(risk_band_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.8 Recent scoring diagnostics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "scoring_output = results.get('scoring_output') or {}\n",
        "scoring_metrics = scoring_output.get('metrics') or results.get('scoring_metrics') or {}\n",
        "if scoring_metrics:\n",
        "    flat_metrics = {\n",
        "        key: value\n",
        "        for key, value in scoring_metrics.items()\n",
        "        if not isinstance(value, (dict, list))\n",
        "    }\n",
        "    if flat_metrics:\n",
        "        summary_df = pd.DataFrame(flat_metrics, index=['value']).T\n",
        "        display(summary_df)\n",
        "    if 'with_target' in scoring_metrics and scoring_metrics['with_target']:\n",
        "        display(pd.DataFrame(scoring_metrics['with_target'], index=['with_target']).T)\n",
        "    if 'without_target' in scoring_metrics and scoring_metrics['without_target']:\n",
        "        display(pd.DataFrame(scoring_metrics['without_target'], index=['without_target']).T)\n",
        "reports = scoring_output.get('reports') or results.get('scoring_reports') or {}\n",
        "for name, report in reports.items():\n",
        "    if isinstance(report, pd.DataFrame) and not report.empty:\n",
        "        print(f\"Report: {name}\")\n",
        "        display(report)\n",
        "scored_df = scoring_output.get('dataframe') or results.get('scoring_results')\n",
        "if isinstance(scored_df, pd.DataFrame) and not scored_df.empty:\n",
        "    scored_view = scored_df.copy()\n",
        "    if 'risk_score' not in scored_view.columns:\n",
        "        scored_view['risk_score'] = np.nan\n",
        "    band_summary = scored_view.groupby('risk_band').agg(\n",
        "        records=('risk_band', 'size'),\n",
        "        avg_score=('risk_score', 'mean'),\n",
        "        min_score=('risk_score', 'min'),\n",
        "        max_score=('risk_score', 'max')\n",
        "    )\n",
        "    if 'target' in scored_view.columns:\n",
        "        band_summary['bads'] = scored_view.groupby('risk_band')['target'].sum()\n",
        "        band_summary['bad_rate'] = scored_view.groupby('risk_band')['target'].mean()\n",
        "    display(band_summary)\n",
        "    display(scored_view.head(10))\n",
        "else:\n",
        "    print(\"Scoring dataframe is not available.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a607124",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "stage1 = pipe.run_stage1_calibration(model_results=pipe.results_['model_results'], calibration_df=cal_long_df, force=True)\n",
        "stage2 = pipe.run_stage2_calibration(stage1_results=stage1, recent_df=cal_recent_df, force=True)\n",
        "\n",
        "if isinstance(stage1, dict) and stage1.get('calibration_curve') is not None:\n",
        "    curve = stage1['calibration_curve']\n",
        "    if hasattr(curve, 'head'):\n",
        "        display(curve.head())\n",
        "\n",
        "pipe.results_['calibration_stage1'] = stage1\n",
        "pipe.results_['calibration_stage2'] = stage2\n",
        "print('Stage 1 and Stage 2 calibration completed.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efff055f",
      "metadata": {},
      "source": [
        "## 9. Risk Band Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "363e6d41",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "bands = pipe.run_risk_bands(stage2_results=stage2, splits=pipe.results_['splits'], force=True)\n",
        "pipe.results_['risk_bands'] = bands\n",
        "\n",
        "if isinstance(bands, dict):\n",
        "    metrics = bands.get('metrics')\n",
        "    if isinstance(metrics, dict):\n",
        "        display(pd.DataFrame(metrics, index=['value']).T)\n",
        "    else:\n",
        "        print('Risk band metrics not available.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3347a97c",
      "metadata": {},
      "source": [
        "## 10. Consolidated Pipeline Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7605948",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "full_pipe = UnifiedRiskPipeline(cfg)\n",
        "full_results = full_pipe.fit(\n",
        "    dev_df,\n",
        "    data_dictionary=data_dictionary,\n",
        "    calibration_df=cal_long_df,\n",
        "    stage2_df=cal_recent_df,\n",
        "    score_df=score_df,\n",
        ")\n",
        "\n",
        "print(f\"Best mode: {full_results.get('best_model_mode')} | Best model: {full_results.get('best_model_name')}\")\n",
        "print('Model registry (top rows):')\n",
        "model_registry = pd.DataFrame(full_results.get('model_registry', []))\n",
        "if not model_registry.empty:\n",
        "    display(model_registry.sort_values(['mode', 'oot_auc'], ascending=[True, False]).head())\n",
        "else:\n",
        "    print('Model registry is empty.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a90f6e6e",
      "metadata": {},
      "source": [
        "## 11. Recent Raw Data Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe3d0c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "scoring_output = pipe.run_scoring(score_df, force=True)\n",
        "scored_df = scoring_output.get('dataframe')\n",
        "if scored_df is not None:\n",
        "    display(scored_df.head())\n",
        "metrics = scoring_output.get('metrics')\n",
        "if metrics:\n",
        "    print('Scoring metrics:')\n",
        "    display(pd.DataFrame(metrics, index=[0]).T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84c253f",
      "metadata": {},
      "source": [
        "For automation examples, see examples/quickstart_demo.py."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}