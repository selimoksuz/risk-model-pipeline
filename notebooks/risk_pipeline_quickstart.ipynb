{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244c9779",
   "metadata": {},
   "source": [
    "# Credit Risk Pipeline Quickstart\n",
    "\n",
    "This notebook runs the **Unified Risk Pipeline** end-to-end on the bundled synthetic dataset.\n",
    "The sample includes stratified monthly observations, calibration hold-outs, stage-2 data, and a future scoring batch\n",
    "so each major step can be validated quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c9a1a",
   "metadata": {},
   "source": [
    "## 1. Environment & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f1bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import importlib.util\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _locate_project_root() -> Path:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    if (cwd / 'src' / 'risk_pipeline').exists():\n",
    "        return cwd\n",
    "    candidate = cwd / 'risk-model-pipeline-dev'\n",
    "    if (candidate / 'src' / 'risk_pipeline').exists():\n",
    "        return candidate\n",
    "    for parent in cwd.parents:\n",
    "        maybe = parent / 'risk-model-pipeline-dev'\n",
    "        if (maybe / 'src' / 'risk_pipeline').exists():\n",
    "            return maybe\n",
    "    return cwd\n",
    "\n",
    "\n",
    "PROJECT_ROOT = _locate_project_root()\n",
    "SRC_PATH = PROJECT_ROOT / 'src'\n",
    "PACKAGE_PATH = SRC_PATH / 'risk_pipeline'\n",
    "MODULE_INIT = PACKAGE_PATH / '__init__.py'\n",
    "if SRC_PATH.exists() and str(SRC_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_PATH))\n",
    "\n",
    "\n",
    "def _load_local_package():\n",
    "    if not MODULE_INIT.exists():\n",
    "        return None\n",
    "    spec = importlib.util.spec_from_file_location('risk_pipeline', MODULE_INIT)\n",
    "    if spec and spec.loader:\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        sys.modules['risk_pipeline'] = module\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "    return None\n",
    "\n",
    "\n",
    "def ensure_risk_pipeline():\n",
    "    module = _load_local_package()\n",
    "    if module is None:\n",
    "        module = importlib.import_module('risk_pipeline')\n",
    "    version = getattr(module, '__version__', 'local-dev')\n",
    "    location = Path(getattr(module, '__file__', 'unknown')).resolve()\n",
    "    print(f'risk-pipeline loaded (version {version}, path={location})')\n",
    "    return module\n",
    "\n",
    "\n",
    "TSFRESH_AVAILABLE = importlib.util.find_spec('tsfresh') is not None\n",
    "if TSFRESH_AVAILABLE:\n",
    "    print('tsfresh available (advanced time-series features can be enabled via config).')\n",
    "else:\n",
    "    print('tsfresh is not installed; pipeline will fall back to lightweight aggregate features when needed.')\n",
    "\n",
    "risk_pipeline_module = ensure_risk_pipeline()\n",
    "NOTEBOOK_FLAGS = globals().setdefault('_NOTEBOOK_FLAGS', {})\n",
    "NOTEBOOK_FLAGS['tsfresh_available'] = TSFRESH_AVAILABLE\n",
    "NOTEBOOK_FLAGS['project_root'] = PROJECT_ROOT\n",
    "NOTEBOOK_FLAGS['src_path'] = SRC_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "from risk_pipeline.core.config import Config\n",
    "from risk_pipeline.unified_pipeline import UnifiedRiskPipeline\n",
    "from risk_pipeline.data.sample import load_credit_risk_sample\n",
    "\n",
    "NOTEBOOK_CONTEXT = globals().setdefault('_NOTEBOOK_CONTEXT', {'data': {}, 'artifacts': {}, 'paths': {}, 'options': {}})\n",
    "\n",
    "# ensure pipeline placeholders exist for diagnostic cells during step-by-step execution\n",
    "if 'pipe' not in globals():\n",
    "    pipe = None\n",
    "if 'results' not in globals():\n",
    "    results = {}\n",
    "if 'full_results' not in globals():\n",
    "    full_results = {}\n",
    "\n",
    "sample = load_credit_risk_sample()\n",
    "OUTPUT_DIR = Path('output/credit_risk_sample_notebook')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MIN_SAMPLE_SIZE = 50000\n",
    "CALIBRATION_SAMPLE_SIZE = 50000\n",
    "STAGE2_SAMPLE_SIZE = 50000\n",
    "RISK_BAND_SAMPLE_SIZE = 50000\n",
    "random_seed = 42\n",
    "\n",
    "\n",
    "def _ensure_min_rows(frame: pd.DataFrame, target: int, *, seed: int = 42) -> pd.DataFrame:\n",
    "    if frame is None or target is None:\n",
    "        return frame\n",
    "    frame = frame.copy()\n",
    "    current = len(frame)\n",
    "    if current >= target:\n",
    "        return frame\n",
    "    multiplier, remainder = divmod(target, current)\n",
    "    pieces = [frame.copy() for _ in range(max(multiplier - 1, 0))]\n",
    "    if remainder:\n",
    "        pieces.append(frame.sample(remainder, replace=True, random_state=seed).reset_index(drop=True))\n",
    "    if pieces:\n",
    "        frame = pd.concat([frame, *pieces], ignore_index=True)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def _harmonize_snapshot_month(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    if frame is not None and 'snapshot_month' in frame.columns:\n",
    "        try:\n",
    "            frame['snapshot_month'] = pd.to_datetime(frame['snapshot_month']).dt.to_period('M').dt.to_timestamp()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return frame\n",
    "\n",
    "\n",
    "def _prepare_with_target_size(frame: pd.DataFrame, target: int) -> pd.DataFrame:\n",
    "    adjusted = _ensure_min_rows(frame, target, seed=random_seed)\n",
    "    return _harmonize_snapshot_month(adjusted)\n",
    "\n",
    "\n",
    "dev_df = _prepare_with_target_size(sample.development, MIN_SAMPLE_SIZE)\n",
    "cal_long_df = _prepare_with_target_size(sample.calibration_longrun, CALIBRATION_SAMPLE_SIZE)\n",
    "cal_recent_df = _prepare_with_target_size(sample.calibration_recent, STAGE2_SAMPLE_SIZE)\n",
    "risk_band_df = _prepare_with_target_size(sample.calibration_longrun, RISK_BAND_SAMPLE_SIZE)\n",
    "score_df = sample.scoring_future.copy()\n",
    "data_dictionary = sample.data_dictionary.copy() if hasattr(sample.data_dictionary, 'copy') else sample.data_dictionary\n",
    "\n",
    "datasets = {\n",
    "    'development': dev_df,\n",
    "    'calibration_longrun': cal_long_df,\n",
    "    'calibration_recent': cal_recent_df,\n",
    "    'risk_band_reference': risk_band_df,\n",
    "    'scoring': score_df,\n",
    "    'dictionary': data_dictionary,\n",
    "}\n",
    "NOTEBOOK_CONTEXT['data'].update(datasets)\n",
    "NOTEBOOK_CONTEXT['paths']['output'] = OUTPUT_DIR\n",
    "\n",
    "# ensure demo missingness as before\n",
    "_exclusion_cols = {'target', 'snapshot_month', 'customer_id', 'app_id', 'application_id', 'app_dt', 'decision_dt'}\n",
    "_rng = np.random.default_rng(random_seed)\n",
    "\n",
    "\n",
    "def _inject_demo_missing(frame, rate=0.01, max_features=5):\n",
    "    if frame.isna().sum().sum() > 0:\n",
    "        return frame\n",
    "    numeric_candidates = [\n",
    "        col for col in frame.select_dtypes(include=['number']).columns\n",
    "        if col.lower() not in _exclusion_cols and not col.lower().endswith('_id')\n",
    "    ]\n",
    "    if not numeric_candidates:\n",
    "        return frame\n",
    "    for col in numeric_candidates[:max_features]:\n",
    "        mask = _rng.random(len(frame)) < rate\n",
    "        if mask.any():\n",
    "            frame.loc[mask, col] = np.nan\n",
    "    return frame\n",
    "\n",
    "for key in ('development', 'calibration_longrun', 'calibration_recent', 'risk_band_reference', 'scoring'):\n",
    "    _inject_demo_missing(datasets[key])\n",
    "\n",
    "dataset_overview = []\n",
    "for name in ('development', 'calibration_longrun', 'calibration_recent', 'risk_band_reference', 'scoring'):\n",
    "    df = NOTEBOOK_CONTEXT['data'].get(name)\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        overview = {\n",
    "            'dataset': name,\n",
    "            'rows': len(df),\n",
    "            'target_non_null': int(df['target'].notna().sum()) if 'target' in df.columns else None,\n",
    "            'unique_customers': df['customer_id'].nunique() if 'customer_id' in df.columns else None,\n",
    "        }\n",
    "        dataset_overview.append(overview)\n",
    "if dataset_overview:\n",
    "    display(pd.DataFrame(dataset_overview))\n",
    "\n",
    "dev_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87230fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "from risk_pipeline.core.config import Config\n",
    "from risk_pipeline.unified_pipeline import UnifiedRiskPipeline\n",
    "from risk_pipeline.data.sample import load_credit_risk_sample\n",
    "\n",
    "NOTEBOOK_CONTEXT = globals().setdefault('_NOTEBOOK_CONTEXT', {'data': {}, 'artifacts': {}, 'paths': {}, 'options': {}})\n",
    "def _current_config():\n",
    "    for name in ('pipe', 'full_pipe', 'raw_pipe'):\n",
    "        candidate = globals().get(name)\n",
    "        cfg = getattr(candidate, 'config', None) if candidate is not None else None\n",
    "        if cfg is not None:\n",
    "            return cfg\n",
    "    return globals().get('cfg')\n",
    "\n",
    "def _config_flag(name, default=False):\n",
    "    cfg_obj = _current_config()\n",
    "    if cfg_obj is None:\n",
    "        return bool(default)\n",
    "    return bool(getattr(cfg_obj, name, default))\n",
    "\n",
    "def _get_pipeline_context():\n",
    "    context = globals().get('_NOTEBOOK_CONTEXT') or {}\n",
    "    pipe_candidates = [\n",
    "        globals().get('pipe'),\n",
    "        globals().get('full_pipe'),\n",
    "        globals().get('raw_pipe'),\n",
    "    ]\n",
    "    pipe = next((p for p in pipe_candidates if p is not None), None)\n",
    "    results = globals().get('results')\n",
    "    if not isinstance(results, dict):\n",
    "        results = {}\n",
    "    if not results and pipe is not None:\n",
    "        results = getattr(pipe, 'results_', {}) or {}\n",
    "    if not results and isinstance(globals().get('full_results'), dict):\n",
    "        results = globals()['full_results']\n",
    "    if pipe is not None:\n",
    "        signature = _config_signature(getattr(pipe, 'config', None))\n",
    "        options = context.setdefault('options', {})\n",
    "        previous = options.get('config_signature')\n",
    "        if signature != previous:\n",
    "            options['config_signature'] = signature\n",
    "            context['artifacts'] = {}\n",
    "            results = {}\n",
    "            globals()['results'] = results\n",
    "            if hasattr(pipe, 'results_'):\n",
    "                pipe.results_ = {}\n",
    "    globals()['results'] = results\n",
    "    return pipe, results if isinstance(results, dict) else {}\n",
    "\n",
    "def _store_artifact(name, value):\n",
    "    context = globals().get('_NOTEBOOK_CONTEXT') or {}\n",
    "    context.setdefault('artifacts', {})[name] = value\n",
    "    return value\n",
    "\n",
    "def _artifact_available(value):\n",
    "    if value is None:\n",
    "        return False\n",
    "    if isinstance(value, pd.DataFrame):\n",
    "        return not value.empty\n",
    "    if isinstance(value, (list, tuple, dict, set)):\n",
    "        return len(value) > 0\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _materialize_model_registry(model_results=None, force=False):\n",
    "    context = globals().get('_NOTEBOOK_CONTEXT') or {}\n",
    "    artifacts = context.setdefault('artifacts', {})\n",
    "    if not force:\n",
    "        cached = artifacts.get('model_registry_df')\n",
    "        if isinstance(cached, pd.DataFrame) and not cached.empty:\n",
    "            return cached\n",
    "    if model_results is None:\n",
    "        stored = artifacts.get('model_results')\n",
    "        if not isinstance(stored, dict):\n",
    "            stored = globals().get('results', {}).get('model_results') if isinstance(globals().get('results'), dict) else None\n",
    "        model_results = stored or {}\n",
    "    if not isinstance(model_results, dict):\n",
    "        model_results = {}\n",
    "    registry_df = pd.DataFrame()\n",
    "    registry = model_results.get('model_registry') if isinstance(model_results, dict) else None\n",
    "    if isinstance(registry, pd.DataFrame):\n",
    "        registry_df = registry.copy()\n",
    "    elif registry:\n",
    "        try:\n",
    "            registry_df = pd.DataFrame(registry)\n",
    "        except Exception:\n",
    "            registry_df = pd.DataFrame()\n",
    "    scores = model_results.get('scores') if isinstance(model_results, dict) else {}\n",
    "    if registry_df.empty and isinstance(scores, dict) and scores:\n",
    "        rows = []\n",
    "        for name, metrics in scores.items():\n",
    "            row = {'model_name': name}\n",
    "            if isinstance(metrics, dict):\n",
    "                for key, value in metrics.items():\n",
    "                    if isinstance(key, str):\n",
    "                        row.setdefault(key, value)\n",
    "            rows.append(row)\n",
    "        if rows:\n",
    "            registry_df = pd.DataFrame(rows)\n",
    "    if registry_df.empty:\n",
    "        registry_df = pd.DataFrame(columns=['model_name'])\n",
    "    else:\n",
    "        if 'model_name' not in registry_df.columns:\n",
    "            if 'name' in registry_df.columns:\n",
    "                registry_df = registry_df.rename(columns={'name': 'model_name'})\n",
    "            else:\n",
    "                registry_df = registry_df.reset_index().rename(columns={'index': 'model_name'})\n",
    "        mode_hint = None\n",
    "        if isinstance(model_results, dict):\n",
    "            for key in ('mode', 'best_model_mode', 'active_model_mode', 'best_mode'):\n",
    "                value = model_results.get(key)\n",
    "                if value:\n",
    "                    mode_hint = value\n",
    "                    break\n",
    "        if 'mode' not in registry_df.columns:\n",
    "            registry_df['mode'] = mode_hint\n",
    "        else:\n",
    "            registry_df['mode'] = registry_df['mode'].fillna(mode_hint)\n",
    "        if isinstance(scores, dict) and scores and 'model_name' in registry_df.columns:\n",
    "            def _score_lookup(model_name, key):\n",
    "                entry = scores.get(model_name)\n",
    "                return entry.get(key) if isinstance(entry, dict) else None\n",
    "            for key in ('oot_auc', 'test_auc', 'train_auc', 'oot_gini', 'test_gini', 'train_gini'):\n",
    "                if key not in registry_df.columns:\n",
    "                    registry_df[key] = registry_df['model_name'].map(lambda name: _score_lookup(name, key))\n",
    "        sort_cols = [col for col in ('mode', 'oot_auc', 'test_auc', 'train_auc') if col in registry_df.columns]\n",
    "        if sort_cols:\n",
    "            ascending = [True] + [False] * (len(sort_cols) - 1)\n",
    "            registry_df = registry_df.sort_values(sort_cols, ascending=ascending, ignore_index=True)\n",
    "        registry_df = registry_df.reset_index(drop=True)\n",
    "    artifacts['model_registry_df'] = registry_df\n",
    "    globals().setdefault('results', {}).setdefault('model_registry_df', registry_df)\n",
    "    return registry_df\n",
    "\n",
    "def _config_signature(cfg):\n",
    "    if cfg is None:\n",
    "        return None\n",
    "    watched = {\n",
    "        'enable_tsfresh_features': getattr(cfg, 'enable_tsfresh_features', None),\n",
    "        'enable_tsfresh_rolling': getattr(cfg, 'enable_tsfresh_rolling', None),\n",
    "        'tsfresh_window_months': getattr(cfg, 'tsfresh_window_months', None),\n",
    "        'tsfresh_min_events': getattr(cfg, 'tsfresh_min_events', None),\n",
    "        'tsfresh_min_unique_months': getattr(cfg, 'tsfresh_min_unique_months', None),\n",
    "        'tsfresh_min_coverage_ratio': getattr(cfg, 'tsfresh_min_coverage_ratio', None),\n",
    "        'tsfresh_include_current_record': getattr(cfg, 'tsfresh_include_current_record', None),\n",
    "        'tsfresh_feature_set': getattr(cfg, 'tsfresh_feature_set', None),\n",
    "        'tsfresh_custom_fc_parameters': bool(getattr(cfg, 'tsfresh_custom_fc_parameters', None)),\n",
    "        'tsfresh_n_jobs': getattr(cfg, 'tsfresh_n_jobs', None),\n",
    "        'tsfresh_use_multiprocessing': getattr(cfg, 'tsfresh_use_multiprocessing', None),\n",
    "        'enable_stage2_calibration': getattr(cfg, 'enable_stage2_calibration', None),\n",
    "        'enable_scoring': getattr(cfg, 'enable_scoring', None),\n",
    "        'optimize_risk_bands': getattr(cfg, 'optimize_risk_bands', None),\n",
    "    }\n",
    "    return tuple(sorted(watched.items()))\n",
    "\n",
    "def _update_results(results_ref, **artifacts):\n",
    "    if not isinstance(results_ref, dict):\n",
    "        return results_ref\n",
    "    results_ref.update(artifacts)\n",
    "    globals()['results'] = results_ref\n",
    "    return results_ref\n",
    "\n",
    "def _ensure_dev_df():\n",
    "    context = globals().get('_NOTEBOOK_CONTEXT') or {}\n",
    "    data = context.get('data', {})\n",
    "    df = data.get('development')\n",
    "    if df is None:\n",
    "        df = globals().get('dev_df')\n",
    "    return df\n",
    "\n",
    "def _ensure_processed(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('processed_data')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('processed_data')\n",
    "        if cached is not None:\n",
    "            _store_artifact('processed_data', cached)\n",
    "            return cached\n",
    "    source_df = _ensure_dev_df()\n",
    "    if source_df is None:\n",
    "        raise RuntimeError('Development dataframe is not loaded yet. Run the data preparation cell first.')\n",
    "    processed = pipe.run_process(source_df, create_map=True, include_noise=False, force=bool(force))\n",
    "    _update_results(results_ref, processed_data=processed)\n",
    "    _store_artifact('processed_data', processed)\n",
    "    return processed\n",
    "\n",
    "def _ensure_splits(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('splits')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('splits')\n",
    "        if cached is not None:\n",
    "            _store_artifact('splits', cached)\n",
    "            return cached\n",
    "    processed = _ensure_processed(force=False)\n",
    "    splits = pipe.run_split(processed, force=True)\n",
    "    _update_results(results_ref, splits=splits)\n",
    "    _store_artifact('splits', splits)\n",
    "    return splits\n",
    "\n",
    "def _ensure_woe(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('woe_results')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('woe_results')\n",
    "        if cached is not None:\n",
    "            _store_artifact('woe_results', cached)\n",
    "            return cached\n",
    "    splits = _ensure_splits(force=False)\n",
    "    if splits is None:\n",
    "        raise RuntimeError('Splits are unavailable; run the split cell first.')\n",
    "    woe_results = pipe.run_woe(splits=splits, force=True)\n",
    "    _update_results(results_ref, woe_results=woe_results)\n",
    "    _store_artifact('woe_results', woe_results)\n",
    "    return woe_results\n",
    "\n",
    "def _ensure_selection(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('selection_results')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('selection_results')\n",
    "        if _artifact_available(cached):\n",
    "            _store_artifact('selection_results', cached)\n",
    "            return cached\n",
    "    splits = _ensure_splits(force=False)\n",
    "    woe_results = _ensure_woe(force=False)\n",
    "    selection_mode = 'WOE' if getattr(pipe.config, 'enable_woe', True) else 'RAW'\n",
    "    selection_results = pipe.run_selection(\n",
    "        mode=selection_mode,\n",
    "        splits=splits,\n",
    "        woe_results=woe_results,\n",
    "        force=True,\n",
    "    )\n",
    "    selected = selection_results.get('selected_features', [])\n",
    "    _update_results(results_ref, selection_results=selection_results, selected_features=selected)\n",
    "    _store_artifact('selection_results', selection_results)\n",
    "    return selection_results\n",
    "\n",
    "def _ensure_model_results(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('model_results')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('model_results')\n",
    "        if _artifact_available(cached):\n",
    "            pipe.selected_features_ = cached.get('selected_features', getattr(pipe, 'selected_features_', []))\n",
    "            _store_artifact('model_results', cached)\n",
    "            registry_df = _materialize_model_registry(cached, force=True)\n",
    "            _update_results(results_ref, model_registry_df=registry_df)\n",
    "            return cached\n",
    "    selection_results = _ensure_selection(force=False) or {}\n",
    "    splits = _ensure_splits(force=False)\n",
    "    dual_enabled = getattr(pipe.config, 'enable_dual_pipeline', getattr(pipe.config, 'enable_dual', False))\n",
    "\n",
    "    def _best_auc(result):\n",
    "        if not isinstance(result, dict):\n",
    "            return float('-inf')\n",
    "        name = result.get('best_model_name')\n",
    "        scores = result.get('scores') or {}\n",
    "        entry = scores.get(name) if name else None\n",
    "        if not isinstance(entry, dict):\n",
    "            return float('-inf')\n",
    "        return entry.get('oot_auc') or entry.get('test_auc') or entry.get('train_auc') or float('-inf')\n",
    "\n",
    "    if dual_enabled:\n",
    "        woe_results = pipe.run_modeling(\n",
    "            mode='WOE',\n",
    "            splits=splits,\n",
    "            selection_results=selection_results if (selection_results.get('mode') if isinstance(selection_results, dict) else 'WOE') == 'WOE' else None,\n",
    "            force=True,\n",
    "        )\n",
    "        raw_results = pipe.run_modeling(\n",
    "            mode='RAW',\n",
    "            splits=splits,\n",
    "            selection_results=None,\n",
    "            force=True,\n",
    "        )\n",
    "        flows = {'WOE': woe_results, 'RAW': raw_results}\n",
    "        best_mode = max(flows.keys(), key=lambda mode: _best_auc(flows[mode]))\n",
    "        best_results = pipe.run_modeling(mode=best_mode, splits=splits, selection_results=None, force=False)\n",
    "        dual_registry = {\n",
    "            mode: {\n",
    "                'best_model_name': res.get('best_model_name'),\n",
    "                'best_auc': _best_auc(res),\n",
    "                'scores': res.get('scores', {}),\n",
    "            }\n",
    "            for mode, res in flows.items()\n",
    "        }\n",
    "        model_results = dict(best_results)\n",
    "        model_results['dual_registry'] = dual_registry\n",
    "        model_results['best_mode'] = best_mode\n",
    "        model_results['best_model_mode'] = best_results.get('mode', best_mode)\n",
    "        model_results['best_auc'] = _best_auc(best_results)\n",
    "        results_ref['model_results_WOE'] = woe_results\n",
    "        results_ref['model_results_RAW'] = raw_results\n",
    "    else:\n",
    "        selection_mode = selection_results.get('mode') if isinstance(selection_results, dict) else 'WOE'\n",
    "        selection_mode = selection_mode or ('WOE' if getattr(pipe.config, 'enable_woe', True) else 'RAW')\n",
    "        model_results = pipe.run_modeling(\n",
    "            mode=selection_mode,\n",
    "            splits=splits,\n",
    "            selection_results=selection_results,\n",
    "            force=True,\n",
    "        )\n",
    "\n",
    "    best_selection = pipe.results_.get('selection_results')\n",
    "    if isinstance(best_selection, dict):\n",
    "        _update_results(results_ref, selection_results=best_selection)\n",
    "    registry_df = _materialize_model_registry(model_results, force=True)\n",
    "    _update_results(results_ref, model_results=model_results, model_registry_df=registry_df)\n",
    "    _store_artifact('model_results', model_results)\n",
    "    return model_results\n",
    "def _ensure_stage1(force=False):\n",
    "    if not _config_flag('enable_stage2_calibration', False):\n",
    "        return None\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('calibration_stage1')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('calibration_stage1')\n",
    "        if _artifact_available(cached):\n",
    "            _store_artifact('calibration_stage1', cached)\n",
    "            return cached\n",
    "    model_results = _ensure_model_results(force=False)\n",
    "    calibration_df = NOTEBOOK_CONTEXT.get('data', {}).get('calibration_longrun')\n",
    "    if calibration_df is None:\n",
    "        calibration_df = NOTEBOOK_CONTEXT.get('data', {}).get('development')\n",
    "    stage1 = pipe.run_stage1_calibration(model_results=model_results, calibration_df=calibration_df, force=True)\n",
    "    _update_results(results_ref, calibration_stage1=stage1)\n",
    "    _store_artifact('calibration_stage1', stage1)\n",
    "    return stage1\n",
    "\n",
    "def _ensure_stage2(force=False):\n",
    "    if not _config_flag('enable_stage2_calibration', False):\n",
    "        return None\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('calibration_stage2')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('calibration_stage2')\n",
    "        if _artifact_available(cached):\n",
    "            _store_artifact('calibration_stage2', cached)\n",
    "            return cached\n",
    "    stage1 = _ensure_stage1(force=False)\n",
    "    if not stage1:\n",
    "        print('Stage-2 calibration skipped: Stage-1 results unavailable.')\n",
    "        return None\n",
    "    recent_df = NOTEBOOK_CONTEXT.get('data', {}).get('calibration_recent')\n",
    "    if recent_df is None:\n",
    "        print('Stage-2 calibration skipped: recent dataset not loaded.')\n",
    "        return None\n",
    "    try:\n",
    "        stage2 = pipe.run_stage2_calibration(stage1_results=stage1, recent_df=recent_df, force=True)\n",
    "    except Exception as exc:\n",
    "        print(f'Stage-2 calibration failed: {exc}')\n",
    "        return None\n",
    "    _update_results(results_ref, calibration_stage2=stage2)\n",
    "    _store_artifact('calibration_stage2', stage2)\n",
    "    return stage2\n",
    "def _ensure_risk_bands(force=False):\n",
    "    if not _config_flag('optimize_risk_bands', True):\n",
    "        return {}\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('risk_bands')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('risk_bands')\n",
    "        if _artifact_available(cached):\n",
    "            _store_artifact('risk_bands', cached)\n",
    "            return cached\n",
    "    stage1 = _ensure_stage1(force=False)\n",
    "    stage2 = _ensure_stage2(force=False)\n",
    "    if not stage2 and stage1:\n",
    "        print('Risk band optimisation: Stage-2 results unavailable; using Stage-1 calibration output.')\n",
    "        stage2 = stage1\n",
    "    if not stage2:\n",
    "        print('Risk band optimisation skipped: calibration results unavailable.')\n",
    "        return {}\n",
    "    splits = _ensure_splits(force=False)\n",
    "    raw_override = NOTEBOOK_CONTEXT.get('data', {}).get('risk_band_reference')\n",
    "    processed_override = pipe.data_.get('risk_band_reference') if hasattr(pipe, 'data_') else None\n",
    "    if processed_override is None and raw_override is not None:\n",
    "        try:\n",
    "            processed_override = pipe._process_data(raw_override, create_map=False, include_noise=False)\n",
    "        except Exception:\n",
    "            processed_override = raw_override\n",
    "    if processed_override is None:\n",
    "        fallback_raw = NOTEBOOK_CONTEXT.get('data', {}).get('development')\n",
    "        if fallback_raw is not None:\n",
    "            try:\n",
    "                processed_override = pipe._process_data(fallback_raw, create_map=False, include_noise=False)\n",
    "            except Exception:\n",
    "                processed_override = fallback_raw\n",
    "    if processed_override is not None:\n",
    "        override_rows = len(processed_override)\n",
    "    else:\n",
    "        override_rows = 0\n",
    "    risk_band_df = processed_override\n",
    "    if risk_band_df is None:\n",
    "        risk_band_df = NOTEBOOK_CONTEXT.get('data', {}).get('development')\n",
    "    bands = pipe.run_risk_bands(stage1_results=stage1, stage2_results=stage2, splits=splits, data_override=risk_band_df, force=True)\n",
    "    if isinstance(bands, dict):\n",
    "        bands.setdefault('override_rows', override_rows)\n",
    "    _update_results(results_ref, risk_bands=bands)\n",
    "    _store_artifact('risk_bands', bands)\n",
    "    return bands\n",
    "def _ensure_scoring(force=False):\n",
    "    if not _config_flag('enable_scoring', False):\n",
    "        return {}\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('scoring_output')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('scoring_output')\n",
    "        if _artifact_available(cached):\n",
    "            _store_artifact('scoring_output', cached)\n",
    "            return cached\n",
    "    score_df = NOTEBOOK_CONTEXT.get('data', {}).get('scoring')\n",
    "    if score_df is None:\n",
    "        score_df = globals().get('score_df')\n",
    "    if score_df is None:\n",
    "        raise RuntimeError('Scoring dataset is not loaded.')\n",
    "    stage2 = _ensure_stage2(force=False)\n",
    "    selection = _ensure_selection(force=False)\n",
    "    woe_results = _ensure_woe(force=False)\n",
    "    model_results = _ensure_model_results(force=False)\n",
    "    splits = _ensure_splits(force=False)\n",
    "    scoring_output = pipe.run_scoring(\n",
    "        score_df,\n",
    "        stage2_results=stage2,\n",
    "        selection_results=selection,\n",
    "        woe_results=woe_results,\n",
    "        model_results=model_results,\n",
    "        splits=splits,\n",
    "        force=True,\n",
    "    )\n",
    "    _update_results(results_ref, scoring_output=scoring_output)\n",
    "    _store_artifact('scoring_output', scoring_output)\n",
    "    return scoring_output\n",
    "\n",
    "def _ensure_reports(force=False):\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        return None\n",
    "    if not force:\n",
    "        cached = results_ref.get('reports')\n",
    "        if cached is None:\n",
    "            cached = pipe.results_.get('reports')\n",
    "        if _artifact_available(cached):\n",
    "            _store_artifact('reports', cached)\n",
    "            return cached\n",
    "    reports = pipe.run_reporting(force=True)\n",
    "    _update_results(results_ref, reports=reports)\n",
    "    _store_artifact('reports', reports)\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5f4f0",
   "metadata": {},
   "source": [
    "## 2. Configure the Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20771cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "\n",
    "import risk_pipeline.core.feature_selector_enhanced as fs_module\n",
    "import risk_pipeline.core.config as config_module\n",
    "import risk_pipeline.unified_pipeline as pipeline_module\n",
    "\n",
    "AdvancedFeatureSelector = importlib.reload(fs_module).AdvancedFeatureSelector\n",
    "Config = importlib.reload(config_module).Config\n",
    "UnifiedRiskPipeline = importlib.reload(pipeline_module).UnifiedRiskPipeline\n",
    "\n",
    "cfg_params = {\n",
    "    # Core identifiers\n",
    "    'target_column': 'target',\n",
    "    'id_column': 'customer_id',\n",
    "    'time_column': 'app_dt',\n",
    "\n",
    "    # Split configuration\n",
    "    'create_test_split': True,\n",
    "    'stratify_test': True,\n",
    "    'train_ratio': 0.8,\n",
    "    'test_ratio': 0.2,\n",
    "    'oot_ratio': 0.0,\n",
    "    'oot_months': 3,\n",
    "\n",
    "    # Output controls\n",
    "    'output_folder': str(NOTEBOOK_CONTEXT['paths']['output']),\n",
    "    'output_excel_path': str(NOTEBOOK_CONTEXT['paths']['output'] / 'risk_pipeline_report.xlsx'),\n",
    "\n",
    "    # TSFresh controls (auto-disabled if package missing)\n",
    "    'enable_tsfresh_features': False,\n",
    "    'tsfresh_feature_set': 'efficient',\n",
    "    'tsfresh_n_jobs': 4,\n",
    "    'enable_tsfresh_rolling': False,\n",
    "    'tsfresh_window_months': 12,\n",
    "    'tsfresh_min_events': 1,\n",
    "    'tsfresh_min_unique_months': 1,\n",
    "    'tsfresh_min_coverage_ratio': 1.0,\n",
    "    'tsfresh_include_current_record': False,\n",
    "\n",
    "    # Feature selection strategy\n",
    "    'selection_steps': [\n",
    "        'univariate',\n",
    "        'psi',\n",
    "        'vif',\n",
    "        'correlation',\n",
    "        'iv',\n",
    "        'boruta',\n",
    "        'stepwise',\n",
    "    ],\n",
    "    'min_univariate_gini': 0.05,\n",
    "    'psi_threshold': 0.25,\n",
    "    'monthly_psi_threshold': 0.15,\n",
    "    'oot_psi_threshold': 0.25,\n",
    "    'vif_threshold': 5.0,\n",
    "    'correlation_threshold': 0.9,\n",
    "    'iv_threshold': 0.02,\n",
    "    'stepwise_method': 'forward',\n",
    "    'stepwise_max_features': 25,\n",
    "\n",
    "    # Model training preferences\n",
    "    'algorithms': [\n",
    "        'logistic',\n",
    "        'lightgbm',\n",
    "        'xgboost',\n",
    "        'catboost',\n",
    "        'randomforest',\n",
    "        'extratrees',\n",
    "        'woe_boost',\n",
    "        'woe_li',\n",
    "        'shao',\n",
    "        'xbooster',\n",
    "    ],\n",
    "    'model_selection_method': 'gini_oot',\n",
    "    'model_stability_weight': 0.2,\n",
    "    'min_gini_threshold': 0.5,\n",
    "    'max_train_oot_gap': 0.03,\n",
    "    'use_optuna': True,\n",
    "    'hpo_trials': 1,\n",
    "    'hpo_timeout_sec': 1800,\n",
    "\n",
    "    # Diagnostics & toggles\n",
    "    'use_noise_sentinel': True,\n",
    "    'enable_dual': True,\n",
    "    'enable_woe_boost_scorecard': True,\n",
    "    'calculate_shap': True,\n",
    "    'enable_scoring': True,\n",
    "    'score_model_name': 'best',\n",
    "    'enable_stage2_calibration': True,\n",
    "\n",
    "    # Risk band settings\n",
    "    'n_risk_bands': 10,\n",
    "    'risk_band_method': 'pd_constraints',\n",
    "    'risk_band_min_bins': 7,\n",
    "    'risk_band_max_bins': 10,\n",
    "    'risk_band_hhi_threshold': 0.15,\n",
    "    'risk_band_binomial_pass_weight': 0.85,\n",
    "\n",
    "    # Runtime controls\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "if not NOTEBOOK_FLAGS.get('tsfresh_available', False):\n",
    "    print('Notebook config: tsfresh features disabled automatically (package not detected).')\n",
    "\n",
    "cfg_field_names = set(Config.__dataclass_fields__.keys())\n",
    "supported_params = {k: v for k, v in cfg_params.items() if k in cfg_field_names}\n",
    "unsupported = sorted(set(cfg_params.keys()) - set(supported_params.keys()))\n",
    "if unsupported:\n",
    "    print(f\"[WARN] Config ignores unsupported parameters: {unsupported}\")\n",
    "\n",
    "cfg = Config(**supported_params)\n",
    "pipe = UnifiedRiskPipeline(cfg)\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356e0a6",
   "metadata": {},
   "source": [
    "## 3. TSFresh Feature Extraction\n",
    "\n",
    "Use `pipe.run_process` to derive TSFresh features and persist processed data.\n",
    "\n",
    "- `enable_tsfresh_rolling`: activates leakage-safe rolling windows built on the last *N* months per record.\n",
    "- `tsfresh_window_months`: lookback window size; rows earlier than this horizon are ignored for feature generation.\n",
    "- `tsfresh_min_events`, `tsfresh_min_unique_months`, `tsfresh_min_coverage_ratio`: minimum history required before a row is flagged as ready.\n",
    "- `tsfresh_include_current_record`: when true, the current observation is part of the window; keep it `False` to avoid leakage.\n",
    "- Coverage diagnostics (`tsfresh_events_count`, `tsfresh_window_ready`, etc.) are stored under `pipeline.data_['tsfresh_coverage']` for QA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e36384",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe, results_ref = _get_pipeline_context()\n",
    "cfg_local = _current_config()\n",
    "if cfg_local is not None and not getattr(cfg_local, 'enable_tsfresh_features', False):\n",
    "    print('TSFresh feature mining disabled via config; skipping processing cell.')\n",
    "else:\n",
    "    if pipe is None:\n",
    "        raise RuntimeError('Pipeline instance is not initialized yet. Run the configuration cell first.')\n",
    "    processed = _ensure_processed(force=False)\n",
    "    _update_results(results_ref, processed_data=processed)\n",
    "    results = results_ref\n",
    "    print(f\"Processed feature space: {processed.shape[1]} columns\")\n",
    "\n",
    "    tsfresh_meta = pipe.data_.get('tsfresh_metadata') if pipe is not None else None\n",
    "    if isinstance(tsfresh_meta, pd.DataFrame) and not tsfresh_meta.empty:\n",
    "        display(tsfresh_meta.head())\n",
    "    else:\n",
    "        flag = 'disabled via config' if cfg_local is not None and not getattr(cfg_local, 'enable_tsfresh_features', False) else 'not generated'\n",
    "        print(f'No TSFresh features were generated ({flag}).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c84fb",
   "metadata": {},
   "source": [
    "## 4. Raw Numeric Processing\n",
    "\n",
    "Split the processed dataset into train/test/OOT partitions and review preprocessing statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85605ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "if pipe is None:\n",
    "    raise RuntimeError('Pipeline instance is not initialized yet. Run the configuration cell first.')\n",
    "processed = _ensure_processed(force=False)\n",
    "splits = _ensure_splits(force=True)\n",
    "_update_results(results_ref, processed_data=processed, splits=splits)\n",
    "results = results_ref\n",
    "\n",
    "raw_layers = pipe.results_.get('raw_numeric_layers', {})\n",
    "print(f\"Identified numeric features: {len(pipe.data_.get('numeric_features', []))}\")\n",
    "if raw_layers:\n",
    "    train_raw = raw_layers.get('train_raw_prepped')\n",
    "    if train_raw is not None:\n",
    "        display(train_raw[pipe.data_.get('numeric_features', [])].head())\n",
    "else:\n",
    "    print('No numeric preprocessing layer was created.')\n",
    "\n",
    "impute_stats = getattr(pipe.data_processor, 'imputation_stats_', {})\n",
    "if impute_stats:\n",
    "    display(pd.DataFrame(impute_stats).T.head())\n",
    "\n",
    "# Summarise configuration choices for quick inspection\n",
    "config_summary = pd.DataFrame([\n",
    "    (\"Target column\", cfg.target_column),\n",
    "    (\"ID column\", cfg.id_column),\n",
    "    (\"Time column\", cfg.time_column),\n",
    "    (\"Train/Test/OOT split\", f\"{cfg.train_ratio:.0%}/{cfg.test_ratio:.0%}/{cfg.oot_ratio:.0%}\"),\n",
    "    (\"OOT holdout months\", cfg.oot_months),\n",
    "    (\"Risk bands\", f\"{cfg.n_risk_bands} (method={cfg.risk_band_method})\"),\n",
    "    (\"Calibration chain\", f\"{cfg.calibration_stage1_method} -> {cfg.calibration_stage2_method}\"),\n",
    "], columns=[\"Parameter\", \"Configured value\"])\n",
    "display(config_summary)\n",
    "\n",
    "flag_toggles = pd.DataFrame({\n",
    "    \"Feature\": [\n",
    "        \"Dual RAW+WOE flow\",\n",
    "        \"TSFresh feature mining\",\n",
    "        \"Scoring on hold-out data\",\n",
    "        \"Stage 2 calibration\",\n",
    "        \"Optuna HPO\",\n",
    "        \"Noise sentinel\",\n",
    "        \"SHAP importance\",\n",
    "    ],\n",
    "    \"Enabled\": [\n",
    "        getattr(cfg, 'enable_dual', False),\n",
    "        getattr(cfg, 'enable_tsfresh_features', False),\n",
    "        getattr(cfg, 'enable_scoring', False),\n",
    "        getattr(cfg, 'enable_stage2_calibration', False),\n",
    "        getattr(cfg, 'use_optuna', False),\n",
    "        getattr(cfg, 'use_noise_sentinel', False),\n",
    "        getattr(cfg, 'calculate_shap', False),\n",
    "    ],\n",
    "})\n",
    "flag_toggles['Enabled'] = flag_toggles['Enabled'].map({True: 'Yes', False: 'No'})\n",
    "display(flag_toggles)\n",
    "\n",
    "thresholds = pd.DataFrame({\n",
    "    \"Threshold\": [\n",
    "        \"PSI\",\n",
    "        \"IV\",\n",
    "        \"Univariate Gini\",\n",
    "        \"Correlation ceiling\",\n",
    "        \"VIF ceiling\",\n",
    "        \"|Train-OOT| Gini gap\",\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        cfg.psi_threshold,\n",
    "        cfg.iv_threshold,\n",
    "        cfg.min_univariate_gini,\n",
    "        cfg.correlation_threshold,\n",
    "        cfg.vif_threshold,\n",
    "        cfg.max_train_oot_gap,\n",
    "    ],\n",
    "})\n",
    "display(thresholds)\n",
    "\n",
    "selection_order = pd.DataFrame({\"Selection step\": cfg.selection_steps})\n",
    "selection_order.index = selection_order.index + 1\n",
    "display(selection_order)\n",
    "\n",
    "algorithms_df = pd.DataFrame({\"Algorithm\": cfg.algorithms})\n",
    "algorithms_df.index = algorithms_df.index + 1\n",
    "display(algorithms_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f0ba9",
   "metadata": {},
   "source": [
    "## 5. WOE Transformation\n",
    "\n",
    "Apply the WOE transformer on the prepared splits and capture bin statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54556c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "if pipe is None:\n",
    "    print('WOE transformation skipped: pipeline instance not available yet.')\n",
    "else:\n",
    "    splits = _ensure_splits(force=False)\n",
    "    woe_results = pipe.run_woe(splits=splits, force=True)\n",
    "    _update_results(results_ref, splits=splits, woe_results=woe_results)\n",
    "    results = results_ref\n",
    "    woe_values = woe_results.get('woe_values', {})\n",
    "    print(f\"WOE computed for {len(woe_values)} features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0c422",
   "metadata": {},
   "source": [
    "## 6. Feature Selection\n",
    "\n",
    "Execute the configured feature selection pipeline (univariate, PSI, IV, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e03082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "if pipe is None:\n",
    "    print('Feature selection skipped: pipeline instance not available yet.')\n",
    "else:\n",
    "    splits = _ensure_splits(force=False)\n",
    "    woe_results = _ensure_woe(force=False)\n",
    "    selection_mode = 'WOE' if getattr(pipe.config, 'enable_woe', True) else 'RAW'\n",
    "    selection_results = pipe.run_selection(\n",
    "        mode=selection_mode,\n",
    "        splits=splits,\n",
    "        woe_results=woe_results,\n",
    "        force=True,\n",
    "    )\n",
    "    selected = selection_results.get('selected_features', [])\n",
    "    _update_results(results_ref, selection_results=selection_results, selected_features=selected)\n",
    "    results = results_ref\n",
    "    print(f\"Selected {len(selected)} features using {selection_mode} flow.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a812e",
   "metadata": {},
   "source": [
    "## 7. Diagnostic Summaries\n",
    "\n",
    "Inspect post-selection diagnostics, stability metrics, and monitoring summaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836dfae",
   "metadata": {},
   "source": [
    "### 7.1 Raw vs prepped numeric diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fbb56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "processed_df = getattr(pipe, 'data_', {}).get('processed') if pipe is not None else None\n",
    "if processed_df is None or processed_df.empty:\n",
    "    processed_df = _ensure_processed(force=False)\n",
    "if processed_df is None or processed_df.empty:\n",
    "    print(\"Processed dataset snapshot is not available.\")\n",
    "else:\n",
    "    numeric_cols = (\n",
    "        dev_df.select_dtypes(include=['number'])\n",
    "        .columns.difference([cfg.target_column])\n",
    "    )\n",
    "    diagnostics = []\n",
    "    for col in numeric_cols:\n",
    "        raw_series = dev_df[col]\n",
    "        proc_series = processed_df[col]\n",
    "        diagnostics.append({\n",
    "            'feature': col,\n",
    "            'raw_missing': int(raw_series.isna().sum()),\n",
    "            'processed_missing': int(proc_series.isna().sum()),\n",
    "            'raw_mean': float(raw_series.mean()),\n",
    "            'processed_mean': float(proc_series.mean()),\n",
    "        })\n",
    "    diag_df = pd.DataFrame(diagnostics)\n",
    "    if diag_df.empty:\n",
    "        print(\"No numeric columns found for diagnostics.\")\n",
    "    else:\n",
    "        diag_df['missing_delta'] = diag_df['raw_missing'] - diag_df['processed_missing']\n",
    "        diag_df['mean_shift'] = diag_df['processed_mean'] - diag_df['raw_mean']\n",
    "        display(diag_df.sort_values(['missing_delta', 'mean_shift'], ascending=[False, False]).head(12))\n",
    "        top_cols = diag_df.sort_values(['missing_delta', 'mean_shift'], ascending=[False, False])['feature'].head(4).tolist()\n",
    "        if top_cols:\n",
    "            comparison = pd.concat({'raw': dev_df[top_cols], 'prepped': processed_df[top_cols]}, axis=1)\n",
    "            display(comparison.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8fac4",
   "metadata": {},
   "source": [
    "### 7.2 TSFresh feature contributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_local = _current_config()\n",
    "if cfg_local is not None and not getattr(cfg_local, 'enable_tsfresh_features', False):\n",
    "    print('TSFresh reports skipped (disabled via config).')\n",
    "else:\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    selection_results = results_ref.get('selection_results')\n",
    "    if not isinstance(selection_results, dict):\n",
    "        selection_results = None\n",
    "\n",
    "    tsfresh_meta = results_ref.get('tsfresh_metadata')\n",
    "    if tsfresh_meta is None and selection_results is not None:\n",
    "        tsfresh_meta = selection_results.get('tsfresh_metadata')\n",
    "    if tsfresh_meta is None and pipe is not None:\n",
    "        tsfresh_meta = pipe.data_.get('tsfresh_metadata')\n",
    "\n",
    "    if isinstance(tsfresh_meta, pd.DataFrame) and not tsfresh_meta.empty:\n",
    "        display(tsfresh_meta.head())\n",
    "    else:\n",
    "        print('TSFresh metadata is empty.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1794661",
   "metadata": {},
   "source": [
    "### 7.3 WOE transformation quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562875c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "selection_results = _ensure_selection(force=False) or {}\n",
    "woe_results = _ensure_woe(force=False) or {}\n",
    "if not woe_results:\n",
    "    print(\"WOE diagnostics skipped: run the WOE transformation cell first.\")\n",
    "else:\n",
    "    woe_values = woe_results.get('woe_values', {})\n",
    "    feature = next(iter(selection_results.get('selected_features', woe_values.keys())), None)\n",
    "    if feature is None:\n",
    "        print('No features available for WOE diagnostic display.')\n",
    "    else:\n",
    "        info = woe_values.get(feature, {})\n",
    "        print(f'Details for feature: {feature}')\n",
    "        if isinstance(info, dict) and info.get('stats'):\n",
    "            display(pd.DataFrame(info['stats']).head())\n",
    "        else:\n",
    "            print('  WOE stats not available for this feature.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b7fb3",
   "metadata": {},
   "source": [
    "### 7.4 Feature selection progression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4810bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "selection_results = _ensure_selection(force=False)\n",
    "if not selection_results:\n",
    "    print(\"Selection history is not available yet.\")\n",
    "else:\n",
    "    history = selection_results.get('selection_history')\n",
    "    if not history:\n",
    "        print('Selection history is empty.')\n",
    "    else:\n",
    "        rows = []\n",
    "        for step in history:\n",
    "            if not isinstance(step, dict):\n",
    "                continue\n",
    "            rows.append({\n",
    "                'method': step.get('method'),\n",
    "                'before': step.get('before'),\n",
    "                'after': step.get('after'),\n",
    "                'removed': ', '.join(sorted(step.get('removed', []))) if step.get('removed') else '',\n",
    "            })\n",
    "        display(pd.DataFrame(rows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e027d",
   "metadata": {},
   "source": [
    "### 7.5 Model performance comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe, results_ref = _get_pipeline_context()\n",
    "model_results = _ensure_model_results(force=False) or {}\n",
    "registry_df = _materialize_model_registry(model_results=model_results, force=False)\n",
    "globals()['MODEL_REGISTRY_DF'] = registry_df\n",
    "if isinstance(registry_df, pd.DataFrame) and not registry_df.empty:\n",
    "    display(registry_df)\n",
    "    if 'model_name' in registry_df.columns:\n",
    "        model_names = registry_df['model_name'].dropna().astype(str).unique().tolist()\n",
    "        if model_names:\n",
    "            print(f\"Available model names: {', '.join(model_names)}\")\n",
    "else:\n",
    "    print('Model registry is empty.')\n",
    "active_name = model_results.get('active_model_name') or model_results.get('best_model_name')\n",
    "if active_name:\n",
    "    print(f'Active model: {active_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb4cb14",
   "metadata": {},
   "source": [
    "## 8. Stage 1 & Stage 2 Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8c00e",
   "metadata": {},
   "source": [
    "### 8.1 Calibration metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c605241",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_local = _current_config()\n",
    "if cfg_local is not None and not getattr(cfg_local, 'enable_stage2_calibration', False):\n",
    "    print('Stage-1/Stage-2 calibration disabled via config; skipping metrics cell.')\n",
    "else:\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    stage1 = _ensure_stage1(force=True)\n",
    "    stage2 = _ensure_stage2(force=False)\n",
    "    if isinstance(stage1, dict) and stage1:\n",
    "        stage1_metrics = stage1.get('calibration_metrics', {})\n",
    "        print('Stage-1 calibration metrics:')\n",
    "        if stage1_metrics:\n",
    "            display(pd.DataFrame([stage1_metrics]))\n",
    "        else:\n",
    "            print('  Metrics unavailable.')\n",
    "    else:\n",
    "        print('Stage-1 calibration metrics are unavailable.')\n",
    "    if isinstance(stage2, dict) and stage2:\n",
    "        stage2_metrics = stage2.get('stage2_metrics', {})\n",
    "        print('Stage-2 calibration metrics:')\n",
    "        if stage2_metrics:\n",
    "            display(pd.DataFrame([stage2_metrics]))\n",
    "        else:\n",
    "            print('  Metrics unavailable.')\n",
    "    else:\n",
    "        print('Stage-2 calibration metrics are unavailable.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b50b0",
   "metadata": {},
   "source": [
    "### 8.2 Refresh calibration pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f61855",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_local = _current_config()\n",
    "if cfg_local is not None and not getattr(cfg_local, 'enable_scoring', False):\n",
    "    print('Scoring disabled via config; skipping.')\n",
    "else:\n",
    "\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    scoring_output = _ensure_scoring(force=False)\n",
    "    scoring_metrics = scoring_output.get('metrics') if isinstance(scoring_output, dict) else None\n",
    "    if scoring_metrics:\n",
    "        display(pd.DataFrame([scoring_metrics]))\n",
    "    else:\n",
    "        print('Scoring metrics not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15fe0ed",
   "metadata": {},
   "source": [
    "### 8.3 Next steps\n",
    "\n",
    "Calibrated outputs feed the risk band optimisation section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_local = _current_config()\n",
    "if cfg_local is not None and not getattr(cfg_local, 'enable_stage2_calibration', False):\n",
    "    print('Calibration pipeline disabled via config; skipping execution.')\n",
    "else:\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        print('Calibration skipped: pipeline instance not available yet.')\n",
    "    else:\n",
    "        stage1 = _ensure_stage1(force=True)\n",
    "        stage2 = _ensure_stage2(force=True)\n",
    "        _update_results(results_ref, calibration_stage1=stage1, calibration_stage2=stage2)\n",
    "        results = results_ref\n",
    "        print('Calibration refreshed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24908e1f",
   "metadata": {},
   "source": [
    "## 9. Risk Band Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42de06f2",
   "metadata": {},
   "source": [
    "### 9.1 Recompute risk bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671439b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe, results_ref = _get_pipeline_context()\n",
    "if pipe is None:\n",
    "    print('Risk band optimisation skipped: pipeline instance not available yet.')\n",
    "else:\n",
    "    stage1 = _ensure_stage1(force=False)\n",
    "    stage2 = _ensure_stage2(force=False)\n",
    "    override_df = pipe.data_.get('risk_band_reference') or NOTEBOOK_CONTEXT.get('data', {}).get('risk_band_reference')\n",
    "    bands = pipe.run_risk_bands(stage1_results=stage1, stage2_results=stage2, splits=pipe.results_.get('splits'), data_override=override_df, force=True)\n",
    "    _update_results(results_ref, risk_bands=bands)\n",
    "    results = results_ref\n",
    "    band_stats = None\n",
    "    if isinstance(bands, dict):\n",
    "        band_stats = bands.get('band_stats')\n",
    "        if (band_stats is None or (isinstance(band_stats, pd.DataFrame) and band_stats.empty)):\n",
    "            alt = bands.get('bands')\n",
    "            if isinstance(alt, pd.DataFrame) and not alt.empty:\n",
    "                band_stats = alt\n",
    "    if isinstance(band_stats, pd.DataFrame) and not band_stats.empty:\n",
    "        display(band_stats)\n",
    "    else:\n",
    "        print('Risk band statistics dataframe is empty.')\n",
    "    print('Risk bands recomputed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9604ac34",
   "metadata": {},
   "source": [
    "### 9.2 Risk band optimisation summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ad5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_local = _current_config()\n",
    "if cfg_local is not None and not getattr(cfg_local, 'optimize_risk_bands', False):\n",
    "    print('Risk band optimizer disabled via config; skipping.')\n",
    "else:\n",
    "\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    risk_band_results = _ensure_risk_bands(force=False)\n",
    "    if not risk_band_results:\n",
    "        print(\"Risk band optimizer did not produce results yet.\")\n",
    "    else:\n",
    "        band_stats = risk_band_results.get('band_stats')\n",
    "        if band_stats is None or (isinstance(band_stats, pd.DataFrame) and band_stats.empty):\n",
    "            bands_alt = risk_band_results.get('bands')\n",
    "            if isinstance(bands_alt, pd.DataFrame) and not bands_alt.empty:\n",
    "                band_stats = bands_alt\n",
    "        if isinstance(band_stats, pd.DataFrame) and not band_stats.empty:\n",
    "            display(band_stats)\n",
    "        else:\n",
    "            print('Risk band statistics dataframe is empty.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586df369",
   "metadata": {},
   "source": [
    "## 10. Optional: Consolidated Pipeline Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c344cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_FULL_PIPELINE = False\n",
    "\n",
    "if RUN_FULL_PIPELINE:\n",
    "    print('Running full pipeline to validate reproducibility...')\n",
    "\n",
    "    pipe, results_ref = _get_pipeline_context()\n",
    "    if pipe is None:\n",
    "        pipe = UnifiedRiskPipeline(cfg)\n",
    "        globals()['pipe'] = pipe\n",
    "    full_results = pipe.fit(\n",
    "        NOTEBOOK_CONTEXT['data']['development'],\n",
    "        data_dictionary=NOTEBOOK_CONTEXT['data']['dictionary'],\n",
    "        calibration_df=NOTEBOOK_CONTEXT['data']['calibration_longrun'],\n",
    "        stage2_df=NOTEBOOK_CONTEXT['data']['calibration_recent'],\n",
    "        risk_band_df=NOTEBOOK_CONTEXT['data']['risk_band_reference'],\n",
    "        score_df=NOTEBOOK_CONTEXT['data']['scoring'],\n",
    "    )\n",
    "    results = full_results\n",
    "    NOTEBOOK_CONTEXT['artifacts'].clear()\n",
    "    NOTEBOOK_CONTEXT['artifacts'].update(full_results)\n",
    "    print(f\"Best mode: {full_results.get('best_model_mode')} | Best model: {full_results.get('best_model_name')}\")\n",
    "    print('Model registry (top rows):')\n",
    "    model_registry = pd.DataFrame(full_results.get('model_registry', []))\n",
    "    if not model_registry.empty:\n",
    "        sort_columns = [col for col in ['mode', 'oot_auc', 'test_auc', 'train_auc'] if col in model_registry.columns]\n",
    "        if sort_columns:\n",
    "            asc_flags = [True] + [False] * (len(sort_columns) - 1)\n",
    "            display(model_registry.sort_values(sort_columns, ascending=asc_flags).head())\n",
    "        else:\n",
    "            display(model_registry.head())\n",
    "    else:\n",
    "        print('Model registry is empty.')\n",
    "else:\n",
    "    print('Skip: set RUN_FULL_PIPELINE = True to rerun the entire pipeline at once.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cef4aa",
   "metadata": {},
   "source": [
    "## 11. Recent Scoring Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e618a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "if pipe is None:\n",
    "    print('Scoring skipped: pipeline instance not available yet.')\n",
    "else:\n",
    "    scoring_output = pipe.run_scoring(NOTEBOOK_CONTEXT['data']['scoring'], force=True)\n",
    "    _update_results(results_ref, scoring_output=scoring_output)\n",
    "    results = results_ref\n",
    "    reports = _ensure_reports(force=True)\n",
    "    excel_path = reports.get('excel_path') if isinstance(reports, dict) else None\n",
    "    if excel_path:\n",
    "        print(f\"Latest reporting workbook: {excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18139d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe, results_ref = _get_pipeline_context()\n",
    "reports = _ensure_reports(force=True)\n",
    "if not reports:\n",
    "    print('Reporting artifacts are not available yet.')\n",
    "else:\n",
    "    excel_path = reports.get('excel_path')\n",
    "    if excel_path:\n",
    "        print(f\"Excel workbook generated: {excel_path}\")\n",
    "    available_keys = sorted(reports.keys())\n",
    "    display(pd.DataFrame({'report_key': available_keys}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c253f",
   "metadata": {},
   "source": [
    "For automation examples, see examples/quickstart_demo.py."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
