{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Risk Model Pipeline - Complete Example\n",
    "\n",
    "This notebook demonstrates the complete functionality of the unified risk model pipeline with all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import pipeline components\n",
    "from risk_pipeline import RiskModelPipeline\n",
    "from risk_pipeline.core.config import Config\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup\n",
    "\n",
    "Configure all pipeline parameters through the unified Config class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration\n",
    "config = Config(\n",
    "    # ==================== DATA COLUMNS ====================\n",
    "    target_column='target',\n",
    "    id_column='customer_id',  # Optional\n",
    "    time_column='application_date',  # Optional for OOT split\n",
    "    \n",
    "    # ==================== DATA SPLITTING ====================\n",
    "    create_test_split=True,\n",
    "    test_size=0.2,\n",
    "    stratify_test=True,  # Preserve event rate\n",
    "    oot_months=3,  # Last 3 months for OOT\n",
    "    oot_size=0.2,  # If no time column, use random split\n",
    "    \n",
    "    # ==================== SCORING ====================\n",
    "    enable_scoring=False,  # Disabled by default\n",
    "    \n",
    "    # ==================== WOE CONFIGURATION ====================\n",
    "    calculate_woe_all=True,  # Calculate WOE for all variables\n",
    "    woe_optimization_metric='iv',  # 'iv' or 'gini'\n",
    "    woe_max_bins=10,\n",
    "    woe_min_bins=2,\n",
    "    woe_min_bin_size=0.05,\n",
    "    woe_monotonic_numeric=True,  # Enforce monotonicity\n",
    "    woe_merge_insignificant=True,  # Merge insignificant bins\n",
    "    \n",
    "    # ==================== UNIVARIATE ANALYSIS ====================\n",
    "    calculate_univariate_gini=True,\n",
    "    check_woe_degradation=True,\n",
    "    woe_degradation_threshold=0.05,\n",
    "    \n",
    "    # ==================== FEATURE SELECTION ====================\n",
    "    selection_steps=[\n",
    "        'univariate',   # Filter by univariate gini/IV\n",
    "        'psi',         # PSI filter\n",
    "        'vif',         # VIF filter\n",
    "        'correlation', # Correlation clustering\n",
    "        'iv',          # IV filter\n",
    "        'boruta',      # Boruta selection\n",
    "        'stepwise'     # Stepwise selection\n",
    "    ],\n",
    "    \n",
    "    # Selection thresholds\n",
    "    min_univariate_gini=0.05,\n",
    "    max_psi=0.25,\n",
    "    max_vif=5.0,\n",
    "    max_correlation=0.95,\n",
    "    min_iv=0.02,\n",
    "    \n",
    "    # Stepwise configuration\n",
    "    stepwise_method='forward',  # 'forward', 'backward', 'stepwise', 'forward_1se'\n",
    "    stepwise_max_features=30,\n",
    "    stepwise_min_features=5,\n",
    "    stepwise_cv_folds=5,\n",
    "    \n",
    "    # Boruta configuration\n",
    "    boruta_estimator='lightgbm',  # 'lightgbm' or 'randomforest'\n",
    "    boruta_max_iter=100,\n",
    "    \n",
    "    # Noise sentinel\n",
    "    use_noise_sentinel=True,\n",
    "    noise_threshold=0.5,\n",
    "    \n",
    "    # ==================== MODEL TRAINING ====================\n",
    "    algorithms=[\n",
    "        'logistic',\n",
    "        'gam',\n",
    "        'catboost',\n",
    "        'lightgbm',\n",
    "        'xgboost',\n",
    "        'randomforest',\n",
    "        'extratrees'\n",
    "    ],\n",
    "    \n",
    "    # Training configuration\n",
    "    cv_folds=5,\n",
    "    scoring_metric='roc_auc',\n",
    "    early_stopping_rounds=50,\n",
    "    \n",
    "    # Hyperparameter optimization\n",
    "    use_optuna=True,\n",
    "    n_trials=100,\n",
    "    optuna_timeout=3600,\n",
    "    \n",
    "    # Dual pipeline\n",
    "    enable_dual=True,  # Run both WOE and RAW pipelines\n",
    "    \n",
    "    # ==================== CALIBRATION ====================\n",
    "    calibration_method='isotonic',  # 'isotonic' or 'sigmoid'\n",
    "    calibration_cv_folds=3,\n",
    "    enable_stage2_calibration=True,\n",
    "    stage2_lower_bound=0.8,\n",
    "    stage2_upper_bound=1.2,\n",
    "    \n",
    "    # ==================== RISK BANDS ====================\n",
    "    optimize_risk_bands=True,\n",
    "    n_risk_bands=10,\n",
    "    risk_band_method='quantile',  # 'quantile', 'equal_width', 'optimal'\n",
    "    risk_band_tests=['binomial', 'hosmer_lemeshow', 'herfindahl'],\n",
    "    business_risk_ratings=['AAA', 'AA', 'A', 'BBB', 'BB', 'B', 'CCC', 'CC', 'C', 'D'],\n",
    "    \n",
    "    # ==================== REPORTING ====================\n",
    "    calculate_shap=True,\n",
    "    shap_sample_size=1000,\n",
    "    include_variable_dictionary=True,\n",
    "    report_components=[\n",
    "        'model_comparison',\n",
    "        'feature_importance',\n",
    "        'woe_bins',\n",
    "        'univariate_analysis',\n",
    "        'risk_bands',\n",
    "        'statistical_tests'\n",
    "    ],\n",
    "    \n",
    "    # ==================== OUTPUT ====================\n",
    "    output_folder='outputs',\n",
    "    model_name_prefix='risk_model',\n",
    "    save_models=True,\n",
    "    save_reports=True,\n",
    "    save_plots=True,\n",
    "    \n",
    "    # ==================== SYSTEM ====================\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Configuration created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "# Replace with your actual data loading\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "print(f\"Data loaded: {train_data.shape}\")\n",
    "print(f\"Target distribution:\\n{train_data['target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Optional: Load calibration data\n",
    "calibration_data = None  # pd.read_csv('data/calibration.csv')\n",
    "stage2_calibration_data = None  # pd.read_csv('data/recent_predictions.csv')\n",
    "\n",
    "# Optional: Load variable dictionary\n",
    "variable_dictionary = None  # pd.read_excel('data/variable_dictionary.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize and Fit Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = RiskModelPipeline(config)\n",
    "print(\"Pipeline initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline\n",
    "results = pipeline.fit(\n",
    "    train_df=train_data,\n",
    "    calibration_df=calibration_data,\n",
    "    stage2_calibration_df=stage2_calibration_data,\n",
    "    variable_dictionary=variable_dictionary\n",
    ")\n",
    "\n",
    "print(\"\\nPipeline training completed!\")\n",
    "print(f\"Best model: {results['best_model']}\")\n",
    "print(f\"Number of selected features: {len(results['selected_features'][results['best_model']])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Examine Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model features\n",
    "best_features = results['selected_features'][results['best_model']]\n",
    "print(f\"\\nSelected features for best model ({len(best_features)}):\")\n",
    "for i, feature in enumerate(best_features[:20], 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "if len(best_features) > 20:\n",
    "    print(f\"... and {len(best_features) - 20} more features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "model_metrics = results['metrics']\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, metrics in model_metrics.items():\n",
    "    row = {'Model': model_name}\n",
    "    for dataset, dataset_metrics in metrics.items():\n",
    "        row[f'{dataset}_auc'] = dataset_metrics.get('auc', 0)\n",
    "        row[f'{dataset}_gini'] = dataset_metrics.get('gini', 0)\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('oot_auc', ascending=False)\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate statistics\n",
    "univariate_summary = []\n",
    "for feature, stats in results['univariate_stats'].items():\n",
    "    univariate_summary.append({\n",
    "        'Feature': feature,\n",
    "        'IV': stats.get('iv', 0),\n",
    "        'Raw_Gini': stats.get('raw_gini', 0),\n",
    "        'WOE_Gini': stats.get('woe_gini', 0),\n",
    "        'Degradation': stats.get('woe_degradation', False)\n",
    "    })\n",
    "\n",
    "univariate_df = pd.DataFrame(univariate_summary)\n",
    "univariate_df = univariate_df.sort_values('IV', ascending=False)\n",
    "print(\"\\nTop 10 Features by Information Value:\")\n",
    "print(univariate_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk bands analysis\n",
    "if results.get('risk_bands'):\n",
    "    band_stats = results['risk_bands'].get('band_stats')\n",
    "    if band_stats is not None and not band_stats.empty:\n",
    "        print(\"\\nRisk Bands:\")\n",
    "        print(band_stats[['band', 'n_samples', 'event_rate', 'sample_pct']])\n",
    "        \n",
    "        # Statistical tests\n",
    "        test_results = results['risk_bands'].get('test_results', {})\n",
    "        print(\"\\nStatistical Tests:\")\n",
    "        for test_name, test_result in test_results.items():\n",
    "            print(f\"  {test_name}: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scoring New Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First enable scoring\n",
    "pipeline.config.enable_scoring = True\n",
    "\n",
    "# Load new data to score\n",
    "# new_data = pd.read_csv('data/new_data.csv')\n",
    "\n",
    "# Score using best model\n",
    "# scores = pipeline.score(\n",
    "#     df=new_data,\n",
    "#     model_name='best',  # or specific model name\n",
    "#     return_calibrated=True\n",
    "# )\n",
    "\n",
    "# print(f\"Scored {len(scores)} records\")\n",
    "# print(f\"Score distribution:\")\n",
    "# print(pd.Series(scores).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fitted pipeline\n",
    "pipeline.save_pipeline()\n",
    "print(\"Pipeline saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Step-by-Step Execution (Alternative Approach)\n",
    "\n",
    "You can also run the pipeline step by step for more control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of manual step-by-step execution\n",
    "# This gives you more control over each step\n",
    "\n",
    "# Step 1: Data splitting\n",
    "# pipeline._split_data(train_data)\n",
    "# print(f\"Train: {len(pipeline.train_data)}, Test: {len(pipeline.test_data)}, OOT: {len(pipeline.oot_data)}\")\n",
    "\n",
    "# Step 2: Variable classification\n",
    "# numeric_cols, categorical_cols = pipeline._classify_variables(pipeline.train_data)\n",
    "# print(f\"Numeric: {len(numeric_cols)}, Categorical: {len(categorical_cols)}\")\n",
    "\n",
    "# Step 3: Preprocessing\n",
    "# pipeline._preprocess_data(numeric_cols, categorical_cols)\n",
    "\n",
    "# Step 4: WOE calculation\n",
    "# pipeline._calculate_woe_all_variables(numeric_cols, categorical_cols)\n",
    "# print(f\"WOE calculated for {len(pipeline.woe_transformers)} variables\")\n",
    "\n",
    "# Continue with other steps..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Configuration Examples for Different Use Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Quick model without optimization\n",
    "quick_config = Config(\n",
    "    target_column='target',\n",
    "    algorithms=['logistic', 'lightgbm'],  # Only 2 algorithms\n",
    "    use_optuna=False,  # No hyperparameter optimization\n",
    "    selection_steps=['correlation', 'iv'],  # Simple selection\n",
    "    enable_dual=False,  # Only WOE pipeline\n",
    "    calculate_shap=False,  # Skip SHAP\n",
    "    optimize_risk_bands=False  # Skip risk bands\n",
    ")\n",
    "\n",
    "print(\"Quick configuration created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Production scoring configuration\n",
    "scoring_config = Config(\n",
    "    target_column='target',\n",
    "    enable_scoring=True,  # Enable scoring\n",
    "    save_models=True,\n",
    "    save_reports=False,  # No reports for scoring\n",
    "    calculate_shap=False,\n",
    "    optimize_risk_bands=True,  # Include risk bands for scoring\n",
    "    enable_stage2_calibration=True  # Apply calibration\n",
    ")\n",
    "\n",
    "print(\"Scoring configuration created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Explainable model configuration\n",
    "explainable_config = Config(\n",
    "    target_column='target',\n",
    "    algorithms=['logistic', 'gam'],  # Interpretable models\n",
    "    calculate_woe_all=True,  # WOE for interpretability\n",
    "    calculate_shap=True,  # SHAP analysis\n",
    "    stepwise_method='forward_1se',  # Conservative selection\n",
    "    stepwise_max_features=15,  # Limit features for interpretability\n",
    "    report_components=['feature_importance', 'woe_bins', 'shap_analysis']\n",
    ")\n",
    "\n",
    "print(\"Explainable model configuration created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Unified Configuration**: All parameters controlled through single Config class\n",
    "2. **Complete Pipeline**: From data splitting to model training to reporting\n",
    "3. **Feature Selection**: Multiple methods including Boruta and stepwise\n",
    "4. **WOE Optimization**: IV/Gini based optimization with monotonicity\n",
    "5. **Model Training**: Multiple algorithms with Optuna optimization\n",
    "6. **Calibration**: Two-stage calibration support\n",
    "7. **Risk Bands**: Optimization with statistical tests\n",
    "8. **Scoring**: Disabled by default, can be enabled when needed\n",
    "\n",
    "The pipeline is designed to be:\n",
    "- **Flexible**: Configure only what you need\n",
    "- **Comprehensive**: All features in one place\n",
    "- **Production-ready**: Scoring disabled by default\n",
    "- **Reproducible**: Random state control throughout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}