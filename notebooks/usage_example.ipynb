{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Model Pipeline - Notebook Usage Examples\n",
    "\n",
    "Bu notebook risk-model-pipeline paketinin farklı kullanım senaryolarını gösterir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Paketi Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paketi GitHub'dan yükleme\n",
    "!pip install git+https://github.com/selimoksuz/risk-model-pipeline.git\n",
    "\n",
    "# veya local olarak clone'ladıysanız\n",
    "# !pip install -e /path/to/risk-model-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Pipeline imports\n",
    "from risk_pipeline.pipeline16 import Config, RiskModelPipeline\n",
    "from risk_pipeline.utils.scoring import load_model_artifacts, score_data\n",
    "from risk_pipeline.utils.pipeline_runner import run_pipeline_from_dataframe, get_full_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Senaryo 1: Sadece Model Eğitimi (Kalibrasyon Yok, Skorlama Yok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame hazırlama (sizin verileriniz olacak)\n",
    "train_df = pd.DataFrame({\n",
    "    'app_id': range(1000),\n",
    "    'app_dt': pd.date_range('2024-01-01', periods=1000, freq='D'),\n",
    "    'target': np.random.binomial(1, 0.2, 1000),  # %20 default rate\n",
    "    'age': np.random.randint(18, 70, 1000),\n",
    "    'income': np.random.lognormal(10, 0.5, 1000),\n",
    "    'credit_score': np.random.randint(300, 850, 1000),\n",
    "    'region': np.random.choice(['A', 'B', 'C'], 1000)\n",
    "})\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Default rate: {train_df['target'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model eğitimi - SADECE MODEL\nconfig = Config(\n    id_col=\"app_id\",\n    time_col=\"app_dt\",\n    target_col=\"target\",\n    \n    # Performans ayarları\n    hpo_trials=5,  # Hızlı test için düşük\n    hpo_timeout_sec=30,\n    \n    # Kalibrasyon YOK\n    calibration_data_path=None,\n    \n    random_state=42\n)\n\n# Pipeline çalıştır\npipeline = RiskModelPipeline(config)\npipeline.run(train_df)  # run() metodu pipeline objesini döndürür (method chaining için)\n\nprint(f\"\\nBest Model: {pipeline.best_model_name_}\")\nprint(f\"Final Features: {pipeline.final_vars_}\")\n\n# run_id'yi config'den alabilirsiniz\nprint(f\"Run ID: {pipeline.cfg.run_id}\")\n\n# Model performansını Excel raporundan kontrol edebilirsiniz\nprint(f\"Check outputs/model_report.xlsx for detailed metrics\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeli kaydet\n",
    "model_path = f\"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "joblib.dump(pipeline.models_[pipeline.best_model_name_], model_path)\n",
    "print(f\"Model saved: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Senaryo 2: Model + Kalibrasyon (Skorlama Yok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Kalibrasyon verisi hazırlama (DataFrame olarak!)\ncalibration_df = pd.DataFrame({\n    'app_id': range(2000, 2500),\n    'app_dt': pd.date_range('2024-06-01', periods=500, freq='D'),\n    'target': np.random.binomial(1, 0.25, 500),\n    'age': np.random.randint(18, 70, 500),\n    'income': np.random.lognormal(10, 0.5, 500),\n    'credit_score': np.random.randint(300, 850, 500),\n    'region': np.random.choice(['A', 'B', 'C'], 500)\n})\n\nprint(f\"Calibration DataFrame: {calibration_df.shape}\")\nprint(\"Note: Calibration can now be provided as DataFrame directly, no CSV file needed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model + Kalibrasyon (DataFrame ile!)\nconfig_with_cal = Config(\n    id_col=\"app_id\",\n    time_col=\"app_dt\",\n    target_col=\"target\",\n    \n    # Kalibrasyon AÇIK - artık DataFrame olarak verilebilir!\n    calibration_df=calibration_df,  # DataFrame direkt olarak\n    # calibration_data_path=\"calibration_data.csv\",  # veya CSV dosyası\n    calibration_method=\"isotonic\",  # veya \"sigmoid\"\n    \n    hpo_trials=5,\n    hpo_timeout_sec=30,\n    random_state=42\n)\n\npipeline_cal = RiskModelPipeline(config_with_cal)\nresults_cal = pipeline_cal.run(train_df)\n\nprint(f\"\\nModel with DataFrame calibration trained!\")\nprint(f\"Calibrator: {type(pipeline_cal.calibrator_) if hasattr(pipeline_cal, 'calibrator_') else 'None'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Senaryo 3: Sadece Skorlama (Önceden Eğitilmiş Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skorlama verisi hazırlama\n",
    "scoring_df = pd.DataFrame({\n",
    "    'app_id': range(3000, 4000),\n",
    "    'app_dt': pd.date_range('2024-08-01', periods=1000, freq='D'),\n",
    "    'target': [np.nan] * 600 + list(np.random.binomial(1, 0.3, 400)),  # %60'ı targetsız\n",
    "    'age': np.random.randint(18, 70, 1000),\n",
    "    'income': np.random.lognormal(10, 0.5, 1000),\n",
    "    'credit_score': np.random.randint(300, 850, 1000),\n",
    "    'region': np.random.choice(['A', 'B', 'C'], 1000)\n",
    "})\n",
    "\n",
    "print(f\"Scoring data: {scoring_df.shape}\")\n",
    "print(f\"With target: {(~scoring_df['target'].isna()).sum()}\")\n",
    "print(f\"Without target: {scoring_df['target'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model artifacts'ları yükle\n# NOT: pipeline.run() None döndürdüğü için run_id'yi pipeline'dan almalıyız\nrun_id = pipeline.cfg.run_id  # Config'den run_id\noutput_folder = 'outputs'\n\n# Alternatif: Son oluşturulan model dosyalarını bulun\nimport os\nfrom pathlib import Path\n\noutput_path = Path(output_folder)\nmodel_files = sorted(output_path.glob(\"best_model_*.joblib\"))\nif model_files:\n    latest_model = model_files[-1]\n    # run_id'yi dosya adından çıkar\n    run_id = latest_model.stem.replace(\"best_model_\", \"\")\n    print(f\"Using model from run: {run_id}\")\n\n# Model artifacts yükle\ntry:\n    model, final_features, woe_mapping, calibrator = load_model_artifacts(output_folder, run_id)\n    print(f\"Model loaded: {type(model).__name__}\")\n    print(f\"Calibrator: {'Available' if calibrator else 'Not available'}\")\nexcept Exception as e:\n    print(f\"Could not load artifacts: {e}\")\n    # Manuel yükleme\n    model = joblib.load(f\"{output_folder}/best_model_{run_id}.joblib\")\n    with open(f\"{output_folder}/final_vars_{run_id}.json\") as f:\n        final_features = json.load(f)\n    with open(f\"{output_folder}/woe_mapping_{run_id}.json\") as f:\n        woe_mapping = json.load(f)\n    calibrator = None\n    print(\"Artifacts loaded manually\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skorlama yap\n",
    "from risk_pipeline.utils.scoring import apply_woe_transform\n",
    "\n",
    "# Training scores for PSI (opsiyonel)\n",
    "train_woe = apply_woe_transform(train_df, woe_mapping)\n",
    "# ... feature mapping logic ...\n",
    "training_scores = None  # PSI istemiyorsanız None bırakabilirsiniz\n",
    "\n",
    "# Skorlama\n",
    "scoring_results = score_data(\n",
    "    scoring_df=scoring_df,\n",
    "    model=model,\n",
    "    final_features=final_features,\n",
    "    woe_mapping=woe_mapping,\n",
    "    calibrator=calibrator,  # Varsa kullanılır\n",
    "    training_scores=training_scores,  # PSI için\n",
    "    feature_mapping=None\n",
    ")\n",
    "\n",
    "print(f\"\\nScoring Results:\")\n",
    "print(f\"Total scored: {scoring_results['n_total']}\")\n",
    "print(f\"With target: {scoring_results['n_with_target']}\")\n",
    "print(f\"Without target: {scoring_results['n_without_target']}\")\n",
    "\n",
    "if 'with_target' in scoring_results:\n",
    "    print(f\"\\nPerformance (with target):\")\n",
    "    print(f\"  AUC: {scoring_results['with_target']['auc']:.3f}\")\n",
    "    print(f\"  Gini: {scoring_results['with_target']['gini']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Senaryo 4: Sonradan Kalibrasyon Ekleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Önceden eğitilmiş model var, şimdi kalibrasyon ekleyelim\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom risk_pipeline.utils.scoring import apply_woe_transform\n\n# Kalibrasyon verisi hazırla\ncal_df = pd.DataFrame({\n    'app_id': range(5000, 5300),\n    'app_dt': pd.date_range('2024-09-01', periods=300, freq='D'),\n    'target': np.random.binomial(1, 0.22, 300),\n    'age': np.random.randint(18, 70, 300),\n    'income': np.random.lognormal(10, 0.5, 300),\n    'credit_score': np.random.randint(300, 850, 300),\n    'region': np.random.choice(['A', 'B', 'C'], 300)\n})\n\n# WOE dönüşümü uygula\ncal_woe = apply_woe_transform(cal_df, woe_mapping)\nX_cal = cal_woe[final_features] if isinstance(final_features, list) else cal_woe\ny_cal = cal_df['target'].values\n\n# Model tahminleri al\nraw_scores = model.predict_proba(X_cal)[:, 1] if hasattr(model, 'predict_proba') else model.predict(X_cal)\n\n# Kalibratör eğit\ncalibrator = IsotonicRegression(out_of_bounds='clip')\ncalibrator.fit(raw_scores.reshape(-1, 1), y_cal)\n\n# Kalibratörü kaydet\n# run_id'yi pipeline'dan veya dosya adından al\nif 'pipeline' in locals() and hasattr(pipeline, 'cfg'):\n    run_id = pipeline.cfg.run_id\nelse:\n    # Son model dosyasından al\n    import os\n    from pathlib import Path\n    model_files = sorted(Path('outputs').glob(\"best_model_*.joblib\"))\n    if model_files:\n        run_id = model_files[-1].stem.replace(\"best_model_\", \"\")\n\njoblib.dump(calibrator, f'calibrator_{run_id}.pkl')\nprint(f\"Calibrator trained and saved for run_id: {run_id}!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Senaryo 5: Tümleşik Kullanım (Helper Function ile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# En kolay kullanım: helper function (DataFrame kalibrasyon desteği ile!)\nfrom risk_pipeline.utils.pipeline_runner import run_pipeline_from_dataframe\n\n# Kalibrasyon DataFrame'i hazırla\ncal_df_for_helper = pd.DataFrame({\n    'app_id': range(3500, 3700),\n    'app_dt': pd.date_range('2024-07-01', periods=200, freq='D'),\n    'target': np.random.binomial(1, 0.23, 200),\n    'age': np.random.randint(18, 70, 200),\n    'income': np.random.lognormal(10, 0.5, 200),\n    'credit_score': np.random.randint(300, 850, 200),\n    'region': np.random.choice(['A', 'B', 'C'], 200)\n})\n\n# Tek fonksiyonda her şey (DataFrame kalibrasyon ile!)\nresults = run_pipeline_from_dataframe(\n    df=train_df,\n    id_col=\"app_id\",\n    time_col=\"app_dt\",\n    target_col=\"target\",\n    \n    # Kalibrasyon artık DataFrame olarak verilebilir!\n    calibration_df=cal_df_for_helper,  # DataFrame olarak\n    # calibration_data_path=\"calibration.csv\",  # veya dosya yolu\n    \n    scoring_df=scoring_df,  # İsterseniz skorlama da\n    \n    # Performans\n    hpo_trials=10,\n    hpo_timeout_sec=60,\n    \n    # Çıktılar\n    output_folder=\"my_outputs\",\n    output_excel=\"my_report.xlsx\"\n)\n\nprint(f\"\\nPipeline Results:\")\nprint(f\"Best Model: {results['best_model']}\")\nprint(f\"Final Features: {results['final_features']}\")\nprint(f\"Files saved to: {results['output_folder']}\")\nprint(\"\\nNOTE: Calibration provided as DataFrame - no CSV file needed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Özel Kullanım: Kendi Modelinizi Entegre Etme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kendi modelinizi kullanmak isterseniz\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Pipeline'dan WOE dönüşümlerini al\n",
    "woe_mapping = pipeline.woe_map\n",
    "final_features = pipeline.final_vars_\n",
    "\n",
    "# WOE dönüşümü uygula\n",
    "from risk_pipeline.stages.woe import apply_woe\n",
    "train_woe = apply_woe(train_df, woe_mapping)\n",
    "X_train = train_woe[final_features]\n",
    "y_train = train_df['target']\n",
    "\n",
    "# Kendi modelinizi eğitin\n",
    "my_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Custom model trained!\")\n",
    "\n",
    "# Skorlama için kullanabilirsiniz\n",
    "scoring_results = score_data(\n",
    "    scoring_df=scoring_df,\n",
    "    model=my_model,  # Kendi modeliniz\n",
    "    final_features=final_features,\n",
    "    woe_mapping=woe_mapping,\n",
    "    calibrator=None,  # İsterseniz ekleyin\n",
    "    training_scores=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Scoring Örneği"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Büyük veri için batch scoring\n",
    "def batch_score(df, model, features, woe_mapping, batch_size=1000):\n",
    "    \"\"\"Score large datasets in batches\"\"\"\n",
    "    all_scores = []\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Score batch\n",
    "        batch_results = score_data(\n",
    "            scoring_df=batch,\n",
    "            model=model,\n",
    "            final_features=features,\n",
    "            woe_mapping=woe_mapping,\n",
    "            calibrator=None,\n",
    "            training_scores=None\n",
    "        )\n",
    "        \n",
    "        all_scores.extend(batch_results['scores'])\n",
    "        print(f\"Batch {i//batch_size + 1}: {len(batch)} records scored\")\n",
    "    \n",
    "    return np.array(all_scores)\n",
    "\n",
    "# Kullanım\n",
    "large_df = pd.concat([scoring_df] * 10)  # 10,000 rows\n",
    "scores = batch_score(large_df, model, final_features, woe_mapping)\n",
    "print(f\"\\nTotal scored: {len(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Performans Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zaman içinde PSI takibi\n",
    "def monitor_psi(model, features, woe_mapping, training_scores, new_data_list):\n",
    "    \"\"\"Monitor PSI over time\"\"\"\n",
    "    psi_history = []\n",
    "    \n",
    "    for period, new_df in enumerate(new_data_list):\n",
    "        results = score_data(\n",
    "            scoring_df=new_df,\n",
    "            model=model,\n",
    "            final_features=features,\n",
    "            woe_mapping=woe_mapping,\n",
    "            calibrator=None,\n",
    "            training_scores=training_scores\n",
    "        )\n",
    "        \n",
    "        psi = results.get('psi_score', None)\n",
    "        if psi:\n",
    "            psi_history.append({\n",
    "                'period': period,\n",
    "                'psi': psi,\n",
    "                'status': 'Stable' if psi < 0.1 else 'Drift' if psi < 0.25 else 'Significant Drift'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(psi_history)\n",
    "\n",
    "# Simülasyon\n",
    "monthly_data = [scoring_df.sample(100) for _ in range(6)]  # 6 aylık veri\n",
    "psi_df = monitor_psi(model, final_features, woe_mapping, training_scores, monthly_data)\n",
    "print(psi_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}